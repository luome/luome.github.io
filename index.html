<!DOCTYPE html>
<head><meta charset="utf-8"></meta>
 <meta content="minimum-scale=1, initial-scale=1, width=device-width, shrink-to-fit=no" name="viewport"></meta>
 <link type="text/css" href="static/css/tabler-icons.min.css" rel="stylesheet"></link>
 <link type="text/css" href="static/css/style.css" rel="stylesheet"></link>
 <link type="text/css" href="static/css/custom.css" rel="stylesheet"></link>
 <link type="text/css" href="static/css/export.css" rel="stylesheet"></link>
 <link href="static/img/logo.png" type="image/png" rel="shortcut icon"></link>
 <link href="static/img/logo.png" sizes="192x192" rel="shortcut icon"></link>
 <link href="static/img/logo.png" rel="apple-touch-icon"></link>
 <meta name="apple-mobile-web-app-title"></meta>
 <meta name="apple-mobile-web-app-capable" content="yes"></meta>
 <meta name="apple-touch-fullscreen" content="yes"></meta>
 <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"></meta>
 <meta name="mobile-web-app-capable" content="yes"></meta>
 <meta property="og:title"></meta>
 <meta content="site" property="og:type"></meta>
  <meta content="static/img/logo.png" property="og:image"></meta>
 <meta property="og:description"></meta>
 <title></title>
 <meta property="og:site_name"></meta>
 <meta></meta>
</head>
 <body><div id="root"></div>
 <script>window.logseq_db="[logseq____&quot;~#datascript/DBlogseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;~:schemalogseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;~:ast/versionlogseq____&quot;,[logseq____&quot;^ logseq____&quot;],logseq____&quot;~:file/contentlogseq____&quot;,[logseq____&quot;^ logseq____&quot;],logseq____&quot;~:block/properties-text-valueslogseq____&quot;,[logseq____&quot;^ logseq____&quot;],logseq____&quot;~:block/aliaslogseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;~:db/valueTypelogseq____&quot;,logseq____&quot;~:db.type/reflogseq____&quot;,logseq____&quot;~:db/cardinalitylogseq____&quot;,logseq____&quot;~:db.cardinality/manylogseq____&quot;],logseq____&quot;~:block/pre-block?logseq____&quot;,[logseq____&quot;^ logseq____&quot;],logseq____&quot;~:block/uuidlogseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;~:db/uniquelogseq____&quot;,logseq____&quot;~:db.unique/identitylogseq____&quot;],logseq____&quot;~:block/prioritylogseq____&quot;,[logseq____&quot;^ logseq____&quot;],logseq____&quot;~:block/propertieslogseq____&quot;,[logseq____&quot;^ logseq____&quot;],logseq____&quot;~:block/journal?logseq____&quot;,[logseq____&quot;^ logseq____&quot;],logseq____&quot;~:block/namespacelogseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^6logseq____&quot;,logseq____&quot;^7logseq____&quot;],logseq____&quot;~:block/updated-atlogseq____&quot;,[logseq____&quot;^ logseq____&quot;],logseq____&quot;~:block/repeated?logseq____&quot;,[logseq____&quot;^ logseq____&quot;],logseq____&quot;~:db/typelogseq____&quot;,[logseq____&quot;^ logseq____&quot;],logseq____&quot;~:file/handlelogseq____&quot;,[logseq____&quot;^ logseq____&quot;],logseq____&quot;~:block/leftlogseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^6logseq____&quot;,logseq____&quot;^7logseq____&quot;,logseq____&quot;~:db/indexlogseq____&quot;,true],logseq____&quot;~:block/refslogseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^6logseq____&quot;,logseq____&quot;^7logseq____&quot;,logseq____&quot;^8logseq____&quot;,logseq____&quot;^9logseq____&quot;],logseq____&quot;~:block/scheduledlogseq____&quot;,[logseq____&quot;^ logseq____&quot;],logseq____&quot;~:block/properties-orderlogseq____&quot;,[logseq____&quot;^ logseq____&quot;],logseq____&quot;~:block/created-atlogseq____&quot;,[logseq____&quot;^ logseq____&quot;],logseq____&quot;~:block/deadlinelogseq____&quot;,[logseq____&quot;^ logseq____&quot;],logseq____&quot;~:block/collapsed?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^Glogseq____&quot;,true],logseq____&quot;~:block/journal-daylogseq____&quot;,[logseq____&quot;^ logseq____&quot;],logseq____&quot;~:block/formatlogseq____&quot;,[logseq____&quot;^ logseq____&quot;],logseq____&quot;~:block/tagslogseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^6logseq____&quot;,logseq____&quot;^7logseq____&quot;,logseq____&quot;^8logseq____&quot;,logseq____&quot;^9logseq____&quot;],logseq____&quot;~:block/contentlogseq____&quot;,[logseq____&quot;^ logseq____&quot;],logseq____&quot;~:recent/pageslogseq____&quot;,[logseq____&quot;^ logseq____&quot;],logseq____&quot;~:block/macroslogseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^6logseq____&quot;,logseq____&quot;^7logseq____&quot;,logseq____&quot;^8logseq____&quot;,logseq____&quot;^9logseq____&quot;],logseq____&quot;~:db/identlogseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^logseq____&lt;logseq____&quot;,logseq____&quot;^=logseq____&quot;],logseq____&quot;~:block/path-refslogseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^6logseq____&quot;,logseq____&quot;^7logseq____&quot;,logseq____&quot;^8logseq____&quot;,logseq____&quot;^9logseq____&quot;],logseq____&quot;~:block/parentlogseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^6logseq____&quot;,logseq____&quot;^7logseq____&quot;,logseq____&quot;^Glogseq____&quot;,true],logseq____&quot;~:block/typelogseq____&quot;,[logseq____&quot;^ logseq____&quot;],logseq____&quot;~:block/pagelogseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^6logseq____&quot;,logseq____&quot;^7logseq____&quot;,logseq____&quot;^Glogseq____&quot;,true],logseq____&quot;~:block/namelogseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^logseq____&lt;logseq____&quot;,logseq____&quot;^=logseq____&quot;],logseq____&quot;~:file/pathlogseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^logseq____&lt;logseq____&quot;,logseq____&quot;^=logseq____&quot;],logseq____&quot;~:block/filelogseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^6logseq____&quot;,logseq____&quot;^7logseq____&quot;],logseq____&quot;~:block/markerlogseq____&quot;,[logseq____&quot;^ logseq____&quot;],logseq____&quot;~:block/original-namelogseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^logseq____&lt;logseq____&quot;,logseq____&quot;^=logseq____&quot;],logseq____&quot;~:schema/versionlogseq____&quot;,[logseq____&quot;^ logseq____&quot;]],logseq____&quot;~:datomslogseq____&quot;,[logseq____&quot;~#listlogseq____&quot;,[[logseq____&quot;~#datascript/Datomlogseq____&quot;,[1,logseq____&quot;^12logseq____&quot;,2,536870913]],[logseq____&quot;^15logseq____&quot;,[2,logseq____&quot;^@logseq____&quot;,false,536870914]],[logseq____&quot;^15logseq____&quot;,[2,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;cardlogseq____&quot;,536870913]],[logseq____&quot;^15logseq____&quot;,[2,logseq____&quot;^11logseq____&quot;,logseq____&quot;cardlogseq____&quot;,536870913]],[logseq____&quot;^15logseq____&quot;,[2,logseq____&quot;^;logseq____&quot;,logseq____&quot;~ub5c06b40-cd3d-44db-97dd-24f0416fcceblogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[3,logseq____&quot;^@logseq____&quot;,false,536870914]],[logseq____&quot;^15logseq____&quot;,[3,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;canceledlogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[3,logseq____&quot;^11logseq____&quot;,logseq____&quot;CANCELEDlogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[3,logseq____&quot;^;logseq____&quot;,logseq____&quot;~ub6e9cbad-a07c-4f46-a227-53b55e3b0575logseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[4,logseq____&quot;^@logseq____&quot;,false,536870914]],[logseq____&quot;^15logseq____&quot;,[4,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;todologseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[4,logseq____&quot;^11logseq____&quot;,logseq____&quot;TODOlogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[4,logseq____&quot;^;logseq____&quot;,logseq____&quot;~uf50b5b92-bf0b-410a-8ce7-b48a9715e4b7logseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[5,logseq____&quot;^@logseq____&quot;,false,536870914]],[logseq____&quot;^15logseq____&quot;,[5,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;nowlogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[5,logseq____&quot;^11logseq____&quot;,logseq____&quot;NOWlogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[5,logseq____&quot;^;logseq____&quot;,logseq____&quot;~ua6aa7b29-8f34-40b8-b628-41148605b740logseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[6,logseq____&quot;^@logseq____&quot;,false,536870914]],[logseq____&quot;^15logseq____&quot;,[6,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;laterlogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[6,logseq____&quot;^11logseq____&quot;,logseq____&quot;LATERlogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[6,logseq____&quot;^;logseq____&quot;,logseq____&quot;~ue6584b7c-d84a-4a6c-a3cb-b7bed98d29belogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[7,logseq____&quot;^@logseq____&quot;,false,536870914]],[logseq____&quot;^15logseq____&quot;,[7,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;donelogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[7,logseq____&quot;^11logseq____&quot;,logseq____&quot;DONElogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[7,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-4857-4997-8361-0f7b8ab5adf6logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[8,logseq____&quot;^@logseq____&quot;,false,536870914]],[logseq____&quot;^15logseq____&quot;,[8,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;doinglogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[8,logseq____&quot;^11logseq____&quot;,logseq____&quot;DOINGlogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[8,logseq____&quot;^;logseq____&quot;,logseq____&quot;~ub5a2609d-9664-420c-9a1e-3fac0f1e05ablogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[9,logseq____&quot;^@logseq____&quot;,false,536870914]],[logseq____&quot;^15logseq____&quot;,[9,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;in-progresslogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[9,logseq____&quot;^11logseq____&quot;,logseq____&quot;IN-PROGRESSlogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[9,logseq____&quot;^;logseq____&quot;,logseq____&quot;~ub7507ebb-d412-4279-a8af-c69b8d4a55c1logseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[10,logseq____&quot;^@logseq____&quot;,false,536870914]],[logseq____&quot;^15logseq____&quot;,[10,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;clogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[10,logseq____&quot;^11logseq____&quot;,logseq____&quot;Clogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[10,logseq____&quot;^;logseq____&quot;,logseq____&quot;~uf9e534b7-9eae-4a38-b91b-b5591d48fb40logseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[11,logseq____&quot;^@logseq____&quot;,false,536870914]],[logseq____&quot;^15logseq____&quot;,[11,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;blogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[11,logseq____&quot;^11logseq____&quot;,logseq____&quot;Blogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[11,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u61ed7a99-8b27-4934-b4b1-ea4fb37badbelogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[12,logseq____&quot;^@logseq____&quot;,false,536870914]],[logseq____&quot;^15logseq____&quot;,[12,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;contentslogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[12,logseq____&quot;^11logseq____&quot;,logseq____&quot;contentslogseq____&quot;,536870918]],[logseq____&quot;^15logseq____&quot;,[12,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-90f8-4ee4-ab93-75920f9b2200logseq____&quot;,536870918]],[logseq____&quot;^15logseq____&quot;,[13,logseq____&quot;^@logseq____&quot;,false,536870914]],[logseq____&quot;^15logseq____&quot;,[13,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;waitinglogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[13,logseq____&quot;^11logseq____&quot;,logseq____&quot;WAITINGlogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[13,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u0c33d488-be0e-42d9-b2f9-cbcb6108e585logseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[14,logseq____&quot;^@logseq____&quot;,false,536870914]],[logseq____&quot;^15logseq____&quot;,[14,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;favoriteslogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[14,logseq____&quot;^11logseq____&quot;,logseq____&quot;Favoriteslogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[14,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u7fed84d6-d215-405c-820c-2b169685c548logseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[15,logseq____&quot;^@logseq____&quot;,false,536870914]],[logseq____&quot;^15logseq____&quot;,[15,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;alogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[15,logseq____&quot;^11logseq____&quot;,logseq____&quot;Alogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[15,logseq____&quot;^;logseq____&quot;,logseq____&quot;~ud4f91520-7811-431c-ac9a-c3a078348b90logseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[16,logseq____&quot;^@logseq____&quot;,false,536870914]],[logseq____&quot;^15logseq____&quot;,[16,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;cancelledlogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[16,logseq____&quot;^11logseq____&quot;,logseq____&quot;CANCELLEDlogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[16,logseq____&quot;^;logseq____&quot;,logseq____&quot;~uc74e6db3-7f9e-4935-aca1-9d70c33aea02logseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[17,logseq____&quot;^@logseq____&quot;,false,536870914]],[logseq____&quot;^15logseq____&quot;,[17,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;waitlogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[17,logseq____&quot;^11logseq____&quot;,logseq____&quot;WAITlogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[17,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u263d08f8-bc8a-465c-ae55-2ba3f2bdedfclogseq____&quot;,536870914]],[logseq____&quot;^15logseq____&quot;,[22,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;logseq____&quot;,536870918]],[logseq____&quot;^15logseq____&quot;,[22,logseq____&quot;^Ologseq____&quot;,logseq____&quot;~:markdownlogseq____&quot;,536870918]],[logseq____&quot;^15logseq____&quot;,[22,logseq____&quot;^Flogseq____&quot;,12,536870918]],[logseq____&quot;^15logseq____&quot;,[22,logseq____&quot;^Xlogseq____&quot;,12,536870918]],[logseq____&quot;^15logseq____&quot;,[22,logseq____&quot;^Vlogseq____&quot;,12,536870918]],[logseq____&quot;^15logseq____&quot;,[22,logseq____&quot;^Ulogseq____&quot;,12,536870918]],[logseq____&quot;^15logseq____&quot;,[22,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-b17f-446a-be3c-e78fc3ef3b9blogseq____&quot;,536870918]],[logseq____&quot;^15logseq____&quot;,[23,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;{{renderer :tocgen2, [[预训练技术]], calc(100vh - 135px)}}logseq____&quot;,536870918]],[logseq____&quot;^15logseq____&quot;,[23,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870918]],[logseq____&quot;^15logseq____&quot;,[23,logseq____&quot;^Flogseq____&quot;,22,536870918]],[logseq____&quot;^15logseq____&quot;,[23,logseq____&quot;^Slogseq____&quot;,24,536870918]],[logseq____&quot;^15logseq____&quot;,[23,logseq____&quot;^Xlogseq____&quot;,12,536870918]],[logseq____&quot;^15logseq____&quot;,[23,logseq____&quot;^Vlogseq____&quot;,12,536870918]],[logseq____&quot;^15logseq____&quot;,[23,logseq____&quot;^Ulogseq____&quot;,12,536870918]],[logseq____&quot;^15logseq____&quot;,[23,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-72d1-4fd1-9ac7-fd5289684d1blogseq____&quot;,536870918]],[logseq____&quot;^15logseq____&quot;,[24,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;~:logseq.macro-namelogseq____&quot;,logseq____&quot;rendererlogseq____&quot;,logseq____&quot;~:logseq.macro-argumentslogseq____&quot;,[logseq____&quot;:tocgen2logseq____&quot;,logseq____&quot;[[预训练技术]]logseq____&quot;,logseq____&quot;calc(100vh - 135px)logseq____&quot;]],536870918]],[logseq____&quot;^15logseq____&quot;,[24,logseq____&quot;^Wlogseq____&quot;,logseq____&quot;macrologseq____&quot;,536870918]],[logseq____&quot;^15logseq____&quot;,[24,logseq____&quot;^Tlogseq____&quot;,logseq____&quot;renderer :tocgen2 [[预训练技术]] calc(100vh - 135px)logseq____&quot;,536870918]],[logseq____&quot;^15logseq____&quot;,[26,logseq____&quot;^Klogseq____&quot;,1723260458360,536870919]],[logseq____&quot;^15logseq____&quot;,[26,logseq____&quot;^@logseq____&quot;,false,536870919]],[logseq____&quot;^15logseq____&quot;,[26,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;alibi, kerple and sandwichlogseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[26,logseq____&quot;^11logseq____&quot;,logseq____&quot;ALiBi, Kerple and Sandwichlogseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[26,logseq____&quot;^Blogseq____&quot;,1723260458360,536870919]],[logseq____&quot;^15logseq____&quot;,[26,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-3332-40b2-aded-77fc656596b8logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[27,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;![alibi.png](../assets/alibi_1711464949195_0.png){:height 245, :width 560}logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[27,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[27,logseq____&quot;^Flogseq____&quot;,37,536870919]],[logseq____&quot;^15logseq____&quot;,[27,logseq____&quot;^Xlogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[27,logseq____&quot;^Vlogseq____&quot;,37,536870919]],[logseq____&quot;^15logseq____&quot;,[27,logseq____&quot;^Ulogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[27,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-34ea-4371-bd74-e33a9d59d773logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[28,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;kerple 是Alibi 的简单推广，它引入了两个训练参数$$r_1, r_2$$来一般化式logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[28,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[28,logseq____&quot;^Flogseq____&quot;,36,536870919]],[logseq____&quot;^15logseq____&quot;,[28,logseq____&quot;^Xlogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[28,logseq____&quot;^Vlogseq____&quot;,36,536870919]],[logseq____&quot;^15logseq____&quot;,[28,logseq____&quot;^Ulogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[28,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-a7c0-48f8-a62e-d6e7f0e4aa43logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[29,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;[ALiBi](https://arxiv.org/pdf/2108.12409.pdf)在attention score上加入位置信息，在较短的句子上训练，在 infer 较长的句子时依然会有较好的效果logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[29,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[29,logseq____&quot;^Flogseq____&quot;,35,536870919]],[logseq____&quot;^15logseq____&quot;,[29,logseq____&quot;^Xlogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[29,logseq____&quot;^Vlogseq____&quot;,35,536870919]],[logseq____&quot;^15logseq____&quot;,[29,logseq____&quot;^Ulogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[29,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-9ca3-4d46-96d4-a46ddaf2e4bclogseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[30,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;$$\\n\\\\begin{equation}\\\\boldsymbol{q}_m^{\\\\top}\\\\boldsymbol{k}_n - \\\\lambda|m - n|\\\\end{equation}\\n$$logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[30,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[30,logseq____&quot;^Flogseq____&quot;,27,536870919]],[logseq____&quot;^15logseq____&quot;,[30,logseq____&quot;^Xlogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[30,logseq____&quot;^Vlogseq____&quot;,37,536870919]],[logseq____&quot;^15logseq____&quot;,[30,logseq____&quot;^Ulogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[30,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-dd2e-4a6d-a978-70f824f1d310logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[31,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;$$\\n\\\\begin{equation}\\\\boldsymbol{q}_m^{\\\\top}\\\\boldsymbol{k}_n + \\\\lambda\\\\boldsymbol{p}_m^{\\\\top}\\\\boldsymbol{p}_n\\\\end{equation}\\n$$logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[31,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[31,logseq____&quot;^Flogseq____&quot;,38,536870919]],[logseq____&quot;^15logseq____&quot;,[31,logseq____&quot;^Xlogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[31,logseq____&quot;^Vlogseq____&quot;,38,536870919]],[logseq____&quot;^15logseq____&quot;,[31,logseq____&quot;^Ulogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[31,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-6db9-4f6b-91be-d2d59b99b738logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[32,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;~```python\\n\\nclass AlibiPositionalBias(nn.Module):\\n    def __init__(self, heads, **kwargs):\\n        super().__init__()\\n        self.heads = heads\\n        slopes = torch.Tensor(self._get_slopes(heads))\\n        slopes = rearrange(slopes, logseq____&apos;h -logseq____&gt; h 1 1logseq____&apos;)\\n        self.register_buffer(logseq____&apos;slopeslogseq____&apos;, slopes, persistent=False)\\n        self.register_buffer(logseq____&apos;biaslogseq____&apos;, None, persistent=False)\\n\\n    def get_bias(self, i, j, device):\\n        i_arange = torch.arange(j - i, j, device=device)\\n        j_arange = torch.arange(j, device=device)\\n        bias = -torch.abs(rearrange(j_arange, logseq____&apos;j -logseq____&gt; 1 1 jlogseq____&apos;) - rearrange(i_arange, logseq____&apos;i -logseq____&gt; 1 i 1logseq____&apos;))\\n        return bias\\n\\n    @staticmethod\\n    def _get_slopes(heads):\\n        def get_slopes_power_of_2(n):\\n            start = (2 ** (-2 ** -(math.log2(n) - 3)))\\n            ratio = start\\n            return [start * ratio ** i for i in range(n)]\\n\\n        if math.log2(heads).is_integer():\\n            return get_slopes_power_of_2(heads)\\n\\n        closest_power_of_2 = 2 ** math.floor(math.log2(heads))\\n        return get_slopes_power_of_2(closest_power_of_2) + get_slopes_power_of_2(2 * closest_power_of_2)[0::2][:heads - closest_power_of_2]\\n    \\n    def forward(self, qk_dots):\\n        h, i, j, device = *qk_dots.shape[-3:], qk_dots.device\\n        if exists(self.bias) and self.bias.shape[-1] logseq____&gt;= j:\\n            return qk_dots + self.bias[..., :i, :j]\\n\\n        bias = self.get_bias(i, j, device)\\n        bias = bias * self.slopes\\n        num_heads_unalibied = h - bias.shape[0]\\n        bias = F.pad(bias, (0, 0, 0, 0, 0, num_heads_unalibied))\\n        self.register_buffer(logseq____&apos;biaslogseq____&apos;, bias, persistent=False)\\n        return qk_dots + self.bias\\n\\n```logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[32,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[32,logseq____&quot;^Flogseq____&quot;,40,536870919]],[logseq____&quot;^15logseq____&quot;,[32,logseq____&quot;^Xlogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[32,logseq____&quot;^Vlogseq____&quot;,40,536870919]],[logseq____&quot;^15logseq____&quot;,[32,logseq____&quot;^Ulogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[32,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-9d9a-439e-b138-0ac7a2977ab7logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[33,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;$\\\\boldsymbol{p}_m^{\\\\top}\\\\boldsymbol{p}_n$：logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[33,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[33,logseq____&quot;^Flogseq____&quot;,41,536870919]],[logseq____&quot;^15logseq____&quot;,[33,logseq____&quot;^Xlogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[33,logseq____&quot;^Vlogseq____&quot;,38,536870919]],[logseq____&quot;^15logseq____&quot;,[33,logseq____&quot;^Ulogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[33,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-cb12-4935-a688-9bd78b5b5f04logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[34,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;![pmpn.png](../assets/pmpn_1711465497708_0.png){:height 434, :width 607}logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[34,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[34,logseq____&quot;^Flogseq____&quot;,33,536870919]],[logseq____&quot;^15logseq____&quot;,[34,logseq____&quot;^Xlogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[34,logseq____&quot;^Vlogseq____&quot;,33,536870919]],[logseq____&quot;^15logseq____&quot;,[34,logseq____&quot;^Ulogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[34,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-f42e-4c48-92f4-482374146021logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[35,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## ALiBilogseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[35,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[35,logseq____&quot;^Flogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[35,logseq____&quot;^Xlogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[35,logseq____&quot;^Vlogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[35,logseq____&quot;^Ulogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[35,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;~:headinglogseq____&quot;,2],536870919]],[logseq____&quot;^15logseq____&quot;,[35,logseq____&quot;^Jlogseq____&quot;,[],536870919]],[logseq____&quot;^15logseq____&quot;,[35,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-df60-4857-8870-df46e3075a19logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[36,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## Kerplelogseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[36,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[36,logseq____&quot;^Flogseq____&quot;,35,536870919]],[logseq____&quot;^15logseq____&quot;,[36,logseq____&quot;^Xlogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[36,logseq____&quot;^Vlogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[36,logseq____&quot;^Ulogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[36,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870919]],[logseq____&quot;^15logseq____&quot;,[36,logseq____&quot;^Jlogseq____&quot;,[],536870919]],[logseq____&quot;^15logseq____&quot;,[36,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-075d-43a6-ad07-51473b3d25f4logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[37,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;**### Attention with Linear Biases**logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[37,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[37,logseq____&quot;^Flogseq____&quot;,29,536870919]],[logseq____&quot;^15logseq____&quot;,[37,logseq____&quot;^Xlogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[37,logseq____&quot;^Vlogseq____&quot;,35,536870919]],[logseq____&quot;^15logseq____&quot;,[37,logseq____&quot;^Ulogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[37,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-0ff9-452a-bba1-8db67309b3c6logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[38,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## Sandwichlogseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[38,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[38,logseq____&quot;^Flogseq____&quot;,36,536870919]],[logseq____&quot;^15logseq____&quot;,[38,logseq____&quot;^Xlogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[38,logseq____&quot;^Vlogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[38,logseq____&quot;^Ulogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[38,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870919]],[logseq____&quot;^15logseq____&quot;,[38,logseq____&quot;^Jlogseq____&quot;,[],536870919]],[logseq____&quot;^15logseq____&quot;,[38,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-2d3b-45bb-8982-80bef880da8flogseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[39,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;$$\\n\\n\\\\begin{equation}\\\\left\\\\{\\\\begin{aligned}logseq____&amp;\\\\boldsymbol{q}_m^{\\\\top}\\\\boldsymbol{k}_n - r_1|m - n|^{r_2} ,\\\\qquad\\\\qquad r_1 logseq____&gt;0, 0 logseq____&lt; r_2 \\\\leq 2\\\\\\\\ \\n\\nlogseq____&amp;\\\\boldsymbol{q}_m^{\\\\top}\\\\boldsymbol{k}_n - r_1\\\\log(1+r_2|m - n|),\\\\qquad\\\\qquad r_1, r_2 logseq____&gt; 0 \\n\\n\\\\end{aligned}\\\\right.\\\\end{equation}\\n\\n$$logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[39,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[39,logseq____&quot;^Flogseq____&quot;,28,536870919]],[logseq____&quot;^15logseq____&quot;,[39,logseq____&quot;^Xlogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[39,logseq____&quot;^Vlogseq____&quot;,28,536870919]],[logseq____&quot;^15logseq____&quot;,[39,logseq____&quot;^Ulogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[39,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-3228-4fdc-a13b-5a9670bde539logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[40,logseq____&quot;^Mlogseq____&quot;,true,536870919]],[logseq____&quot;^15logseq____&quot;,[40,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;### code\\ncollapsed:: truelogseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[40,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[40,logseq____&quot;^Flogseq____&quot;,37,536870919]],[logseq____&quot;^15logseq____&quot;,[40,logseq____&quot;^Xlogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[40,logseq____&quot;^Vlogseq____&quot;,35,536870919]],[logseq____&quot;^15logseq____&quot;,[40,logseq____&quot;^Ulogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[40,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,3],536870919]],[logseq____&quot;^15logseq____&quot;,[40,logseq____&quot;^Jlogseq____&quot;,[],536870919]],[logseq____&quot;^15logseq____&quot;,[40,logseq____&quot;^4logseq____&quot;,[logseq____&quot;^ logseq____&quot;],536870919]],[logseq____&quot;^15logseq____&quot;,[40,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-2989-4987-a727-dfd0ed3ed4d3logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[41,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;其中$p_m,p_n$是Sinusoidal位置编码，$\\\\lambdalogseq____&gt;0$是超参数。logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[41,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[41,logseq____&quot;^Flogseq____&quot;,31,536870919]],[logseq____&quot;^15logseq____&quot;,[41,logseq____&quot;^Xlogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[41,logseq____&quot;^Vlogseq____&quot;,38,536870919]],[logseq____&quot;^15logseq____&quot;,[41,logseq____&quot;^Ulogseq____&quot;,26,536870919]],[logseq____&quot;^15logseq____&quot;,[41,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-6186-441f-9faa-a62730a2ac1clogseq____&quot;,536870919]],[logseq____&quot;^15logseq____&quot;,[43,logseq____&quot;^Klogseq____&quot;,1723260458442,536870920]],[logseq____&quot;^15logseq____&quot;,[43,logseq____&quot;^@logseq____&quot;,false,536870920]],[logseq____&quot;^15logseq____&quot;,[43,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;finetune, adapter, prompt tuning logseq____&amp; loralogseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[43,logseq____&quot;^11logseq____&quot;,logseq____&quot;Finetune, Adapter, Prompt tuning logseq____&amp; LoRAlogseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[43,logseq____&quot;^Blogseq____&quot;,1723260458442,536870920]],[logseq____&quot;^15logseq____&quot;,[43,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-306d-4115-8416-317c6d548c6alogseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[44,logseq____&quot;^Klogseq____&quot;,1723260458444,536870920]],[logseq____&quot;^15logseq____&quot;,[44,logseq____&quot;^@logseq____&quot;,false,536870920]],[logseq____&quot;^15logseq____&quot;,[44,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;maelogseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[44,logseq____&quot;^11logseq____&quot;,logseq____&quot;MAElogseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[44,logseq____&quot;^Blogseq____&quot;,1723260458444,536870920]],[logseq____&quot;^15logseq____&quot;,[44,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-7b2e-4ba0-bfa2-878fb4ef3415logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[45,logseq____&quot;^Klogseq____&quot;,1723260458445,536870920]],[logseq____&quot;^15logseq____&quot;,[45,logseq____&quot;^@logseq____&quot;,false,536870920]],[logseq____&quot;^15logseq____&quot;,[45,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;模板学习logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[45,logseq____&quot;^11logseq____&quot;,logseq____&quot;模板学习logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[45,logseq____&quot;^Blogseq____&quot;,1723260458445,536870920]],[logseq____&quot;^15logseq____&quot;,[45,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-026c-4f8f-b711-2a7ccedc3fbblogseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[46,logseq____&quot;^Klogseq____&quot;,1723260458446,536870920]],[logseq____&quot;^15logseq____&quot;,[46,logseq____&quot;^@logseq____&quot;,false,536870920]],[logseq____&quot;^15logseq____&quot;,[46,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;loralogseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[46,logseq____&quot;^11logseq____&quot;,logseq____&quot;LoRAlogseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[46,logseq____&quot;^Blogseq____&quot;,1723260458446,536870920]],[logseq____&quot;^15logseq____&quot;,[46,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-4087-49fe-bdd6-a58acc6eb695logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[47,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## [[LoRA]]logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[47,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[47,logseq____&quot;^Flogseq____&quot;,54,536870920]],[logseq____&quot;^15logseq____&quot;,[47,logseq____&quot;^Xlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[47,logseq____&quot;^Vlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[47,logseq____&quot;^Ulogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[47,logseq____&quot;^Ulogseq____&quot;,46,536870920]],[logseq____&quot;^15logseq____&quot;,[47,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870920]],[logseq____&quot;^15logseq____&quot;,[47,logseq____&quot;^Jlogseq____&quot;,[],536870920]],[logseq____&quot;^15logseq____&quot;,[47,logseq____&quot;^Hlogseq____&quot;,46,536870920]],[logseq____&quot;^15logseq____&quot;,[47,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-266a-40fb-bf60-b548ca4a8507logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[48,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;*左图**是 Adapter-BERT 中的 transformer layer，我们可以看到每一个 transformer layer 增加了两个 Adapter layer，分别加在 LayerNorm 之前，当然了，在进行 LayerNorm 之前，我们需要进行讲 Apdater layer 的输出进行残差连接。logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[48,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[48,logseq____&quot;^Flogseq____&quot;,60,536870920]],[logseq____&quot;^15logseq____&quot;,[48,logseq____&quot;^Xlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[48,logseq____&quot;^Vlogseq____&quot;,60,536870920]],[logseq____&quot;^15logseq____&quot;,[48,logseq____&quot;^Ulogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[48,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-f327-4fb9-926f-25333e600f41logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[49,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## Finetunelogseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[49,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[49,logseq____&quot;^Flogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[49,logseq____&quot;^Xlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[49,logseq____&quot;^Vlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[49,logseq____&quot;^Ulogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[49,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870920]],[logseq____&quot;^15logseq____&quot;,[49,logseq____&quot;^Jlogseq____&quot;,[],536870920]],[logseq____&quot;^15logseq____&quot;,[49,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-1ee0-4c03-84c1-eb0f3a2eba8alogseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[50,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;Fine-tune layerNorm 没有什么用。logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[50,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[50,logseq____&quot;^Flogseq____&quot;,70,536870920]],[logseq____&quot;^15logseq____&quot;,[50,logseq____&quot;^Xlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[50,logseq____&quot;^Vlogseq____&quot;,70,536870920]],[logseq____&quot;^15logseq____&quot;,[50,logseq____&quot;^Ulogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[50,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-9fb6-4fd1-a363-c571efc7a6a9logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[51,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;从[[MAE]]可以看出，与 BERT 相似的以 embedding 为主的预模型需要finetune以提升效果；logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[51,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[51,logseq____&quot;^Flogseq____&quot;,55,536870920]],[logseq____&quot;^15logseq____&quot;,[51,logseq____&quot;^Xlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[51,logseq____&quot;^Vlogseq____&quot;,49,536870920]],[logseq____&quot;^15logseq____&quot;,[51,logseq____&quot;^Ulogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[51,logseq____&quot;^Ulogseq____&quot;,44,536870920]],[logseq____&quot;^15logseq____&quot;,[51,logseq____&quot;^Hlogseq____&quot;,44,536870920]],[logseq____&quot;^15logseq____&quot;,[51,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-9802-4ea4-9a08-97fbebfec630logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[52,logseq____&quot;^Mlogseq____&quot;,true,536870920]],[logseq____&quot;^15logseq____&quot;,[52,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;### 模型架构\\ncollapsed:: truelogseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[52,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[52,logseq____&quot;^Flogseq____&quot;,56,536870920]],[logseq____&quot;^15logseq____&quot;,[52,logseq____&quot;^Xlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[52,logseq____&quot;^Vlogseq____&quot;,53,536870920]],[logseq____&quot;^15logseq____&quot;,[52,logseq____&quot;^Ulogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[52,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,3],536870920]],[logseq____&quot;^15logseq____&quot;,[52,logseq____&quot;^Jlogseq____&quot;,[],536870920]],[logseq____&quot;^15logseq____&quot;,[52,logseq____&quot;^4logseq____&quot;,[logseq____&quot;^ logseq____&quot;],536870920]],[logseq____&quot;^15logseq____&quot;,[52,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-a925-4276-88c1-a6eda38f0a94logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[53,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## Adapterlogseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[53,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[53,logseq____&quot;^Flogseq____&quot;,49,536870920]],[logseq____&quot;^15logseq____&quot;,[53,logseq____&quot;^Xlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[53,logseq____&quot;^Vlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[53,logseq____&quot;^Ulogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[53,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870920]],[logseq____&quot;^15logseq____&quot;,[53,logseq____&quot;^Jlogseq____&quot;,[],536870920]],[logseq____&quot;^15logseq____&quot;,[53,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-a062-4532-a760-753cb76438e9logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[54,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## [[模板学习]]logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[54,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[54,logseq____&quot;^Flogseq____&quot;,53,536870920]],[logseq____&quot;^15logseq____&quot;,[54,logseq____&quot;^Xlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[54,logseq____&quot;^Vlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[54,logseq____&quot;^Ulogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[54,logseq____&quot;^Ulogseq____&quot;,45,536870920]],[logseq____&quot;^15logseq____&quot;,[54,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870920]],[logseq____&quot;^15logseq____&quot;,[54,logseq____&quot;^Jlogseq____&quot;,[],536870920]],[logseq____&quot;^15logseq____&quot;,[54,logseq____&quot;^Hlogseq____&quot;,45,536870920]],[logseq____&quot;^15logseq____&quot;,[54,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-ebcb-4212-8f06-8d9db5d06f55logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[55,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;fine-tuning 指的是首先预训练模型，然后接入下游任务，将预训练阶段的权重作为 finetuning 的初始化权重，之后将下游任务与预训练部分一起训练，侧重的是 co-training。logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[55,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[55,logseq____&quot;^Flogseq____&quot;,49,536870920]],[logseq____&quot;^15logseq____&quot;,[55,logseq____&quot;^Xlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[55,logseq____&quot;^Vlogseq____&quot;,49,536870920]],[logseq____&quot;^15logseq____&quot;,[55,logseq____&quot;^Ulogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[55,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-d63c-4a24-a783-4e4497c4fcdalogseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[56,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;Adapter-Bert 来源于 Google 的《Parameter-Efﬁcient Transfer Learning for NLP》论文，主要目的在不降低模型效果的情况下，减小 finetune 时候的参数。logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[56,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[56,logseq____&quot;^Flogseq____&quot;,53,536870920]],[logseq____&quot;^15logseq____&quot;,[56,logseq____&quot;^Xlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[56,logseq____&quot;^Vlogseq____&quot;,53,536870920]],[logseq____&quot;^15logseq____&quot;,[56,logseq____&quot;^Ulogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[56,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-0273-4ca7-82c4-ebc36a7f9e53logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[57,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;### LLM adapterlogseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[57,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[57,logseq____&quot;^Flogseq____&quot;,58,536870920]],[logseq____&quot;^15logseq____&quot;,[57,logseq____&quot;^Xlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[57,logseq____&quot;^Vlogseq____&quot;,53,536870920]],[logseq____&quot;^15logseq____&quot;,[57,logseq____&quot;^Ulogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[57,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,3],536870920]],[logseq____&quot;^15logseq____&quot;,[57,logseq____&quot;^Jlogseq____&quot;,[],536870920]],[logseq____&quot;^15logseq____&quot;,[57,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-6aba-4dd4-a906-53ed3000c120logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[58,logseq____&quot;^Mlogseq____&quot;,true,536870920]],[logseq____&quot;^15logseq____&quot;,[58,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;### 实验结果\\ncollapsed:: truelogseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[58,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[58,logseq____&quot;^Flogseq____&quot;,52,536870920]],[logseq____&quot;^15logseq____&quot;,[58,logseq____&quot;^Xlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[58,logseq____&quot;^Vlogseq____&quot;,53,536870920]],[logseq____&quot;^15logseq____&quot;,[58,logseq____&quot;^Ulogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[58,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,3],536870920]],[logseq____&quot;^15logseq____&quot;,[58,logseq____&quot;^Jlogseq____&quot;,[],536870920]],[logseq____&quot;^15logseq____&quot;,[58,logseq____&quot;^4logseq____&quot;,[logseq____&quot;^ logseq____&quot;],536870920]],[logseq____&quot;^15logseq____&quot;,[58,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-5df1-4036-b39b-0534cc55a6dflogseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[59,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;![mae_finetune.png](../assets/mae_finetune.png){:height 408, :width 548}logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[59,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[59,logseq____&quot;^Flogseq____&quot;,51,536870920]],[logseq____&quot;^15logseq____&quot;,[59,logseq____&quot;^Xlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[59,logseq____&quot;^Vlogseq____&quot;,51,536870920]],[logseq____&quot;^15logseq____&quot;,[59,logseq____&quot;^Ulogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[59,logseq____&quot;^Ulogseq____&quot;,44,536870920]],[logseq____&quot;^15logseq____&quot;,[59,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-8b16-41c5-bba4-7c3c3b734436logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[60,logseq____&quot;^Mlogseq____&quot;,true,536870920]],[logseq____&quot;^15logseq____&quot;,[60,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;![bert_adaptor.png](../assets/bert_adaptor.png){:height 555, :width 508}\\ncollapsed:: truelogseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[60,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[60,logseq____&quot;^Flogseq____&quot;,62,536870920]],[logseq____&quot;^15logseq____&quot;,[60,logseq____&quot;^Xlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[60,logseq____&quot;^Vlogseq____&quot;,52,536870920]],[logseq____&quot;^15logseq____&quot;,[60,logseq____&quot;^Ulogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[60,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;],536870920]],[logseq____&quot;^15logseq____&quot;,[60,logseq____&quot;^Jlogseq____&quot;,[],536870920]],[logseq____&quot;^15logseq____&quot;,[60,logseq____&quot;^4logseq____&quot;,[logseq____&quot;^ logseq____&quot;],536870920]],[logseq____&quot;^15logseq____&quot;,[60,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-bb1a-499d-968f-0bc7bc23d494logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[61,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;（但在NLP 预训练任务中，通常linear probing 不会用来评估预训练的结果的好坏）logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[61,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[61,logseq____&quot;^Flogseq____&quot;,59,536870920]],[logseq____&quot;^15logseq____&quot;,[61,logseq____&quot;^Xlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[61,logseq____&quot;^Vlogseq____&quot;,51,536870920]],[logseq____&quot;^15logseq____&quot;,[61,logseq____&quot;^Ulogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[61,logseq____&quot;^Ulogseq____&quot;,44,536870920]],[logseq____&quot;^15logseq____&quot;,[61,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-c66f-49a7-8645-15d6b178242clogseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[62,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;Apdater-Bert 的想法是将 task-specific layer 放在预训练模型中间，也就是加入 Adapter 结构，然后冻结住预训练模型参数，最后我们 fientuning 的时候，只更新 Apdater、layerNorm 以及与具体任务相关的 layer 的参数。具体结构图如下：logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[62,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[62,logseq____&quot;^Flogseq____&quot;,52,536870920]],[logseq____&quot;^15logseq____&quot;,[62,logseq____&quot;^Xlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[62,logseq____&quot;^Vlogseq____&quot;,52,536870920]],[logseq____&quot;^15logseq____&quot;,[62,logseq____&quot;^Ulogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[62,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-6679-4e8d-8587-fe56620b3154logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[63,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;这里为什么要用残差连接？主要是因为当初始化的时候，权重都很小，残差连接可以保证模型输出与预训练模型相同。logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[63,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[63,logseq____&quot;^Flogseq____&quot;,64,536870920]],[logseq____&quot;^15logseq____&quot;,[63,logseq____&quot;^Xlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[63,logseq____&quot;^Vlogseq____&quot;,60,536870920]],[logseq____&quot;^15logseq____&quot;,[63,logseq____&quot;^Ulogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[63,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-1874-4888-9d20-25b48c8442bdlogseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[64,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;**右图**是 Adapter layer 的具体结构示意。这个其实让我想到了 ALBERT 中的低秩因式分解。假设输入 input 的维度是 $d$ ，我们首先通过一个 FFN 让其维度变为 $m$ ，且 $mlogseq____&lt;logseq____&lt;d$ ；之后再通过一个 FFN 得到输出结果 ouput，其其维度变为 $d$ 。最后，我们进行残差连接，即 input+output 作为 Adapter layer 的输出结果。另外，分析一下 Apdater layer 的参数，对于一个 transformer layer 来说，增加的参数是 2*(2dm+d+m+2d+2d)，其中 2d 表示是 LN 的参数量，增加的参数量占总的参数量的 3%。logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[64,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[64,logseq____&quot;^Flogseq____&quot;,48,536870920]],[logseq____&quot;^15logseq____&quot;,[64,logseq____&quot;^Xlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[64,logseq____&quot;^Vlogseq____&quot;,60,536870920]],[logseq____&quot;^15logseq____&quot;,[64,logseq____&quot;^Ulogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[64,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-bf4a-4d91-a260-f08babbab33flogseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[65,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;[LLaMA-Adapter](https://github.com/ZrrSkywalker/LLaMA-Adapter)logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[65,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[65,logseq____&quot;^Flogseq____&quot;,69,536870920]],[logseq____&quot;^15logseq____&quot;,[65,logseq____&quot;^Xlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[65,logseq____&quot;^Vlogseq____&quot;,69,536870920]],[logseq____&quot;^15logseq____&quot;,[65,logseq____&quot;^Ulogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[65,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-8b6f-442d-84f3-97e00a196d82logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[66,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;对于 fine-tune 来说，减小训练的层数会大幅降低准确率；而对于 Adapter-based 来说，几乎没有什么影响；logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[66,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[66,logseq____&quot;^Flogseq____&quot;,71,536870920]],[logseq____&quot;^15logseq____&quot;,[66,logseq____&quot;^Xlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[66,logseq____&quot;^Vlogseq____&quot;,71,536870920]],[logseq____&quot;^15logseq____&quot;,[66,logseq____&quot;^Ulogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[66,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-6761-43f4-9c1c-a7e832d8b51elogseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[67,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;从结果来看基本上能接近 fine-tuning 的结果。logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[67,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[67,logseq____&quot;^Flogseq____&quot;,68,536870920]],[logseq____&quot;^15logseq____&quot;,[67,logseq____&quot;^Xlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[67,logseq____&quot;^Vlogseq____&quot;,58,536870920]],[logseq____&quot;^15logseq____&quot;,[67,logseq____&quot;^Ulogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[67,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-fa25-4ff6-b1e8-7a7dfcb0190clogseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[68,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;![bert_adaptor_results.png](../assets/bert_adaptor_results.png)logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[68,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[68,logseq____&quot;^Flogseq____&quot;,58,536870920]],[logseq____&quot;^15logseq____&quot;,[68,logseq____&quot;^Xlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[68,logseq____&quot;^Vlogseq____&quot;,58,536870920]],[logseq____&quot;^15logseq____&quot;,[68,logseq____&quot;^Ulogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[68,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-fc30-4195-b1ac-fb6ad786b6e2logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[69,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;同样使用类似 model injection 的思路进行 PEFT 的方法：logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[69,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[69,logseq____&quot;^Flogseq____&quot;,57,536870920]],[logseq____&quot;^15logseq____&quot;,[69,logseq____&quot;^Xlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[69,logseq____&quot;^Vlogseq____&quot;,57,536870920]],[logseq____&quot;^15logseq____&quot;,[69,logseq____&quot;^Ulogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[69,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-eaf7-4ac8-9141-187041a9641alogseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[70,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;![bert_adaptor_norm.png](../assets/bert_adaptor_norm.png)logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[70,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[70,logseq____&quot;^Flogseq____&quot;,71,536870920]],[logseq____&quot;^15logseq____&quot;,[70,logseq____&quot;^Xlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[70,logseq____&quot;^Vlogseq____&quot;,58,536870920]],[logseq____&quot;^15logseq____&quot;,[70,logseq____&quot;^Ulogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[70,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-1dad-4be0-86d2-8790f64d685elogseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[71,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;![bert_adaptor_ablation.png](../assets/bert_adaptor_ablation.png)logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[71,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[71,logseq____&quot;^Flogseq____&quot;,67,536870920]],[logseq____&quot;^15logseq____&quot;,[71,logseq____&quot;^Xlogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[71,logseq____&quot;^Vlogseq____&quot;,58,536870920]],[logseq____&quot;^15logseq____&quot;,[71,logseq____&quot;^Ulogseq____&quot;,43,536870920]],[logseq____&quot;^15logseq____&quot;,[71,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-20c9-477a-88fd-67f26d85866blogseq____&quot;,536870920]],[logseq____&quot;^15logseq____&quot;,[73,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;https://github.com/Guitaricet/reloralogseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[73,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[73,logseq____&quot;^Flogseq____&quot;,75,536870921]],[logseq____&quot;^15logseq____&quot;,[73,logseq____&quot;^Xlogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[73,logseq____&quot;^Vlogseq____&quot;,76,536870921]],[logseq____&quot;^15logseq____&quot;,[73,logseq____&quot;^Ulogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[73,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-a05f-4002-9872-39aeb1e1f23clogseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[74,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;**## QLoRA**logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[74,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[74,logseq____&quot;^Flogseq____&quot;,82,536870921]],[logseq____&quot;^15logseq____&quot;,[74,logseq____&quot;^Xlogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[74,logseq____&quot;^Vlogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[74,logseq____&quot;^Ulogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[74,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-f198-4e09-bd88-3c8335c6ad8flogseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[75,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;https://arxiv.org/abs/2307.05695logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[75,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[75,logseq____&quot;^Flogseq____&quot;,76,536870921]],[logseq____&quot;^15logseq____&quot;,[75,logseq____&quot;^Xlogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[75,logseq____&quot;^Vlogseq____&quot;,76,536870921]],[logseq____&quot;^15logseq____&quot;,[75,logseq____&quot;^Ulogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[75,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-26dd-43be-bbb0-8585b692d700logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[76,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;**## ReLoRA**logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[76,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[76,logseq____&quot;^Flogseq____&quot;,74,536870921]],[logseq____&quot;^15logseq____&quot;,[76,logseq____&quot;^Xlogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[76,logseq____&quot;^Vlogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[76,logseq____&quot;^Ulogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[76,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-a873-4c42-8c04-392d99541bf5logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[77,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;https://github.com/artidoro/qloralogseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[77,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[77,logseq____&quot;^Flogseq____&quot;,74,536870921]],[logseq____&quot;^15logseq____&quot;,[77,logseq____&quot;^Xlogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[77,logseq____&quot;^Vlogseq____&quot;,74,536870921]],[logseq____&quot;^15logseq____&quot;,[77,logseq____&quot;^Ulogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[77,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-d13b-4387-82fa-962efeec4a43logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[78,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;![lora_results.png](../assets/lora_results.png)logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[78,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[78,logseq____&quot;^Flogseq____&quot;,85,536870921]],[logseq____&quot;^15logseq____&quot;,[78,logseq____&quot;^Xlogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[78,logseq____&quot;^Vlogseq____&quot;,79,536870921]],[logseq____&quot;^15logseq____&quot;,[78,logseq____&quot;^Ulogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[78,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-deea-4714-8103-6e72eaa2a13elogseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[79,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;直接实现 Adaptor 需要修改基座的网络结构，而 LoRA 对于预训练的参数矩阵 $$W_0\\\\in R^{m \\\\times n}$$ ，不直接微调 $$W_0$$ ，而是对增量做低秩分解假设：logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[79,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[79,logseq____&quot;^Flogseq____&quot;,82,536870921]],[logseq____&quot;^15logseq____&quot;,[79,logseq____&quot;^Xlogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[79,logseq____&quot;^Vlogseq____&quot;,82,536870921]],[logseq____&quot;^15logseq____&quot;,[79,logseq____&quot;^Ulogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[79,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-6517-491e-b557-3c8d709ab1eelogseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[80,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;$$W = W_0 + UV, U\\\\in \\\\mathbb{R}^{m \\\\times r}, V \\\\in \\\\mathbb{R}^{r\\\\times n}$$logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[80,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[80,logseq____&quot;^Flogseq____&quot;,79,536870921]],[logseq____&quot;^15logseq____&quot;,[80,logseq____&quot;^Xlogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[80,logseq____&quot;^Vlogseq____&quot;,79,536870921]],[logseq____&quot;^15logseq____&quot;,[80,logseq____&quot;^Ulogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[80,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-d97f-4567-bd30-cd5dfd1ea3c7logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[81,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;**## LongLoRA**logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[81,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[81,logseq____&quot;^Flogseq____&quot;,76,536870921]],[logseq____&quot;^15logseq____&quot;,[81,logseq____&quot;^Xlogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[81,logseq____&quot;^Vlogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[81,logseq____&quot;^Ulogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[81,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-e452-48f3-a1e1-48a649e1a53alogseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[82,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## LoRAlogseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[82,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[82,logseq____&quot;^Flogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[82,logseq____&quot;^Xlogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[82,logseq____&quot;^Vlogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[82,logseq____&quot;^Ulogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[82,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870921]],[logseq____&quot;^15logseq____&quot;,[82,logseq____&quot;^Jlogseq____&quot;,[],536870921]],[logseq____&quot;^15logseq____&quot;,[82,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-7845-4be0-92c8-74bf4907a567logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[83,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[83,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[83,logseq____&quot;^Flogseq____&quot;,81,536870921]],[logseq____&quot;^15logseq____&quot;,[83,logseq____&quot;^Xlogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[83,logseq____&quot;^Vlogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[83,logseq____&quot;^Ulogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[83,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-2551-487b-9741-860879378c76logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[84,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;https://github.com/dvlab-research/LongLoRAlogseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[84,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[84,logseq____&quot;^Flogseq____&quot;,81,536870921]],[logseq____&quot;^15logseq____&quot;,[84,logseq____&quot;^Xlogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[84,logseq____&quot;^Vlogseq____&quot;,81,536870921]],[logseq____&quot;^15logseq____&quot;,[84,logseq____&quot;^Ulogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[84,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-16f3-4130-bf1a-0f3ab345716dlogseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[85,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;其中 $$U, V$$ 之一用全零初始化， $$W_0$$ 固定不变，优化器只优化 $$U,V$$ 。r 可以取得很小，甚至可以取 1。logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[85,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[85,logseq____&quot;^Flogseq____&quot;,80,536870921]],[logseq____&quot;^15logseq____&quot;,[85,logseq____&quot;^Xlogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[85,logseq____&quot;^Vlogseq____&quot;,79,536870921]],[logseq____&quot;^15logseq____&quot;,[85,logseq____&quot;^Ulogseq____&quot;,46,536870921]],[logseq____&quot;^15logseq____&quot;,[85,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-aed8-48ce-a10c-c823b5fee628logseq____&quot;,536870921]],[logseq____&quot;^15logseq____&quot;,[87,logseq____&quot;^Klogseq____&quot;,1723260458526,536870922]],[logseq____&quot;^15logseq____&quot;,[87,logseq____&quot;^@logseq____&quot;,false,536870922]],[logseq____&quot;^15logseq____&quot;,[87,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;multi-query attentionlogseq____&quot;,536870922]],[logseq____&quot;^15logseq____&quot;,[87,logseq____&quot;^11logseq____&quot;,logseq____&quot;Multi-Query Attentionlogseq____&quot;,536870922]],[logseq____&quot;^15logseq____&quot;,[87,logseq____&quot;^Blogseq____&quot;,1723260458526,536870922]],[logseq____&quot;^15logseq____&quot;,[87,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-457f-4956-b111-cf3e68e49ebelogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[88,logseq____&quot;^Klogseq____&quot;,1723260458527,536870922]],[logseq____&quot;^15logseq____&quot;,[88,logseq____&quot;^@logseq____&quot;,false,536870922]],[logseq____&quot;^15logseq____&quot;,[88,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;starcoderlogseq____&quot;,536870922]],[logseq____&quot;^15logseq____&quot;,[88,logseq____&quot;^11logseq____&quot;,logseq____&quot;StarCoderlogseq____&quot;,536870922]],[logseq____&quot;^15logseq____&quot;,[88,logseq____&quot;^Blogseq____&quot;,1723260458527,536870922]],[logseq____&quot;^15logseq____&quot;,[88,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-6a51-434c-bc8c-1bc3180246calogseq____&quot;,536870922]],[logseq____&quot;^15logseq____&quot;,[89,logseq____&quot;^Klogseq____&quot;,1723260458528,536870922]],[logseq____&quot;^15logseq____&quot;,[89,logseq____&quot;^@logseq____&quot;,false,536870922]],[logseq____&quot;^15logseq____&quot;,[89,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;falconlogseq____&quot;,536870922]],[logseq____&quot;^15logseq____&quot;,[89,logseq____&quot;^11logseq____&quot;,logseq____&quot;Falconlogseq____&quot;,536870922]],[logseq____&quot;^15logseq____&quot;,[89,logseq____&quot;^Blogseq____&quot;,1723260458528,536870922]],[logseq____&quot;^15logseq____&quot;,[89,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-6541-441d-9554-4fd214f8cf71logseq____&quot;,536870922]],[logseq____&quot;^15logseq____&quot;,[90,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;多查询注意力(Multi Query Attention)是多头注意力的一种变体。其主要区别在于，在多查询注意力中不同的注意力头共享一个键和值的集合，每个头只单独保留了一份查询参数。因此键和值的矩阵仅有一份，这大幅度减少了显存占用，使其更高效。logseq____&quot;,536870922]],[logseq____&quot;^15logseq____&quot;,[90,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870922]],[logseq____&quot;^15logseq____&quot;,[90,logseq____&quot;^Flogseq____&quot;,87,536870922]],[logseq____&quot;^15logseq____&quot;,[90,logseq____&quot;^Xlogseq____&quot;,87,536870922]],[logseq____&quot;^15logseq____&quot;,[90,logseq____&quot;^Vlogseq____&quot;,87,536870922]],[logseq____&quot;^15logseq____&quot;,[90,logseq____&quot;^Ulogseq____&quot;,87,536870922]],[logseq____&quot;^15logseq____&quot;,[90,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-2228-439a-9515-c4f018f05d6blogseq____&quot;,536870922]],[logseq____&quot;^15logseq____&quot;,[91,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;由于多查询注意力改变了注意力机制的结构，因此模型通常需要从训练开始就支持多查询注意力。包括 [[Falcon]], [[StarCoder]]等在内很多模型都采用了多查询注意力机制。logseq____&quot;,536870922]],[logseq____&quot;^15logseq____&quot;,[91,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870922]],[logseq____&quot;^15logseq____&quot;,[91,logseq____&quot;^Flogseq____&quot;,90,536870922]],[logseq____&quot;^15logseq____&quot;,[91,logseq____&quot;^Xlogseq____&quot;,87,536870922]],[logseq____&quot;^15logseq____&quot;,[91,logseq____&quot;^Vlogseq____&quot;,90,536870922]],[logseq____&quot;^15logseq____&quot;,[91,logseq____&quot;^Ulogseq____&quot;,87,536870922]],[logseq____&quot;^15logseq____&quot;,[91,logseq____&quot;^Ulogseq____&quot;,88,536870922]],[logseq____&quot;^15logseq____&quot;,[91,logseq____&quot;^Ulogseq____&quot;,89,536870922]],[logseq____&quot;^15logseq____&quot;,[91,logseq____&quot;^Hlogseq____&quot;,88,536870922]],[logseq____&quot;^15logseq____&quot;,[91,logseq____&quot;^Hlogseq____&quot;,89,536870922]],[logseq____&quot;^15logseq____&quot;,[91,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-00db-4715-8115-a95824cc1bdclogseq____&quot;,536870922]],[logseq____&quot;^15logseq____&quot;,[92,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;![研究结果表明](https://arxiv.org/abs/2305.13245)，可以通过对已经训练好的模型进行微调来添加多查询注意力支持，仅需要约 5% 的原始训练数据量就可以达到不错的效果。logseq____&quot;,536870922]],[logseq____&quot;^15logseq____&quot;,[92,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870922]],[logseq____&quot;^15logseq____&quot;,[92,logseq____&quot;^Flogseq____&quot;,91,536870922]],[logseq____&quot;^15logseq____&quot;,[92,logseq____&quot;^Xlogseq____&quot;,87,536870922]],[logseq____&quot;^15logseq____&quot;,[92,logseq____&quot;^Vlogseq____&quot;,90,536870922]],[logseq____&quot;^15logseq____&quot;,[92,logseq____&quot;^Ulogseq____&quot;,87,536870922]],[logseq____&quot;^15logseq____&quot;,[92,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-4d52-4d48-8bb4-62a10afab545logseq____&quot;,536870922]],[logseq____&quot;^15logseq____&quot;,[93,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;~```python\\n# Multi Head Attention\\n# Multi-Head Attention 的创建方法\\n# 查询、键和值 3 个矩阵, 所以是 3 * d_model\\n# 每个 tensor 都是 (1, 512, 768)\\n\\nself.Wqkv = nn.Linear(self.d_model, 3 * self.d_model, device=device)\\nquery, key, value = qkv.chunk(3, dim=2)\\n# Multi Query Attention\\n# Multi-Query Attention 的创建方法\\n# 只创建查询的头向量，所以是 1* d_model\\n# 而键和值不再具备单独的头向量\\n        # query -logseq____&gt; (1, 512, 768)\\n        # key   -logseq____&gt; (1, 512, 96）\\n        # value -logseq____&gt; (1, 512, 96)\\nself.Wqkv = nn.Linear(d_model, d_model + 2 * self.head_dim, device=device,)、\\nquery, key, value = qkv.split(\\n    [self.d_model, self.head_dim, self.head_dim],\\n    dim=2\\n)\\n```logseq____&quot;,536870922]],[logseq____&quot;^15logseq____&quot;,[93,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870922]],[logseq____&quot;^15logseq____&quot;,[93,logseq____&quot;^Flogseq____&quot;,92,536870922]],[logseq____&quot;^15logseq____&quot;,[93,logseq____&quot;^Xlogseq____&quot;,87,536870922]],[logseq____&quot;^15logseq____&quot;,[93,logseq____&quot;^Vlogseq____&quot;,90,536870922]],[logseq____&quot;^15logseq____&quot;,[93,logseq____&quot;^Ulogseq____&quot;,87,536870922]],[logseq____&quot;^15logseq____&quot;,[93,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-fc21-47df-9630-635757535bdclogseq____&quot;,536870922]],[logseq____&quot;^15logseq____&quot;,[95,logseq____&quot;^Klogseq____&quot;,1723260458610,536870923]],[logseq____&quot;^15logseq____&quot;,[95,logseq____&quot;^@logseq____&quot;,false,536870923]],[logseq____&quot;^15logseq____&quot;,[95,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;rlhf中的ppo算法笔记logseq____&quot;,536870923]],[logseq____&quot;^15logseq____&quot;,[95,logseq____&quot;^11logseq____&quot;,logseq____&quot;RLHF中的PPO算法笔记logseq____&quot;,536870923]],[logseq____&quot;^15logseq____&quot;,[95,logseq____&quot;^Blogseq____&quot;,1723260458610,536870923]],[logseq____&quot;^15logseq____&quot;,[95,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-600e-4cf8-a622-8f2f3e03ec51logseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[96,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;# 语言模型中的强化学习\\n**环境**：在这个情境下，强化学习的“环境”是人类的互动，LLM 根据与人类的对话（即对话历史）来决定下一步该生成什么内容。\\n**策略**：接受提示并返回文本序列的语言模型；\\n**行动**：在每个时间步，LLM 会根据对话历史和当前状态生成下一个token\\n**奖励**：环境会根据从人类偏好数据训练的奖励函数为每个动作给予一个奖励\\n**目标**：强化学习的目标是找到一种策略，使得 LLM 在整个对话中能够获得最大的累积奖励\\n![rlhf.png](../assets/rlhf_ppo1.png){:height 295, :width 400}logseq____&quot;,536870923]],[logseq____&quot;^15logseq____&quot;,[96,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870923]],[logseq____&quot;^15logseq____&quot;,[96,logseq____&quot;^Flogseq____&quot;,95,536870923]],[logseq____&quot;^15logseq____&quot;,[96,logseq____&quot;^Xlogseq____&quot;,95,536870923]],[logseq____&quot;^15logseq____&quot;,[96,logseq____&quot;^Vlogseq____&quot;,95,536870923]],[logseq____&quot;^15logseq____&quot;,[96,logseq____&quot;^Ulogseq____&quot;,95,536870923]],[logseq____&quot;^15logseq____&quot;,[96,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,1],536870923]],[logseq____&quot;^15logseq____&quot;,[96,logseq____&quot;^Jlogseq____&quot;,[],536870923]],[logseq____&quot;^15logseq____&quot;,[96,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-dd06-4f52-8b1b-346b87180b7alogseq____&quot;,536870923]],[logseq____&quot;^15logseq____&quot;,[97,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;# 策略梯度\\n策略梯度算法是一种直接优化 agent 策略的强化学习算法，它直接优化从状态到行动的策略，而不是像值函数方法一样学习一个将状态映射到预期累积奖励的函数。策略梯度算法的核心思想是利用梯度上升算法来最优化策略，通过调整策略的参数来最大化回报的期望。一般来说，策略 $\\\\pi$ 会被参数化，用  $\\\\pi(a|s, \\\\theta)$ 来表示，表示的是在 s 状态下采取行动 a 的概率。那么梯度上升方法更新的方式为：\\n$$\\\\theta \\\\leftarrow \\\\theta + \\\\alpha \\\\nabla_\\\\theta J(\\\\theta)$$其中 $\\\\alpha$ 为学习率，而 $J(\\\\theta)$表示在使用策略 $\\\\pi_\\\\theta$ 期望的回报。$\\\\nabla_\\\\theta J(\\\\theta)$ 被称为策略梯度。\\n策略梯度的通用形式为：\\n$$\\\\nabla_\\\\theta J(\\\\theta) = \\\\mathbb{E}_{\\\\tau \\\\sim \\\\pi_\\\\theta} \\\\left[ \\\\sum_{t=0}^T \\\\nabla_\\\\theta \\\\log \\\\pi_\\\\theta(a_t|s_t) \\\\Phi_t \\\\right]$$\\n其中 $\\\\Phi_t$ 可以是 $\\\\Phi_t=R(\\\\tau)$ 或者 $\\\\Phi_t = \\\\sum_{tlogseq____&apos;=t}^T R(s_{tlogseq____&apos;}, a_{tlogseq____&apos;})$ 或者 $\\\\Phi_t = \\\\sum_{tlogseq____&apos;=t}^T R(s_{tlogseq____&apos;}, a_{tlogseq____&apos;}) - b(s_t)$ ，这些选择都会得到相同的期望值，尽管有不同的方差。\\n通常来说，回报是使用蒙特卡洛采样来计算的，如果回报是好的，那么所有的行为都会通过增加他们的概率而被强化。这种方式是无偏差的，因为我们依靠的是真实的获得的回报，而不是去估计它。但是，这样做具有很高的方差。方差的来源是不同的行动的轨迹可能导致非常多样的回报，因为环境的随机性和策略本身。\\n为了降低这种方差，一种通用的方法是在策略梯度更新的过程中，使用优势函数估计，而不直接使用原始的回报。优势函数$A(s_t, a_t)$ 代表在状态  $s_t$ 下特定的行动 $a_t$ 时，与在相同策略条件下的平均行动相比，有多好。\\n$$\\\\Phi_t=A(s_t, a_t)$$\\n可以通过 $A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$ 计算，其中 $Q(s_t, a_t)$ 是行动-值函数，表示的是在 $s_t$ 状态下采取行为 $a_t$ 后的平均回报，而 $V(s_t)$ 表示的是值函数，即是在 $s_t$ 状态下的平均期望回报。\\n优势函数的估计方法在不同的算法中是不同的，而 Generalized Advantage Estimation (GAE) 是一种常用的算法。logseq____&quot;,536870923]],[logseq____&quot;^15logseq____&quot;,[97,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870923]],[logseq____&quot;^15logseq____&quot;,[97,logseq____&quot;^Flogseq____&quot;,96,536870923]],[logseq____&quot;^15logseq____&quot;,[97,logseq____&quot;^Xlogseq____&quot;,95,536870923]],[logseq____&quot;^15logseq____&quot;,[97,logseq____&quot;^Vlogseq____&quot;,95,536870923]],[logseq____&quot;^15logseq____&quot;,[97,logseq____&quot;^Ulogseq____&quot;,95,536870923]],[logseq____&quot;^15logseq____&quot;,[97,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,1],536870923]],[logseq____&quot;^15logseq____&quot;,[97,logseq____&quot;^Jlogseq____&quot;,[],536870923]],[logseq____&quot;^15logseq____&quot;,[97,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-c43b-4e90-a0ef-b535f41a9db6logseq____&quot;,536870923]],[logseq____&quot;^15logseq____&quot;,[98,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;# 广义优势估计 GAE \\n优势函数 A 是通过 Q 函数与价值函数之间的差值得到的。Q 函数考虑的是一个特定的动作，而价值函数则是根据策略对所有可能的行动进行平均。然而，在实际应用中，我们使用实际情节中的回报（奖励的总和）来估计 Q 函数。由于未来的回报可能会非常不稳定，这会引入大量的方差。减少这种噪音的一种方法是使用价值函数来估计未来的回报（从时间步 t 开始之后）。GAE 算法实际上是在简单的一步时序差分（TD）回报和完整的蒙特卡罗回报之间平衡偏差和方差。以下是对 GAE 推导过程的通俗解释。\\n$TD-k$ 回报 $\\\\hat{R}^k_t$ 是实际奖励和估计回报的组合：\\n$$\\\\hat{R}^k_t = r_t + \\\\gamma r_{t+1} + ... + \\\\gamma^{(k-1)}r_{t+k-1} + \\\\gamma^k V(s_{t+k})$$\\n其中 $\\\\gamma$ 是折扣因子。使用 TD-k 回报的优势估计被称为 k 步优势，其定义为：\\n$$A^k_t = \\\\hat{R}^k_t - V(s_t) = \\\\sum^k_{i=1} \\\\gamma^i \\\\delta_{t+i} = -V(s_t) + r_t + \\\\gamma r_{t+1} + ... + \\\\gamma^{(k-1)}r_{t+k-1} + \\\\gamma^k V(s_{t+k})$$\\n其中 $\\\\delta_t = r_t + \\\\gamma V(s_{t+1}) - V(s_t)$ 是差分误差。使用 k 步优势存在显著的偏差-方差权衡。如果 k 较小，偏差会较高，因为优势估计是基于更少的步骤，因此严重依赖于价值函数的准确性。另一方面，如果 k 较大，方差可能较高，因为优势估计涉及汇总许多噪声奖励。所以在较为稳定的环境中，可以选择较大的 k 值，以利用更多的回报信息，减少偏差。在较为不稳定或高噪声的环境中，可以选择较小的 k 值，以减少方差，增强稳定性。\\n为了平衡优势估计中的偏差-方差权衡，GAE 将优势函数定义为 k 步优势的指数移动平均，权重为$(1-\\\\lambda)\\\\lambda^{(k-1)}$:\\n$$\\\\begin{aligned} \\nA^{GAE}(\\\\gamma,\\\\lambda)_t logseq____&amp;= (1 - \\\\lambda)(A^{(1)}_t + \\\\lambda A^{(2)}_t + \\\\lambda^2 A^{(3)}_t + \\\\cdots) \\\\\\\\ \\nlogseq____&amp;= (1 - \\\\lambda)(\\\\delta_t + \\\\lambda(\\\\delta_t + \\\\gamma\\\\delta_{t+1}) + \\\\lambda^2(\\\\delta_t + \\\\gamma\\\\delta_{t+1} + \\\\gamma^2\\\\delta_{t+2}) + \\\\cdots)\\\\\\\\\\nlogseq____&amp;= (1 - \\\\lambda)(\\\\delta_t(1 + \\\\lambda + \\\\lambda^2 + ...) + \\\\gamma\\\\delta_{t+1}(\\\\lambda + \\\\lambda^2 + \\\\lambda^3 + \\\\cdots) + \\\\gamma^2\\\\delta_{t+2}(\\\\lambda^2 + \\\\lambda^3 + \\\\lambda^4 + \\\\cdots) + \\\\cdots)\\\\\\\\\\nlogseq____&amp;= (1 - \\\\lambda)\\\\left(\\\\frac{\\\\delta_t}{1-\\\\lambda} + \\\\frac{\\\\gamma\\\\delta_{t+1}}{1-\\\\lambda} + \\\\frac{\\\\gamma^2\\\\delta_{t+2}}{1-\\\\lambda} + \\\\cdots \\\\right)\\\\\\\\\\nlogseq____&amp;= \\\\sum_{i=0}^{\\\\infty}(\\\\gamma\\\\lambda)^i\\\\delta_{t+i}\\n\\\\end{aligned}\\n$$\\nGAE 的定义平滑地在高偏差（$\\\\lambda = 0$）与高方差($\\\\lambda=1$)差值，很有效地管理了偏差和方差的 trade-off。\\n\\n$$GAE(\\\\gamma,0) : \\\\hat{A}_t = \\\\delta_t = r_t + \\\\gamma V(s_{t+1}) - V(s_t)$$\\n$$GAE(\\\\gamma,1) : \\\\hat{A}_t = \\\\sum_{i=0}^{\\\\infty}\\\\gamma^i\\\\delta_{t+i} = \\\\sum_{i=0}^{\\\\infty}\\\\gamma^i r_{t+i} - V(s_t)$$\\n通过GAE，我们可以准确地估计优势函数 $A(s_t, a_t)$ 的 $\\\\hat{A}_t$。这个估计在构建策略梯度估计器时将起到至关重要的作用:\\n$$\\n\\\\begin{aligned}\\n\\\\nabla_{\\\\theta}J(\\\\theta) logseq____&amp;= \\\\frac{1}{|D|}\\\\sum_{\\\\tau\\\\in D}\\\\sum_{t=1}^{T}\\\\nabla_{\\\\theta}\\\\log\\\\pi_{\\\\theta}(a_t|s_t)\\\\hat{A}_t \\\\\\\\\\nlogseq____&amp; = \\\\hat{\\\\mathbb{E}}_{t} \\\\nabla_{\\\\theta}\\\\log\\\\pi_{\\\\theta}(a_t|s_t)\\\\hat{A}_t\\n\\\\end{aligned}$$\\n其中 D 是一个有限的样本集。\\n在`trl`包中用简单的几行代码实现了GAE，通过循环和递归方式计算，最终得到每个时间步的优势估计：\\n\\n```python\\ndef compute_advantages(  \\n  self,  \\n  values: torch.FloatTensor,  \\n  rewards: torch.FloatTensor,  \\n  mask: torch.FloatTensor,  \\n):   \\n  lastgaelam = 0  # 初始化最后一步的广义优势估计量（GAE）\\n  advantages_reversed = []  # 用于存储反转后的优势估计\\n  gen_len = rewards.shape[-1]  # 序列长度\\n  \\n  # 应用掩码处理值和奖励\\n  values = values * mask  \\n  rewards = rewards * mask  \\n  if self.config.whiten_rewards:  \\n      rewards = masked_whiten(rewards, mask, shift_mean=False)  \\n  \\n  # 反向遍历每个时间步计算优势\\n  for t in reversed(range(gen_len)):  \\n      # 计算下一个时间步的值\\n      nextvalues = values[:, t + 1] if t logseq____&lt; gen_len - 1 else 0.0  \\n      # 计算时序差分误差 \\n      delta = rewards[:, t] + self.config.gamma * nextvalues - values[:, t]  \\n      # 计算当前时间步的广义优势估计量\\n      lastgaelam = delta + self.config.gamma * self.config.lam * lastgaelam  \\n      # 将计算的优势添加到列表中\\n      advantages_reversed.append(lastgaelam)  \\n  \\n  # 将反转的优势列表恢复正常顺序，并转置为原来的形状\\n  advantages = torch.stack(advantages_reversed[::-1]).transpose(0, 1)  \\n  \\n  # 计算回报\\n  returns = advantages + values  \\n  advantages = masked_whiten(advantages, mask)  \\n  advantages = advantages.detach()  \\n  return values, advantages, returns\\n```logseq____&quot;,536870923]],[logseq____&quot;^15logseq____&quot;,[98,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870923]],[logseq____&quot;^15logseq____&quot;,[98,logseq____&quot;^Flogseq____&quot;,97,536870923]],[logseq____&quot;^15logseq____&quot;,[98,logseq____&quot;^Xlogseq____&quot;,95,536870923]],[logseq____&quot;^15logseq____&quot;,[98,logseq____&quot;^Vlogseq____&quot;,95,536870923]],[logseq____&quot;^15logseq____&quot;,[98,logseq____&quot;^Ulogseq____&quot;,95,536870923]],[logseq____&quot;^15logseq____&quot;,[98,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,1],536870923]],[logseq____&quot;^15logseq____&quot;,[98,logseq____&quot;^Jlogseq____&quot;,[],536870923]],[logseq____&quot;^15logseq____&quot;,[98,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-ae70-40cb-a0ce-ac7484703698logseq____&quot;,536870923]],[logseq____&quot;^15logseq____&quot;,[99,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;# 近端策略优化\\nPPO 和 TRPO 是强化学习（RL）中的两项关键技术，它们能够有效地训练策略，并保持其稳定性。这两个方法的基本理念是“稳步、小步前进”：即通过轻推策略逐步优化，而不是采用激进的更新方式，这种方式可能会破坏整个学习过程的稳定性。\\n在传统的强化学习中，策略梯度的原则要求新旧策略在参数空间内保持接近。然而，这种在参数空间内的接近并不一定意味着性能上的相似，参数的微小变化可能会显著影响策略的有效性。此外，如果进行一个大幅度且不受限制的步骤，可能会导致策略性能的崩溃，这种情况通常被称为“falling off the cliff”。这一固有风险限制了原始策略梯度的样本效率。\\nTRPO 没有通过参数空间接近，而是在策略更新中通过引入另一种不同的限制，它通过把策略的变化通过 KL 散度进行限制，保留了一个可以接受的限制：\\n$$\\\\text{maximize}_{\\\\theta} \\\\, \\\\mathbb{E}_t \\\\left[ \\\\frac{\\\\pi_{\\\\theta}(a_t|s_t)}{\\\\pi_{\\\\theta_{\\\\text{old}}}(a_t|s_t)} \\\\hat{A}_t \\\\right] \\\\ \\n\\\\text{subject to}\\\\ \\\\mathbb{E}_t \\\\left[ \\\\text{KL}(\\\\pi_{\\\\theta_{\\\\text{old}}}(\\\\cdot|s_t), \\\\pi_{\\\\theta}(\\\\cdot|s_t)) \\\\right] \\\\leq \\\\delta$$\\n\\n其中 $\\\\theta_{old}$ 是更新之前的老的策略参数。\\nTRPO 将 KL 散度作为硬性限制来阻止策略的有害的更新，而 PPO 有两种基础的变形来防止有害的策略更新：PPO-Penalty 和 PPO-Clip，其中 PPO-Penalty 通过采用基于惩罚的方法而不是约束来解决无约束优化问题：\\n$$\\\\mathcal{L}_{\\\\text{ppo-penalty}}(\\\\theta) = \\\\mathbb{E}_t \\\\left[ \\\\frac{\\\\pi_{\\\\theta}(a_t|s_t)}{\\\\pi_{\\\\theta_{\\\\text{old}}}(a_t|s_t)} \\\\hat{A}_t \\\\right] - \\\\beta \\\\text{KL}(\\\\pi_{\\\\theta_{\\\\text{old}}}(\\\\cdot|s_t), \\\\pi_{\\\\theta}(\\\\cdot|s_t))$$\\n其中 $\\\\beta$ 是惩罚因子。\\n\\n**PPO-Clip**: PPO-Clip 在目标函数中使用了策略比率的裁剪版本，其目标函数可以被表示为:\\n$$   \\\\mathcal{L}_{\\\\text{ppo-clip}}(\\\\theta) = \\\\mathbb{E}_t \\\\left[ \\\\min \\\\left( \\\\frac{\\\\pi_{\\\\theta}(a_t|s_t)}{\\\\pi_{\\\\theta_{\\\\text{old}}}(a_t|s_t)} \\\\hat{A}_t, \\\\, \\\\text{clip} \\\\left( \\\\frac{\\\\pi_{\\\\theta}(a_t|s_t)}{\\\\pi_{\\\\theta_{\\\\text{old}}}(a_t|s_t)}, 1 - \\\\epsilon, 1 + \\\\epsilon \\\\right) \\\\hat{A}_t \\\\right) \\\\right]$$\\n其中 $\\\\frac{\\\\pi_{\\\\theta}(a_t|s_t)}{\\\\pi_{\\\\theta_{\\\\text{old}}}(a_t|s_t)}$ 是新策略与旧策略的概率之比，clip 函数把这个值限制在一个区间内，$\\\\epsilon$ 是一个超参数，用来决定新策略与旧策略之间可以偏离的程度。裁剪起到正则化的作用，限制策略在每次迭代中发生剧烈变化的程度。防止过大的策略更新可以确保学习过程的稳健性，同时比普通策略梯度方法保持更高的样本效率。\\n\\n**值函数估计**：在 PPO 算法中，评论家模型（通常称为值函数）估计每个状态的预期回报。该模型的学习目标是最小化其预测值与实际回报值之间的差异。评论家模型的损失函数通常使用均方误差（MSE）定义，具体公式如下：\\n$$   \\\\mathcal{L}_{\\\\text{critic}}(\\\\phi) = \\\\mathbb{E}_t \\\\left[ \\\\left( V_{\\\\phi}(s_t) - \\\\hat{R}_t \\\\right)^2 \\\\right]$$\\n其中 $V_\\\\phi({S_{t}})$ 表示的是评论家模型用参数 $\\\\phi$  在状态 $s_{t}$ 下的预测的值，而 $\\\\hat{R}_t$ 则表示在状态 $s_{t}$ 下真正的回报的值。\\n\\n在 `trl` 中，这部分两部分的 Loss 代码如下：\\n\\n```python\\ndef loss(  \\n  self,  \\n  old_logprobs: torch.FloatTensor,  \\n  values: torch.FloatTensor,  \\n  logits: torch.FloatTensor,  \\n  vpreds: torch.FloatTensor,  \\n  logprobs: torch.FloatTensor,  \\n  mask: torch.LongTensor,  \\n  advantages: torch.FloatTensor,  \\n  returns: torch.FloatTensor,  \\n):   \\n  \\n  vpredclipped = clip_by_value(  \\n      vpreds,  \\n      values - self.config.cliprange_value,  \\n      values + self.config.cliprange_value,  \\n  )  \\n  # 计算评论家模型的Loss，对应公式L_critic\\n  vf_losses1 = (vpreds - returns) ** 2  \\n  vf_losses2 = (vpredclipped - returns) ** 2  \\n  vf_loss = 0.5 * masked_mean(torch.max(vf_losses1, vf_losses2), mask)  \\n  vf_clipfrac = masked_mean(torch.gt(vf_losses2, vf_losses1).float(), mask)  \\n  \\n  # 计算 ppo-clip\\n  ratio = torch.exp(logprobs - old_logprobs)  \\n  pg_losses = -advantages * ratio  \\n  pg_losses2 = -advantages * torch.clamp(ratio, 1.0 - self.config.cliprange, 1.0 + self.config.cliprange)  \\n  pg_loss = masked_mean(torch.max(pg_losses, pg_losses2), mask)  \\n  \\n  pg_clipfrac = masked_mean(torch.gt(pg_losses2, pg_losses).float(), mask)  \\n\\n  loss = pg_loss + self.config.vf_coef * vf_loss  \\n\\n  avg_ratio = masked_mean(ratio, mask).item() \\n\\n  # KL 不能过大\\n  if avg_ratio logseq____&gt; self.config.ratio_threshold:  \\n      warnings.warn(  \\n          f\\logseq____&quot;The average ratio of batch ({avg_ratio:.2f}) exceeds threshold {self.config.ratio_threshold:.2f}. Skipping batch.\\logseq____&quot;  \\n      )  \\n      pg_loss = pg_loss * 0.0  \\n      vf_loss = vf_loss * 0.0  \\n      loss = loss * 0.0  \\n\\n  entropy = masked_mean(entropy_from_logits(logits), mask)  \\n\\n  approxkl = 0.5 * masked_mean((logprobs - old_logprobs) ** 2, mask)  \\n  # policykl的作用：早停：如果策略的 KL 大于目标 KL，则将梯度置零，并跳过优化步骤。\\n  policykl = masked_mean(old_logprobs - logprobs, mask)  \\n\\n  return_mean, return_var = masked_mean(returns, mask), masked_var(returns, mask)  \\n  value_mean, value_var = masked_mean(values, mask), masked_var(values, mask)  \\n\\n  stats = dict(  \\n      loss=dict(policy=pg_loss.detach(), value=vf_loss.detach(), total=loss.detach()),  \\n      policy=dict(  \\n          entropy=entropy.detach(),  \\n          approxkl=approxkl.detach(),  \\n          policykl=policykl.detach(),  \\n          clipfrac=pg_clipfrac.detach(),  \\n          advantages=advantages.detach(),  \\n          advantages_mean=masked_mean(advantages, mask).detach(),  \\n          ratio=ratio.detach(),  \\n      ),  \\n      returns=dict(mean=return_mean.detach(), var=return_var.detach()),  \\n      val=dict(  \\n          vpred=masked_mean(vpreds, mask).detach(),  \\n          error=masked_mean((vpreds - returns) ** 2, mask).detach(),  \\n          clipfrac=vf_clipfrac.detach(),  \\n          mean=value_mean.detach(),  \\n          var=value_var.detach(),  \\n      ),  \\n  )  \\n  return pg_loss, self.config.vf_coef * vf_loss, flatten_dict(stats)\\n```logseq____&quot;,536870923]],[logseq____&quot;^15logseq____&quot;,[99,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870923]],[logseq____&quot;^15logseq____&quot;,[99,logseq____&quot;^Flogseq____&quot;,98,536870923]],[logseq____&quot;^15logseq____&quot;,[99,logseq____&quot;^Xlogseq____&quot;,95,536870923]],[logseq____&quot;^15logseq____&quot;,[99,logseq____&quot;^Vlogseq____&quot;,95,536870923]],[logseq____&quot;^15logseq____&quot;,[99,logseq____&quot;^Ulogseq____&quot;,95,536870923]],[logseq____&quot;^15logseq____&quot;,[99,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,1],536870923]],[logseq____&quot;^15logseq____&quot;,[99,logseq____&quot;^Jlogseq____&quot;,[],536870923]],[logseq____&quot;^15logseq____&quot;,[99,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-e7aa-414d-8cd3-69aaf82c1e7alogseq____&quot;,536870923]],[logseq____&quot;^15logseq____&quot;,[100,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;# 值函数建模\\n值函数 $V_\\\\phi({S_{t}})$ 的建模的做法是在 LLM 上使用一个 `ValueHead` 网络来预测当前步的期望回报。如下面的代码所示：\\n\\n```python\\nclass ValueHead(nn.Module):  \\n  r\\logseq____&quot;\\logseq____&quot;\\logseq____&quot;  \\n  The ValueHead class implements a head for GPT2 that returns a scalar for each output token.    \\logseq____&quot;\\logseq____&quot;\\logseq____&quot;  \\n  def __init__(self, config, **kwargs):  \\n      super().__init__()  \\n      if not hasattr(config, \\logseq____&quot;summary_dropout_prob\\logseq____&quot;):  \\n          summary_dropout_prob = kwargs.pop(\\logseq____&quot;summary_dropout_prob\\logseq____&quot;, 0.1)  \\n      else:  \\n          summary_dropout_prob = config.summary_dropout_prob  \\n\\n      self.dropout = nn.Dropout(summary_dropout_prob) if summary_dropout_prob else nn.Identity()  \\n\\n      # some models such as OPT have a projection layer before the word embeddings - e.g. OPT-350m  \\n      if hasattr(config, \\logseq____&quot;hidden_size\\logseq____&quot;):  \\n          hidden_size = config.hidden_size  \\n      if hasattr(config, \\logseq____&quot;word_embed_proj_dim\\logseq____&quot;):  \\n          hidden_size = config.word_embed_proj_dim  \\n      elif hasattr(config, \\logseq____&quot;is_encoder_decoder\\logseq____&quot;):  \\n          if config.is_encoder_decoder and hasattr(config, \\logseq____&quot;decoder\\logseq____&quot;):  \\n              if hasattr(config.decoder, \\logseq____&quot;hidden_size\\logseq____&quot;):  \\n                  hidden_size = config.decoder.hidden_size  \\n\\n      self.summary = nn.Linear(hidden_size, 1)  \\n\\n      self.flatten = nn.Flatten()  \\n\\n  def forward(self, hidden_states):  \\n      output = self.dropout(hidden_states)  \\n\\n      # For now force upcast in fp32 if needed. Letlogseq____&apos;s keep the  \\n      # output in fp32 for numerical stability.        if output.dtype != self.summary.weight.dtype:  \\n          output = output.to(self.summary.weight.dtype)  \\n\\n      output = self.summary(output)  \\n      return output\\n```logseq____&quot;,536870923]],[logseq____&quot;^15logseq____&quot;,[100,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870923]],[logseq____&quot;^15logseq____&quot;,[100,logseq____&quot;^Flogseq____&quot;,99,536870923]],[logseq____&quot;^15logseq____&quot;,[100,logseq____&quot;^Xlogseq____&quot;,95,536870923]],[logseq____&quot;^15logseq____&quot;,[100,logseq____&quot;^Vlogseq____&quot;,95,536870923]],[logseq____&quot;^15logseq____&quot;,[100,logseq____&quot;^Ulogseq____&quot;,95,536870923]],[logseq____&quot;^15logseq____&quot;,[100,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,1],536870923]],[logseq____&quot;^15logseq____&quot;,[100,logseq____&quot;^Jlogseq____&quot;,[],536870923]],[logseq____&quot;^15logseq____&quot;,[100,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-37b7-4f01-b62d-99fc8d1d91d4logseq____&quot;,536870923]],[logseq____&quot;^15logseq____&quot;,[101,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;# 算法流程\\n---\\n**输入：** 初始策略参数 $\\\\theta_0$，初始价值函数参数 $\\\\phi_0$。\\n**for** \\\\(n = 0, 1, 2, \\\\dots\\\\) **do**\\n收集一组轨迹 $D_n = \\\\{\\\\tau_i\\\\}$，通过在环境中执行策略 $\\\\pi(\\\\theta_n)$。\\n计算回报 $\\\\hat{R}_t$。\\n基于当前价值函数 $V_{\\\\phi_n}$，使用广义优势估计方法计算优势估计$\\\\hat{A}_t$。\\n通过最大化 PPO-penalty/clip 目标函数来更新策略：$$\\\\theta_{n+1} = \\\\arg \\\\max_{\\\\theta} \\\\mathcal{L}_{\\\\text{ppo-clip}}(\\\\theta_n)$$\\n  通过最小化均方误差更新价值函数：$$\\\\phi_{n+1} = \\\\arg \\\\min_{\\\\phi}\\\\mathcal{L}_{\\\\text{critic}}(\\\\phi_n)$$\\n**end for**\\n---\\n\\n上面的算法在`trl`包中的实现如下，省略了多余的代码，并加了一些必要的注释：\\n\\n```python\\n# 计算 LLM 得到当前策略下的语言模型的概率、预测的回报值 \\nwith torch.no_grad():  \\n  all_logprobs, logits_or_none, values, masks = self.batched_forward_pass(  \\n      self.model,  \\n      queries,  \\n      responses,  \\n      model_inputs,  \\n      response_masks=response_masks,  \\n      return_logits=full_kl_penalty,  \\n  )  \\n  with self.optional_peft_ctx():  \\n      ref_logprobs, ref_logits_or_none, _, _ = self.batched_forward_pass(  \\n          self.model if self.is_peft_model else self.ref_model,  \\n          queries,  \\n          responses,  \\n          model_inputs,  \\n          return_logits=full_kl_penalty,  \\n      )  \\n\\ntiming[\\logseq____&quot;time/ppo/forward_pass\\logseq____&quot;] = time.time() - t  \\n\\n# 计算奖励，从reward model 中进行计算\\nwith torch.no_grad():  \\n  t = time.time()  \\n  if full_kl_penalty:  \\n      active_full_logprobs = logprobs_from_logits(logits_or_none, None, gather=False)  \\n      ref_full_logprobs = logprobs_from_logits(ref_logits_or_none, None, gather=False)  \\n      rewards, non_score_reward, kls = self.compute_rewards(  \\n          scores, active_full_logprobs, ref_full_logprobs, masks  \\n      )  \\n  else:  \\n      rewards, non_score_reward, kls = self.compute_rewards(scores, all_logprobs, ref_logprobs, masks)  \\n  timing[\\logseq____&quot;time/ppo/compute_rewards\\logseq____&quot;] = time.time() - t  \\n  # 计算优势与回报\\n  t = time.time()  \\n  values, advantages, returns = self.compute_advantages(values, rewards, masks)  \\n  timing[\\logseq____&quot;time/ppo/compute_advantages\\logseq____&quot;] = time.time() - t  \\n\\n# upcast to float32 to avoid dataset issues  \\nbatch_dict = {  \\n  \\logseq____&quot;queries\\logseq____&quot;: queries,  \\n  \\logseq____&quot;responses\\logseq____&quot;: responses,  \\n  \\logseq____&quot;logprobs\\logseq____&quot;: all_logprobs.to(torch.float32),  \\n  \\logseq____&quot;values\\logseq____&quot;: values.to(torch.float32),  \\n  \\logseq____&quot;masks\\logseq____&quot;: masks,  \\n  \\logseq____&quot;advantages\\logseq____&quot;: advantages,  \\n  \\logseq____&quot;returns\\logseq____&quot;: returns,  \\n}  \\nbatch_dict.update(model_inputs)  \\n\\nt = time.time()  \\nall_stats = []  \\nearly_stop = False  \\n\\n# 进入 PPO 的训练循环\\nfor _ in range(self.config.ppo_epochs):  \\n  if early_stop:  \\n      break  \\n  b_inds = np.random.permutation(bs)  \\n  for backward_batch_start in range(0, bs, self.config.backward_batch_size):  \\n      backward_batch_end = backward_batch_start + self.config.backward_batch_size  \\n      backward_batch_inds = b_inds[backward_batch_start:backward_batch_end]  \\n\\n      for mini_batch_start in range(0, self.config.backward_batch_size, self.config.mini_batch_size):  \\n          mini_batch_end = mini_batch_start + self.config.mini_batch_size  \\n          mini_batch_inds = backward_batch_inds[mini_batch_start:mini_batch_end]  \\n          mini_batch_dict = {  \\n              \\logseq____&quot;logprobs\\logseq____&quot;: batch_dict[\\logseq____&quot;logprobs\\logseq____&quot;][mini_batch_inds],  \\n              \\logseq____&quot;values\\logseq____&quot;: batch_dict[\\logseq____&quot;values\\logseq____&quot;][mini_batch_inds],  \\n              \\logseq____&quot;masks\\logseq____&quot;: batch_dict[\\logseq____&quot;masks\\logseq____&quot;][mini_batch_inds],  \\n              # hacks: the queries and responses are ragged.  \\n              \\logseq____&quot;queries\\logseq____&quot;: [batch_dict[\\logseq____&quot;queries\\logseq____&quot;][i] for i in mini_batch_inds],  \\n              \\logseq____&quot;responses\\logseq____&quot;: [batch_dict[\\logseq____&quot;responses\\logseq____&quot;][i] for i in mini_batch_inds],  \\n              \\logseq____&quot;advantages\\logseq____&quot;: batch_dict[\\logseq____&quot;advantages\\logseq____&quot;][mini_batch_inds],  \\n              \\logseq____&quot;returns\\logseq____&quot;: batch_dict[\\logseq____&quot;returns\\logseq____&quot;][mini_batch_inds],  \\n          }  \\n          for k in model_inputs_names:  \\n              mini_batch_dict[k] = batch_dict[k][mini_batch_inds]  \\n          with self.accelerator.accumulate(self.model):  \\n              model_inputs = {k: mini_batch_dict[k] for k in model_inputs_names}  \\n              logprobs, logits, vpreds, _ = self.batched_forward_pass(  \\n                  self.model,  \\n                  mini_batch_dict[\\logseq____&quot;queries\\logseq____&quot;],  \\n                  mini_batch_dict[\\logseq____&quot;responses\\logseq____&quot;],  \\n                  model_inputs,  \\n                  return_logits=True,  \\n              )  \\n              # 计算 Loss 并执行梯度下降\\n              train_stats = self.train_minibatch(  \\n                  mini_batch_dict[\\logseq____&quot;logprobs\\logseq____&quot;],  \\n                  mini_batch_dict[\\logseq____&quot;values\\logseq____&quot;],  \\n                  logprobs,  \\n                  logits,  \\n                  vpreds,  \\n                  mini_batch_dict[\\logseq____&quot;masks\\logseq____&quot;],  \\n                  mini_batch_dict[\\logseq____&quot;advantages\\logseq____&quot;],  \\n                  mini_batch_dict[\\logseq____&quot;returns\\logseq____&quot;],  \\n              )  \\n              all_stats.append(train_stats)  \\n\\n  # typically, early stopping is done at the epoch level  \\n  if self.config.early_stopping:  \\n      policykl = train_stats[\\logseq____&quot;policy/policykl\\logseq____&quot;]  \\n      early_stop = self._early_stop(policykl)  \\n      if early_stop:  \\n          break\\n```logseq____&quot;,536870923]],[logseq____&quot;^15logseq____&quot;,[101,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870923]],[logseq____&quot;^15logseq____&quot;,[101,logseq____&quot;^Flogseq____&quot;,100,536870923]],[logseq____&quot;^15logseq____&quot;,[101,logseq____&quot;^Xlogseq____&quot;,95,536870923]],[logseq____&quot;^15logseq____&quot;,[101,logseq____&quot;^Vlogseq____&quot;,95,536870923]],[logseq____&quot;^15logseq____&quot;,[101,logseq____&quot;^Ulogseq____&quot;,95,536870923]],[logseq____&quot;^15logseq____&quot;,[101,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,1],536870923]],[logseq____&quot;^15logseq____&quot;,[101,logseq____&quot;^Jlogseq____&quot;,[],536870923]],[logseq____&quot;^15logseq____&quot;,[101,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-59ba-49b8-87a0-be1b9ff82068logseq____&quot;,536870923]],[logseq____&quot;^15logseq____&quot;,[103,logseq____&quot;^Klogseq____&quot;,1723260458622,536870924]],[logseq____&quot;^15logseq____&quot;,[103,logseq____&quot;^@logseq____&quot;,false,536870924]],[logseq____&quot;^15logseq____&quot;,[103,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;recent updatelogseq____&quot;,536870924]],[logseq____&quot;^15logseq____&quot;,[103,logseq____&quot;^11logseq____&quot;,logseq____&quot;Recent Updatelogseq____&quot;,536870924]],[logseq____&quot;^15logseq____&quot;,[103,logseq____&quot;^Blogseq____&quot;,1723260458622,536870924]],[logseq____&quot;^15logseq____&quot;,[103,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-044d-4901-a766-15365b4675cflogseq____&quot;,536870924]],[logseq____&quot;^15logseq____&quot;,[104,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;2024.8.10  [[RLHF中的PPO算法笔记]]logseq____&quot;,536870924]],[logseq____&quot;^15logseq____&quot;,[104,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870924]],[logseq____&quot;^15logseq____&quot;,[104,logseq____&quot;^Flogseq____&quot;,103,536870924]],[logseq____&quot;^15logseq____&quot;,[104,logseq____&quot;^Xlogseq____&quot;,103,536870924]],[logseq____&quot;^15logseq____&quot;,[104,logseq____&quot;^Vlogseq____&quot;,103,536870924]],[logseq____&quot;^15logseq____&quot;,[104,logseq____&quot;^Ulogseq____&quot;,95,536870924]],[logseq____&quot;^15logseq____&quot;,[104,logseq____&quot;^Ulogseq____&quot;,103,536870924]],[logseq____&quot;^15logseq____&quot;,[104,logseq____&quot;^Hlogseq____&quot;,95,536870924]],[logseq____&quot;^15logseq____&quot;,[104,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-3c6f-4324-9fb5-fd61ed151329logseq____&quot;,536870924]],[logseq____&quot;^15logseq____&quot;,[106,logseq____&quot;^Klogseq____&quot;,1723260458632,536870925]],[logseq____&quot;^15logseq____&quot;,[106,logseq____&quot;^@logseq____&quot;,false,536870925]],[logseq____&quot;^15logseq____&quot;,[106,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;attention in fast waylogseq____&quot;,536870925]],[logseq____&quot;^15logseq____&quot;,[106,logseq____&quot;^11logseq____&quot;,logseq____&quot;attention in fast waylogseq____&quot;,536870925]],[logseq____&quot;^15logseq____&quot;,[106,logseq____&quot;^Blogseq____&quot;,1723260458632,536870925]],[logseq____&quot;^15logseq____&quot;,[106,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-6f65-4ba6-83b0-0c17d36586ablogseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[107,logseq____&quot;^Klogseq____&quot;,1723260458633,536870925]],[logseq____&quot;^15logseq____&quot;,[107,logseq____&quot;^@logseq____&quot;,false,536870925]],[logseq____&quot;^15logseq____&quot;,[107,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;group-query attentionlogseq____&quot;,536870925]],[logseq____&quot;^15logseq____&quot;,[107,logseq____&quot;^11logseq____&quot;,logseq____&quot;Group-Query Attentionlogseq____&quot;,536870925]],[logseq____&quot;^15logseq____&quot;,[107,logseq____&quot;^Blogseq____&quot;,1723260458633,536870925]],[logseq____&quot;^15logseq____&quot;,[107,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-de97-4eb1-8e31-099a7e7bec47logseq____&quot;,536870925]],[logseq____&quot;^15logseq____&quot;,[108,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;[[Multi-Query Attention]]\\ntitle:: attention in fast waylogseq____&quot;,536870925]],[logseq____&quot;^15logseq____&quot;,[108,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870925]],[logseq____&quot;^15logseq____&quot;,[108,logseq____&quot;^Flogseq____&quot;,106,536870925]],[logseq____&quot;^15logseq____&quot;,[108,logseq____&quot;^Xlogseq____&quot;,106,536870925]],[logseq____&quot;^15logseq____&quot;,[108,logseq____&quot;^Vlogseq____&quot;,106,536870925]],[logseq____&quot;^15logseq____&quot;,[108,logseq____&quot;^Ulogseq____&quot;,87,536870925]],[logseq____&quot;^15logseq____&quot;,[108,logseq____&quot;^Ulogseq____&quot;,106,536870925]],[logseq____&quot;^15logseq____&quot;,[108,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;~:titlelogseq____&quot;,logseq____&quot;attention in fast waylogseq____&quot;],536870925]],[logseq____&quot;^15logseq____&quot;,[108,logseq____&quot;^Jlogseq____&quot;,[logseq____&quot;^1:logseq____&quot;],536870925]],[logseq____&quot;^15logseq____&quot;,[108,logseq____&quot;^4logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^1:logseq____&quot;,logseq____&quot;attention in fast waylogseq____&quot;],536870925]],[logseq____&quot;^15logseq____&quot;,[108,logseq____&quot;^Hlogseq____&quot;,87,536870925]],[logseq____&quot;^15logseq____&quot;,[108,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-8fd9-462d-8169-bdddf635b195logseq____&quot;,536870925]],[logseq____&quot;^15logseq____&quot;,[109,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;[[Group-Query Attention]]logseq____&quot;,536870925]],[logseq____&quot;^15logseq____&quot;,[109,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870925]],[logseq____&quot;^15logseq____&quot;,[109,logseq____&quot;^Flogseq____&quot;,108,536870925]],[logseq____&quot;^15logseq____&quot;,[109,logseq____&quot;^Xlogseq____&quot;,106,536870925]],[logseq____&quot;^15logseq____&quot;,[109,logseq____&quot;^Vlogseq____&quot;,106,536870925]],[logseq____&quot;^15logseq____&quot;,[109,logseq____&quot;^Ulogseq____&quot;,106,536870925]],[logseq____&quot;^15logseq____&quot;,[109,logseq____&quot;^Ulogseq____&quot;,107,536870925]],[logseq____&quot;^15logseq____&quot;,[109,logseq____&quot;^Hlogseq____&quot;,107,536870925]],[logseq____&quot;^15logseq____&quot;,[109,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-72bf-44e9-aa88-8f7557a42ac4logseq____&quot;,536870925]],[logseq____&quot;^15logseq____&quot;,[110,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;logseq____&quot;,536870925]],[logseq____&quot;^15logseq____&quot;,[110,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870925]],[logseq____&quot;^15logseq____&quot;,[110,logseq____&quot;^Flogseq____&quot;,109,536870925]],[logseq____&quot;^15logseq____&quot;,[110,logseq____&quot;^Xlogseq____&quot;,106,536870925]],[logseq____&quot;^15logseq____&quot;,[110,logseq____&quot;^Vlogseq____&quot;,106,536870925]],[logseq____&quot;^15logseq____&quot;,[110,logseq____&quot;^Ulogseq____&quot;,106,536870925]],[logseq____&quot;^15logseq____&quot;,[110,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-513c-4781-a70e-8992be714bdblogseq____&quot;,536870925]],[logseq____&quot;^15logseq____&quot;,[112,logseq____&quot;^Klogseq____&quot;,1723260458665,536870926]],[logseq____&quot;^15logseq____&quot;,[112,logseq____&quot;^@logseq____&quot;,false,536870926]],[logseq____&quot;^15logseq____&quot;,[112,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;readlogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[112,logseq____&quot;^11logseq____&quot;,logseq____&quot;readlogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[112,logseq____&quot;^Blogseq____&quot;,1723260458665,536870926]],[logseq____&quot;^15logseq____&quot;,[112,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-786c-43c1-81f4-49d7b89b9ddclogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[113,logseq____&quot;^Klogseq____&quot;,1723260458668,536870926]],[logseq____&quot;^15logseq____&quot;,[113,logseq____&quot;^@logseq____&quot;,false,536870926]],[logseq____&quot;^15logseq____&quot;,[113,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;vllmlogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[113,logseq____&quot;^11logseq____&quot;,logseq____&quot;vllmlogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[113,logseq____&quot;^Blogseq____&quot;,1723260458668,536870926]],[logseq____&quot;^15logseq____&quot;,[113,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-c3a1-448c-a095-b620fbff46c0logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[114,logseq____&quot;^Klogseq____&quot;,1723260458670,536870926]],[logseq____&quot;^15logseq____&quot;,[114,logseq____&quot;^@logseq____&quot;,false,536870926]],[logseq____&quot;^15logseq____&quot;,[114,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;量化llmlogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[114,logseq____&quot;^11logseq____&quot;,logseq____&quot;量化LLMlogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[114,logseq____&quot;^Blogseq____&quot;,1723260458670,536870926]],[logseq____&quot;^15logseq____&quot;,[114,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-45fa-406c-b687-d04512a5e599logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[115,logseq____&quot;^Klogseq____&quot;,1723260458666,536870926]],[logseq____&quot;^15logseq____&quot;,[115,logseq____&quot;^@logseq____&quot;,false,536870926]],[logseq____&quot;^15logseq____&quot;,[115,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;faster inferencelogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[115,logseq____&quot;^11logseq____&quot;,logseq____&quot;faster inferencelogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[115,logseq____&quot;^Blogseq____&quot;,1723260458666,536870926]],[logseq____&quot;^15logseq____&quot;,[115,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-fd0a-4ed6-81de-4c6ca3183acalogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[116,logseq____&quot;^Klogseq____&quot;,1723260458669,536870926]],[logseq____&quot;^15logseq____&quot;,[116,logseq____&quot;^@logseq____&quot;,false,536870926]],[logseq____&quot;^15logseq____&quot;,[116,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;speculative decodinglogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[116,logseq____&quot;^11logseq____&quot;,logseq____&quot;speculative decodinglogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[116,logseq____&quot;^Blogseq____&quot;,1723260458669,536870926]],[logseq____&quot;^15logseq____&quot;,[116,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-bf17-40a9-a04a-8665a24bfc77logseq____&quot;,536870931]],[logseq____&quot;^15logseq____&quot;,[117,logseq____&quot;^Klogseq____&quot;,1723260458669,536870926]],[logseq____&quot;^15logseq____&quot;,[117,logseq____&quot;^@logseq____&quot;,false,536870926]],[logseq____&quot;^15logseq____&quot;,[117,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;pagedattentionlogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[117,logseq____&quot;^11logseq____&quot;,logseq____&quot;PagedAttentionlogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[117,logseq____&quot;^Blogseq____&quot;,1723260458669,536870926]],[logseq____&quot;^15logseq____&quot;,[117,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-d5e2-4e87-80cb-2979026ac304logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[118,logseq____&quot;^Klogseq____&quot;,1723260458670,536870926]],[logseq____&quot;^15logseq____&quot;,[118,logseq____&quot;^@logseq____&quot;,false,536870926]],[logseq____&quot;^15logseq____&quot;,[118,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;flash attentionlogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[118,logseq____&quot;^11logseq____&quot;,logseq____&quot;flash attentionlogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[118,logseq____&quot;^Blogseq____&quot;,1723260458670,536870926]],[logseq____&quot;^15logseq____&quot;,[118,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-1e29-4d16-9b0d-2da41fe26db2logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[119,logseq____&quot;^Klogseq____&quot;,1723260458667,536870926]],[logseq____&quot;^15logseq____&quot;,[119,logseq____&quot;^@logseq____&quot;,false,536870926]],[logseq____&quot;^15logseq____&quot;,[119,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;kv cachelogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[119,logseq____&quot;^11logseq____&quot;,logseq____&quot;KV Cachelogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[119,logseq____&quot;^Blogseq____&quot;,1723260458667,536870926]],[logseq____&quot;^15logseq____&quot;,[119,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-55a0-4e75-add0-17c5754f2e82logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[120,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## PagedAttentionlogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[120,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[120,logseq____&quot;^Flogseq____&quot;,123,536870926]],[logseq____&quot;^15logseq____&quot;,[120,logseq____&quot;^Xlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[120,logseq____&quot;^Vlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[120,logseq____&quot;^Ulogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[120,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870926]],[logseq____&quot;^15logseq____&quot;,[120,logseq____&quot;^Jlogseq____&quot;,[],536870926]],[logseq____&quot;^15logseq____&quot;,[120,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-c7be-462b-8dbf-31ccdf0a3760logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[121,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## 量化模型logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[121,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[121,logseq____&quot;^Flogseq____&quot;,132,536870926]],[logseq____&quot;^15logseq____&quot;,[121,logseq____&quot;^Xlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[121,logseq____&quot;^Vlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[121,logseq____&quot;^Ulogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[121,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870926]],[logseq____&quot;^15logseq____&quot;,[121,logseq____&quot;^Jlogseq____&quot;,[],536870926]],[logseq____&quot;^15logseq____&quot;,[121,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-886c-4b6f-ae2c-2f263a760dfdlogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[122,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;[[量化LLM]]logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[122,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[122,logseq____&quot;^Flogseq____&quot;,121,536870926]],[logseq____&quot;^15logseq____&quot;,[122,logseq____&quot;^Xlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[122,logseq____&quot;^Vlogseq____&quot;,121,536870926]],[logseq____&quot;^15logseq____&quot;,[122,logseq____&quot;^Ulogseq____&quot;,114,536870926]],[logseq____&quot;^15logseq____&quot;,[122,logseq____&quot;^Ulogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[122,logseq____&quot;^Hlogseq____&quot;,114,536870926]],[logseq____&quot;^15logseq____&quot;,[122,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-0b31-4922-8243-d372991d0011logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[123,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## Multi Query Attentionlogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[123,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[123,logseq____&quot;^Flogseq____&quot;,133,536870926]],[logseq____&quot;^15logseq____&quot;,[123,logseq____&quot;^Xlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[123,logseq____&quot;^Vlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[123,logseq____&quot;^Ulogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[123,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870926]],[logseq____&quot;^15logseq____&quot;,[123,logseq____&quot;^Jlogseq____&quot;,[],536870926]],[logseq____&quot;^15logseq____&quot;,[123,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-c5df-417f-bb61-6c8eca725735logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[124,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;好处：可以 batch 化的 inferencelogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[124,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[124,logseq____&quot;^Flogseq____&quot;,125,536870926]],[logseq____&quot;^15logseq____&quot;,[124,logseq____&quot;^Xlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[124,logseq____&quot;^Vlogseq____&quot;,131,536870926]],[logseq____&quot;^15logseq____&quot;,[124,logseq____&quot;^Ulogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[124,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-ab5c-4a26-a5da-bfd2820718d9logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[125,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;left padding不同长度的句子logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[125,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[125,logseq____&quot;^Flogseq____&quot;,131,536870926]],[logseq____&quot;^15logseq____&quot;,[125,logseq____&quot;^Xlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[125,logseq____&quot;^Vlogseq____&quot;,131,536870926]],[logseq____&quot;^15logseq____&quot;,[125,logseq____&quot;^Ulogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[125,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-5a0d-4547-a105-44a477303331logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[126,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;[[flash attention]]logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[126,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[126,logseq____&quot;^Flogseq____&quot;,132,536870926]],[logseq____&quot;^15logseq____&quot;,[126,logseq____&quot;^Xlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[126,logseq____&quot;^Vlogseq____&quot;,132,536870926]],[logseq____&quot;^15logseq____&quot;,[126,logseq____&quot;^Ulogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[126,logseq____&quot;^Ulogseq____&quot;,118,536870926]],[logseq____&quot;^15logseq____&quot;,[126,logseq____&quot;^Hlogseq____&quot;,118,536870926]],[logseq____&quot;^15logseq____&quot;,[126,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-8cf1-4ca3-8d3f-dff0261e3975logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[127,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;![left_padding](../assets/left_padding.png){:height 605, :width 702}logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[127,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[127,logseq____&quot;^Flogseq____&quot;,125,536870926]],[logseq____&quot;^15logseq____&quot;,[127,logseq____&quot;^Xlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[127,logseq____&quot;^Vlogseq____&quot;,125,536870926]],[logseq____&quot;^15logseq____&quot;,[127,logseq____&quot;^Ulogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[127,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-a4ed-4d4c-9cf7-38ca529dd872logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[128,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;[[PagedAttention]] 将kv cache 切成小块管理，[[vllm]] 基于paged attention进行内存管理，加速inference。logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[128,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[128,logseq____&quot;^Flogseq____&quot;,120,536870926]],[logseq____&quot;^15logseq____&quot;,[128,logseq____&quot;^Xlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[128,logseq____&quot;^Vlogseq____&quot;,120,536870926]],[logseq____&quot;^15logseq____&quot;,[128,logseq____&quot;^Ulogseq____&quot;,113,536870926]],[logseq____&quot;^15logseq____&quot;,[128,logseq____&quot;^Ulogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[128,logseq____&quot;^Ulogseq____&quot;,117,536870926]],[logseq____&quot;^15logseq____&quot;,[128,logseq____&quot;^Hlogseq____&quot;,113,536870926]],[logseq____&quot;^15logseq____&quot;,[128,logseq____&quot;^Hlogseq____&quot;,117,536870926]],[logseq____&quot;^15logseq____&quot;,[128,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-67d8-4d0f-9472-39eb109ccc53logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[129,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;[[KV Cache]]logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[129,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[129,logseq____&quot;^Flogseq____&quot;,133,536870926]],[logseq____&quot;^15logseq____&quot;,[129,logseq____&quot;^Xlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[129,logseq____&quot;^Vlogseq____&quot;,133,536870926]],[logseq____&quot;^15logseq____&quot;,[129,logseq____&quot;^Ulogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[129,logseq____&quot;^Ulogseq____&quot;,119,536870926]],[logseq____&quot;^15logseq____&quot;,[129,logseq____&quot;^Hlogseq____&quot;,119,536870926]],[logseq____&quot;^15logseq____&quot;,[129,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-2dfb-4b1e-865d-46d5afb7ecfdlogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[130,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;在一个句子结束后 inference 下一个句子logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[130,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[130,logseq____&quot;^Flogseq____&quot;,142,536870926]],[logseq____&quot;^15logseq____&quot;,[130,logseq____&quot;^Xlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[130,logseq____&quot;^Vlogseq____&quot;,142,536870926]],[logseq____&quot;^15logseq____&quot;,[130,logseq____&quot;^Ulogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[130,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-99de-4456-8b09-bc3805813d3flogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[131,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## Batchinglogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[131,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[131,logseq____&quot;^Flogseq____&quot;,139,536870926]],[logseq____&quot;^15logseq____&quot;,[131,logseq____&quot;^Xlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[131,logseq____&quot;^Vlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[131,logseq____&quot;^Ulogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[131,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870926]],[logseq____&quot;^15logseq____&quot;,[131,logseq____&quot;^Jlogseq____&quot;,[],536870926]],[logseq____&quot;^15logseq____&quot;,[131,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-efcc-4df1-bec8-e16da8a65ad4logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[132,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## flash attentionlogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[132,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[132,logseq____&quot;^Flogseq____&quot;,135,536870926]],[logseq____&quot;^15logseq____&quot;,[132,logseq____&quot;^Xlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[132,logseq____&quot;^Vlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[132,logseq____&quot;^Ulogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[132,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870926]],[logseq____&quot;^15logseq____&quot;,[132,logseq____&quot;^Jlogseq____&quot;,[],536870926]],[logseq____&quot;^15logseq____&quot;,[132,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-3b03-4eb1-9609-ee7a32481ffdlogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[133,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## KVCachelogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[133,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[133,logseq____&quot;^Flogseq____&quot;,142,536870926]],[logseq____&quot;^15logseq____&quot;,[133,logseq____&quot;^Xlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[133,logseq____&quot;^Vlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[133,logseq____&quot;^Ulogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[133,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870926]],[logseq____&quot;^15logseq____&quot;,[133,logseq____&quot;^Jlogseq____&quot;,[],536870926]],[logseq____&quot;^15logseq____&quot;,[133,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-ed7a-48e1-83fe-c36b9c6d3c63logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[134,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;DONE #read https://vgel.me/posts/faster-inference/logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[134,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[134,logseq____&quot;^Flogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[134,logseq____&quot;^10logseq____&quot;,logseq____&quot;DONElogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[134,logseq____&quot;^Xlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[134,logseq____&quot;^Vlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[134,logseq____&quot;^Ulogseq____&quot;,7,536870926]],[logseq____&quot;^15logseq____&quot;,[134,logseq____&quot;^Ulogseq____&quot;,112,536870926]],[logseq____&quot;^15logseq____&quot;,[134,logseq____&quot;^Ulogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[134,logseq____&quot;^Hlogseq____&quot;,7,536870926]],[logseq____&quot;^15logseq____&quot;,[134,logseq____&quot;^Hlogseq____&quot;,112,536870926]],[logseq____&quot;^15logseq____&quot;,[134,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-3cd3-43fb-8655-c425f45f7814logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[135,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## Speculative Decodinglogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[135,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[135,logseq____&quot;^Flogseq____&quot;,120,536870926]],[logseq____&quot;^15logseq____&quot;,[135,logseq____&quot;^Xlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[135,logseq____&quot;^Vlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[135,logseq____&quot;^Ulogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[135,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870926]],[logseq____&quot;^15logseq____&quot;,[135,logseq____&quot;^Jlogseq____&quot;,[],536870926]],[logseq____&quot;^15logseq____&quot;,[135,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-8125-49af-890f-363f40a93ca2logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[136,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;[[Multi-Query Attention]] 节省参数与计算量logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[136,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[136,logseq____&quot;^Flogseq____&quot;,123,536870926]],[logseq____&quot;^15logseq____&quot;,[136,logseq____&quot;^Xlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[136,logseq____&quot;^Vlogseq____&quot;,123,536870926]],[logseq____&quot;^15logseq____&quot;,[136,logseq____&quot;^Ulogseq____&quot;,87,536870926]],[logseq____&quot;^15logseq____&quot;,[136,logseq____&quot;^Ulogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[136,logseq____&quot;^Hlogseq____&quot;,87,536870926]],[logseq____&quot;^15logseq____&quot;,[136,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-d91a-4989-8815-1b31f6aa1020logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[137,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;[[speculative decoding]] 通过draft model 进行 decode ，同时利用 oracle model 进行纠错。由于 draft model 是小模型，于是可以大大加速inference 过程。logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[137,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[137,logseq____&quot;^Flogseq____&quot;,135,536870926]],[logseq____&quot;^15logseq____&quot;,[137,logseq____&quot;^Xlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[137,logseq____&quot;^Vlogseq____&quot;,135,536870926]],[logseq____&quot;^15logseq____&quot;,[137,logseq____&quot;^Ulogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[137,logseq____&quot;^Ulogseq____&quot;,116,536870926]],[logseq____&quot;^15logseq____&quot;,[137,logseq____&quot;^Hlogseq____&quot;,116,536870926]],[logseq____&quot;^15logseq____&quot;,[137,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-a689-4527-94fa-b17aa322be9alogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[138,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;使用torch.compile加速算子logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[138,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[138,logseq____&quot;^Flogseq____&quot;,139,536870926]],[logseq____&quot;^15logseq____&quot;,[138,logseq____&quot;^Xlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[138,logseq____&quot;^Vlogseq____&quot;,139,536870926]],[logseq____&quot;^15logseq____&quot;,[138,logseq____&quot;^Ulogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[138,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-bad1-4892-87a7-3e5fc539e501logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[139,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## torch.compilelogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[139,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[139,logseq____&quot;^Flogseq____&quot;,134,536870926]],[logseq____&quot;^15logseq____&quot;,[139,logseq____&quot;^Xlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[139,logseq____&quot;^Vlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[139,logseq____&quot;^Ulogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[139,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870926]],[logseq____&quot;^15logseq____&quot;,[139,logseq____&quot;^Jlogseq____&quot;,[],536870926]],[logseq____&quot;^15logseq____&quot;,[139,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-be8a-40f7-860a-21a88d8cba5elogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[140,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;![continuous_batching](../assets/continuous_batching.png){:height 496, :width 590}logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[140,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[140,logseq____&quot;^Flogseq____&quot;,130,536870926]],[logseq____&quot;^15logseq____&quot;,[140,logseq____&quot;^Xlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[140,logseq____&quot;^Vlogseq____&quot;,130,536870926]],[logseq____&quot;^15logseq____&quot;,[140,logseq____&quot;^Ulogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[140,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-81b0-4df0-88c7-44b222439321logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[141,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;坏处：在一个句子已经 [end] 时，依然需要不停地输出 random，会占用 GPUlogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[141,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[141,logseq____&quot;^Flogseq____&quot;,124,536870926]],[logseq____&quot;^15logseq____&quot;,[141,logseq____&quot;^Xlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[141,logseq____&quot;^Vlogseq____&quot;,131,536870926]],[logseq____&quot;^15logseq____&quot;,[141,logseq____&quot;^Ulogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[141,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-317d-4513-8194-a8f5b5b41864logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[142,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## Continuous Batchinglogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[142,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[142,logseq____&quot;^Flogseq____&quot;,131,536870926]],[logseq____&quot;^15logseq____&quot;,[142,logseq____&quot;^Xlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[142,logseq____&quot;^Vlogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[142,logseq____&quot;^Ulogseq____&quot;,115,536870926]],[logseq____&quot;^15logseq____&quot;,[142,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870926]],[logseq____&quot;^15logseq____&quot;,[142,logseq____&quot;^Jlogseq____&quot;,[],536870926]],[logseq____&quot;^15logseq____&quot;,[142,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-e234-4662-8921-2772aa900b9flogseq____&quot;,536870926]],[logseq____&quot;^15logseq____&quot;,[144,logseq____&quot;^Klogseq____&quot;,1723260458705,536870927]],[logseq____&quot;^15logseq____&quot;,[144,logseq____&quot;^@logseq____&quot;,false,536870927]],[logseq____&quot;^15logseq____&quot;,[144,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;hls__2002.05202v1_1680252170645_0logseq____&quot;,536870927]],[logseq____&quot;^15logseq____&quot;,[144,logseq____&quot;^11logseq____&quot;,logseq____&quot;hls__2002.05202v1_1680252170645_0logseq____&quot;,536870927]],[logseq____&quot;^15logseq____&quot;,[144,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;~:filelogseq____&quot;,logseq____&quot;[2002.05202v1_1680252170645_0.pdf](../assets/2002.05202v1_1680252170645_0.pdf)logseq____&quot;,logseq____&quot;~:file-pathlogseq____&quot;,logseq____&quot;../assets/2002.05202v1_1680252170645_0.pdflogseq____&quot;],536870927]],[logseq____&quot;^15logseq____&quot;,[144,logseq____&quot;^4logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^1;logseq____&quot;,logseq____&quot;[2002.05202v1_1680252170645_0.pdf](../assets/2002.05202v1_1680252170645_0.pdf)logseq____&quot;,logseq____&quot;^1logseq____&lt;logseq____&quot;,logseq____&quot;../assets/2002.05202v1_1680252170645_0.pdflogseq____&quot;],536870927]],[logseq____&quot;^15logseq____&quot;,[144,logseq____&quot;^Blogseq____&quot;,1723260458705,536870927]],[logseq____&quot;^15logseq____&quot;,[144,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-f725-4772-aa00-4bbafc6a47f8logseq____&quot;,536870927]],[logseq____&quot;^15logseq____&quot;,[145,logseq____&quot;^Klogseq____&quot;,1723260458705,536870927]],[logseq____&quot;^15logseq____&quot;,[145,logseq____&quot;^@logseq____&quot;,false,536870927]],[logseq____&quot;^15logseq____&quot;,[145,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;filelogseq____&quot;,536870927]],[logseq____&quot;^15logseq____&quot;,[145,logseq____&quot;^11logseq____&quot;,logseq____&quot;filelogseq____&quot;,536870927]],[logseq____&quot;^15logseq____&quot;,[145,logseq____&quot;^Blogseq____&quot;,1723260458705,536870927]],[logseq____&quot;^15logseq____&quot;,[145,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-c334-46aa-9331-41e0e824f345logseq____&quot;,536870927]],[logseq____&quot;^15logseq____&quot;,[146,logseq____&quot;^Klogseq____&quot;,1723260458705,536870927]],[logseq____&quot;^15logseq____&quot;,[146,logseq____&quot;^@logseq____&quot;,false,536870927]],[logseq____&quot;^15logseq____&quot;,[146,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;file-pathlogseq____&quot;,536870927]],[logseq____&quot;^15logseq____&quot;,[146,logseq____&quot;^11logseq____&quot;,logseq____&quot;file-pathlogseq____&quot;,536870927]],[logseq____&quot;^15logseq____&quot;,[146,logseq____&quot;^Blogseq____&quot;,1723260458705,536870927]],[logseq____&quot;^15logseq____&quot;,[146,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-5c81-43db-9add-fc361ceb3c46logseq____&quot;,536870927]],[logseq____&quot;^15logseq____&quot;,[147,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;file:: [2002.05202v1_1680252170645_0.pdf](../assets/2002.05202v1_1680252170645_0.pdf)\\nfile-path:: ../assets/2002.05202v1_1680252170645_0.pdf\\nlogseq____&quot;,536870927]],[logseq____&quot;^15logseq____&quot;,[147,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870927]],[logseq____&quot;^15logseq____&quot;,[147,logseq____&quot;~:block/invalid-propertieslogseq____&quot;,[logseq____&quot;~#setlogseq____&quot;,[]],536870927]],[logseq____&quot;^15logseq____&quot;,[147,logseq____&quot;^Flogseq____&quot;,144,536870927]],[logseq____&quot;^15logseq____&quot;,[147,logseq____&quot;^Xlogseq____&quot;,144,536870927]],[logseq____&quot;^15logseq____&quot;,[147,logseq____&quot;^Vlogseq____&quot;,144,536870927]],[logseq____&quot;^15logseq____&quot;,[147,logseq____&quot;^Ulogseq____&quot;,144,536870927]],[logseq____&quot;^15logseq____&quot;,[147,logseq____&quot;^Ulogseq____&quot;,145,536870927]],[logseq____&quot;^15logseq____&quot;,[147,logseq____&quot;^Ulogseq____&quot;,146,536870927]],[logseq____&quot;^15logseq____&quot;,[147,logseq____&quot;^:logseq____&quot;,true,536870927]],[logseq____&quot;^15logseq____&quot;,[147,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^1;logseq____&quot;,logseq____&quot;[2002.05202v1_1680252170645_0.pdf](../assets/2002.05202v1_1680252170645_0.pdf)logseq____&quot;,logseq____&quot;^1logseq____&lt;logseq____&quot;,logseq____&quot;../assets/2002.05202v1_1680252170645_0.pdflogseq____&quot;],536870927]],[logseq____&quot;^15logseq____&quot;,[147,logseq____&quot;^Jlogseq____&quot;,[logseq____&quot;^1;logseq____&quot;,logseq____&quot;^1logseq____&lt;logseq____&quot;],536870927]],[logseq____&quot;^15logseq____&quot;,[147,logseq____&quot;^4logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^1;logseq____&quot;,logseq____&quot;[2002.05202v1_1680252170645_0.pdf](../assets/2002.05202v1_1680252170645_0.pdf)logseq____&quot;,logseq____&quot;^1logseq____&lt;logseq____&quot;,logseq____&quot;../assets/2002.05202v1_1680252170645_0.pdflogseq____&quot;],536870927]],[logseq____&quot;^15logseq____&quot;,[147,logseq____&quot;^Hlogseq____&quot;,145,536870927]],[logseq____&quot;^15logseq____&quot;,[147,logseq____&quot;^Hlogseq____&quot;,146,536870927]],[logseq____&quot;^15logseq____&quot;,[147,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-ffe9-42e3-bc37-cabe09b4ee0clogseq____&quot;,536870927]],[logseq____&quot;^15logseq____&quot;,[149,logseq____&quot;^Klogseq____&quot;,1723260458735,536870928]],[logseq____&quot;^15logseq____&quot;,[149,logseq____&quot;^@logseq____&quot;,false,536870928]],[logseq____&quot;^15logseq____&quot;,[149,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;outlines:大模型固定格式解码logseq____&quot;,536870928]],[logseq____&quot;^15logseq____&quot;,[149,logseq____&quot;^11logseq____&quot;,logseq____&quot;outlines:大模型固定格式解码logseq____&quot;,536870928]],[logseq____&quot;^15logseq____&quot;,[149,logseq____&quot;^Blogseq____&quot;,1723260458735,536870928]],[logseq____&quot;^15logseq____&quot;,[149,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-ef84-4d33-8a1e-a01ad172eeaelogseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[150,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;最近，我在用 GPT-4 生成特定的 JSON 时，没有使用 OpenAI 自带的 JSON 模式，结果发现生成的 JSON 格式有时并不标准。而当使用 instruct 能力较弱的模型时，我不想通过微调来实现模型生成特定格式的文本，我想起了之前看到的一个叫 Outlines 的工具声称让模型可以做特定格式的 guided generation，于是试用了一下，发现在我的模型上确实可以生成 JSON，于是看了一下![论文](https://arxiv.org/pdf/2307.09702) 与代码。在论文中，Outlines 将大模型生成重新构建为有限状态机之间的转换，解决了在严格格式要求下引导 LLM 生成的问题。在现在模版工程比较流行时，在这个领域工作都会遇到的一个问题，尤其是 JSON 生成。logseq____&quot;,536870928]],[logseq____&quot;^15logseq____&quot;,[150,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870928]],[logseq____&quot;^15logseq____&quot;,[150,logseq____&quot;^Flogseq____&quot;,149,536870928]],[logseq____&quot;^15logseq____&quot;,[150,logseq____&quot;^Xlogseq____&quot;,149,536870928]],[logseq____&quot;^15logseq____&quot;,[150,logseq____&quot;^Vlogseq____&quot;,149,536870928]],[logseq____&quot;^15logseq____&quot;,[150,logseq____&quot;^Ulogseq____&quot;,149,536870928]],[logseq____&quot;^15logseq____&quot;,[150,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-6277-40b3-8b98-216d0f1052e0logseq____&quot;,536870928]],[logseq____&quot;^15logseq____&quot;,[151,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## 简化问题\\n\\n为了简化，假设我们的模型的词表只有 5 个词：\\nlogseq____&gt; \\logseq____&quot;A\\logseq____&quot;, \\logseq____&quot; .\\logseq____&quot; , \\logseq____&quot;42\\logseq____&quot; , \\logseq____&quot;.2\\logseq____&quot; ,  \\logseq____&quot;1\\logseq____&quot;\\n\\n如果我们需要使大模型生成一个满足`[0-9]*)?\\\\.?[0-9]` 的浮点数，Outline 会建立起以下的有限状态机（FSM）\\n\\n![outlines1](../assets/outlines1.png){:height 321, :width 655}\\n\\n当开始产生时，FSM 在状态0，此时算法会 mask 词典中的非数字“A”，因为它不会被 FSM 所接受，此时其他四个数字都可以被采样；如果采样了“.2”，状态机将会到状态3，此时，只有“1”与“42”是合法的，其他不符合正则的字符在采样时将会被 mask；如果采样了“1”，状态要将转移到状态  1，此时，除了“A” 都是合法的采样，因此此时只会 mask “A”。\\n\\n![outlines2](../assets/outlines2.png){:height 359, :width 655}\\n\\n如上面所示，Outlines 使用的将 FSM 与 decoding 结合的方式的思路是简单有效的。logseq____&quot;,536870928]],[logseq____&quot;^15logseq____&quot;,[151,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870928]],[logseq____&quot;^15logseq____&quot;,[151,logseq____&quot;^Flogseq____&quot;,150,536870928]],[logseq____&quot;^15logseq____&quot;,[151,logseq____&quot;^Xlogseq____&quot;,149,536870928]],[logseq____&quot;^15logseq____&quot;,[151,logseq____&quot;^Vlogseq____&quot;,149,536870928]],[logseq____&quot;^15logseq____&quot;,[151,logseq____&quot;^Ulogseq____&quot;,149,536870928]],[logseq____&quot;^15logseq____&quot;,[151,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870928]],[logseq____&quot;^15logseq____&quot;,[151,logseq____&quot;^Jlogseq____&quot;,[],536870928]],[logseq____&quot;^15logseq____&quot;,[151,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-2356-48fd-b350-45b9a0d916f5logseq____&quot;,536870928]],[logseq____&quot;^15logseq____&quot;,[152,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## 算法解读\\n\\nOutlines 使用正则表达式的有限状态机（FSM）对词汇表进行预处理，并建立一个索引。它会考虑从每一个可行的 FSM 状态开始，因为词汇表中的字符串可能匹配正则表达式的任意部分，而这些部分隐含地对应着 FSM 状态。下面的算法会通过遍历状态转移图，找到所有可以接受输入字符串的路径，并将这些路径作为子序列返回。每当遇到无法继续转移的状态时，算法会尝试下一个可能的起始状态。\\n```\\nfunction FIND_SUB_SEQUENCES(M, v)\\n\\tM = (Q, Σ, δ, q0, F)\\n    res ← ()                      // 初始化结果集为空\\n    for r ∈ δ^(-1)(., v0) do      // 遍历所有可以读取 v0 的状态\\n      p ← (r)                     // 初始化路径\\n      for i ← 1, |v| - 1 do       // 遍历字符串 v 的每个字符\\n          if δ(r, vi) = ∅ then    // 检查是否有转移\\n              p ← ()              // 无转移，清空路径\\n              break               // 中断循环\\n          end if\\n          r ← δ(r, vi)            // 更新状态\\n          p ← append(p, r)        // 路径中追加状态\\n      end for\\n      res ← append(res, p)        // 将路径添加到结果集\\n\\tend for\\n    return res                    // 返回结果集\\nend function\\n```\\n\\n\\n遍历词汇表来确定有效的未来状态仍然会带来较大的计算开销，而 outlines的 FSM 在给定约束条件下被预先计算，并创建了状态转移的索引。下面的算法通过遍历词汇表中的每个词，找到所有能够接受该词的子序列，并根据子序列的初始状态构建一个映射，将词汇与 FSM 的状态关联起来。Outlines 会在解码开始时一次性缓存，从而在每次解码时不需要额外耗时。\\n\\n```\\nfunction MAP_STATES_TO_VOCAB(M, V)\\n\\tM = (Q, Σ, δ, q0, F)  \\n\\tInitialize the map σ with empty sets for each element in Q  \\n\\tfor v ∈ V do                      // 遍历词汇表  \\n\\t\\tZ ← find_sub_sequences(M, v)  // 查找接受 v 的子序列  \\n\\t\\tfor z ∈ Z do                  // 遍历每个子序列  \\n\\t\\t\\tσ(z0) ← σ(z0) ∪ v         // 将 v 加入到对应的集合中  \\n\\t\\tend for  \\n\\tend for  \\n\\treturn σ                           // 返回映射  \\nend function\\n```\\n\\n结合到 LLM 的 decode 部分，在 mask 时，由于有了上述 FSM 的提前缓存，因此 mask 步骤只需要 $$O(1)$$的时间。\\n\\n```\\nfunction SAMPLE_TOKENS(L)\\n\\ts ← ()                                 // 初始化空序列\\n\\tfor i ← 1, L do                        // 遍历 token 的序列长度\\n\\t\\tα ← LLM(s, θ)                      // 根据当前序列和模型参数生成预测分布\\n\\t\\tConstruct the mask m(s)            // 构建掩码\\n\\t\\tã ← m ⊙ α                         // 将掩码应用到预测分布\\n\\t\\tSample  ̃s ~ Categorical(ã)        // 从新的分布中采样下一个token\\n\\t\\tif  ̃s = EOS then                  // 如果采样到结束符\\n\\t\\t\\tbreak                          // 中断循环\\n\\t\\tend if\\n\\t\\ts ← append(s, ̃s)                  // 将采样到的token添加到序列\\n\\tend for\\n\\treturn s                                // 返回生成的序列\\nend function\\n\\n```logseq____&quot;,536870928]],[logseq____&quot;^15logseq____&quot;,[152,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870928]],[logseq____&quot;^15logseq____&quot;,[152,logseq____&quot;^Flogseq____&quot;,151,536870928]],[logseq____&quot;^15logseq____&quot;,[152,logseq____&quot;^Xlogseq____&quot;,149,536870928]],[logseq____&quot;^15logseq____&quot;,[152,logseq____&quot;^Vlogseq____&quot;,149,536870928]],[logseq____&quot;^15logseq____&quot;,[152,logseq____&quot;^Ulogseq____&quot;,149,536870928]],[logseq____&quot;^15logseq____&quot;,[152,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870928]],[logseq____&quot;^15logseq____&quot;,[152,logseq____&quot;^Jlogseq____&quot;,[],536870928]],[logseq____&quot;^15logseq____&quot;,[152,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-409b-4574-ae19-29717a82445dlogseq____&quot;,536870928]],[logseq____&quot;^15logseq____&quot;,[153,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## 加速 LLM\\n\\n在另一个[博客](https://vgel.me/posts/faster-inference/#openai-json-mode)中，有提到 Outlines 用来进行加速 LLM，除了在生成固定格式时，由于固定格式的存在，在生成 JSON 数据时，只有一部分来自模型的输出，而更多的时候会被 FSM 自动插入字符，节约了一大笔 token 数，如在生成下列固定 key 的 json时：\\n\\n{\\n  \\logseq____&quot;name\\logseq____&quot;: \\logseq____&quot;clerame\\logseq____&quot;,    (4 ambiguous tokens: cl er ame \\logseq____&quot;,\\\\n)\\n  \\logseq____&quot;age\\logseq____&quot;: 7,             (2 ambiguous tokens: 7 ,\\\\n)\\n  \\logseq____&quot;armor\\logseq____&quot;: \\logseq____&quot;plate\\logseq____&quot;,     (1 ambiguous token:  plate)\\n  \\logseq____&quot;weapon\\logseq____&quot;: \\logseq____&quot;mace\\logseq____&quot;,     (1 ambiguous token:  m)\\n  \\logseq____&quot;strength\\logseq____&quot;: 4171      (3 ambiguous tokens: 417 1 \\\\n)\\n}\\n\\n共 41 个 tokens，但只有 11  个token 需要从模型中产生。logseq____&quot;,536870928]],[logseq____&quot;^15logseq____&quot;,[153,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870928]],[logseq____&quot;^15logseq____&quot;,[153,logseq____&quot;^Flogseq____&quot;,152,536870928]],[logseq____&quot;^15logseq____&quot;,[153,logseq____&quot;^Xlogseq____&quot;,149,536870928]],[logseq____&quot;^15logseq____&quot;,[153,logseq____&quot;^Vlogseq____&quot;,149,536870928]],[logseq____&quot;^15logseq____&quot;,[153,logseq____&quot;^Ulogseq____&quot;,149,536870928]],[logseq____&quot;^15logseq____&quot;,[153,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870928]],[logseq____&quot;^15logseq____&quot;,[153,logseq____&quot;^Jlogseq____&quot;,[],536870928]],[logseq____&quot;^15logseq____&quot;,[153,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-d51b-4cb0-8016-93a5f69b52a4logseq____&quot;,536870928]],[logseq____&quot;^15logseq____&quot;,[154,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;logseq____&quot;,536870928]],[logseq____&quot;^15logseq____&quot;,[154,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870928]],[logseq____&quot;^15logseq____&quot;,[154,logseq____&quot;^Flogseq____&quot;,153,536870928]],[logseq____&quot;^15logseq____&quot;,[154,logseq____&quot;^Xlogseq____&quot;,149,536870928]],[logseq____&quot;^15logseq____&quot;,[154,logseq____&quot;^Vlogseq____&quot;,149,536870928]],[logseq____&quot;^15logseq____&quot;,[154,logseq____&quot;^Ulogseq____&quot;,149,536870928]],[logseq____&quot;^15logseq____&quot;,[154,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-0664-4df2-9394-fb1ff10cea02logseq____&quot;,536870928]],[logseq____&quot;^15logseq____&quot;,[155,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## 相关\\n\\nhttps://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/logseq____&quot;,536870928]],[logseq____&quot;^15logseq____&quot;,[155,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870928]],[logseq____&quot;^15logseq____&quot;,[155,logseq____&quot;^Flogseq____&quot;,154,536870928]],[logseq____&quot;^15logseq____&quot;,[155,logseq____&quot;^Xlogseq____&quot;,149,536870928]],[logseq____&quot;^15logseq____&quot;,[155,logseq____&quot;^Vlogseq____&quot;,149,536870928]],[logseq____&quot;^15logseq____&quot;,[155,logseq____&quot;^Ulogseq____&quot;,149,536870928]],[logseq____&quot;^15logseq____&quot;,[155,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870928]],[logseq____&quot;^15logseq____&quot;,[155,logseq____&quot;^Jlogseq____&quot;,[],536870928]],[logseq____&quot;^15logseq____&quot;,[155,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-8371-44b7-887a-bada20d075cdlogseq____&quot;,536870928]],[logseq____&quot;^15logseq____&quot;,[157,logseq____&quot;^Klogseq____&quot;,1723260458743,536870929]],[logseq____&quot;^15logseq____&quot;,[157,logseq____&quot;^@logseq____&quot;,false,536870929]],[logseq____&quot;^15logseq____&quot;,[157,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;reading list for andrej karpathys intro llmlogseq____&quot;,536870929]],[logseq____&quot;^15logseq____&quot;,[157,logseq____&quot;^11logseq____&quot;,logseq____&quot;reading list for andrej karpathys intro llmlogseq____&quot;,536870929]],[logseq____&quot;^15logseq____&quot;,[157,logseq____&quot;^Blogseq____&quot;,1723260458743,536870929]],[logseq____&quot;^15logseq____&quot;,[157,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-efa7-4e14-bc4c-f16dd101d16dlogseq____&quot;,536870929]],[logseq____&quot;^15logseq____&quot;,[158,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;logseq____&quot;,536870929]],[logseq____&quot;^15logseq____&quot;,[158,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870929]],[logseq____&quot;^15logseq____&quot;,[158,logseq____&quot;^Flogseq____&quot;,157,536870929]],[logseq____&quot;^15logseq____&quot;,[158,logseq____&quot;^Xlogseq____&quot;,157,536870929]],[logseq____&quot;^15logseq____&quot;,[158,logseq____&quot;^Vlogseq____&quot;,157,536870929]],[logseq____&quot;^15logseq____&quot;,[158,logseq____&quot;^Ulogseq____&quot;,157,536870929]],[logseq____&quot;^15logseq____&quot;,[158,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-2431-4a90-8de1-e955a3a5d08alogseq____&quot;,536870929]],[logseq____&quot;^15logseq____&quot;,[159,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;logseq____&quot;,536870929]],[logseq____&quot;^15logseq____&quot;,[159,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870929]],[logseq____&quot;^15logseq____&quot;,[159,logseq____&quot;^Flogseq____&quot;,158,536870929]],[logseq____&quot;^15logseq____&quot;,[159,logseq____&quot;^Xlogseq____&quot;,157,536870929]],[logseq____&quot;^15logseq____&quot;,[159,logseq____&quot;^Vlogseq____&quot;,157,536870929]],[logseq____&quot;^15logseq____&quot;,[159,logseq____&quot;^Ulogseq____&quot;,157,536870929]],[logseq____&quot;^15logseq____&quot;,[159,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-c2b3-4203-93fa-f09eee73e05blogseq____&quot;,536870929]],[logseq____&quot;^15logseq____&quot;,[161,logseq____&quot;^Klogseq____&quot;,1723260458760,536870930]],[logseq____&quot;^15logseq____&quot;,[161,logseq____&quot;^@logseq____&quot;,false,536870930]],[logseq____&quot;^15logseq____&quot;,[161,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;sinusoidal position encodinglogseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[161,logseq____&quot;^11logseq____&quot;,logseq____&quot;sinusoidal position encodinglogseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[161,logseq____&quot;^Blogseq____&quot;,1723260458760,536870930]],[logseq____&quot;^15logseq____&quot;,[161,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-95f1-40cb-a0f6-093aa02a79d9logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[162,logseq____&quot;^Klogseq____&quot;,1723260458760,536870930]],[logseq____&quot;^15logseq____&quot;,[162,logseq____&quot;^@logseq____&quot;,false,536870930]],[logseq____&quot;^15logseq____&quot;,[162,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;加性位置编码logseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[162,logseq____&quot;^11logseq____&quot;,logseq____&quot;加性位置编码logseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[162,logseq____&quot;^Blogseq____&quot;,1723260458760,536870930]],[logseq____&quot;^15logseq____&quot;,[162,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-3dee-4403-b873-39242f0a2edelogseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[163,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;来自经典论文attention is all you need，sinusoidal 对于 token 位置$$k=1, \\\\cdots, L$$， 维度$d = 1, \\\\cdots, d$logseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[163,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[163,logseq____&quot;^Flogseq____&quot;,161,536870930]],[logseq____&quot;^15logseq____&quot;,[163,logseq____&quot;^Xlogseq____&quot;,161,536870930]],[logseq____&quot;^15logseq____&quot;,[163,logseq____&quot;^Vlogseq____&quot;,161,536870930]],[logseq____&quot;^15logseq____&quot;,[163,logseq____&quot;^Ulogseq____&quot;,161,536870930]],[logseq____&quot;^15logseq____&quot;,[163,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-3fe2-410e-a1e8-74e70421d94blogseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[164,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;$$p_{k, 2i} = \\\\sin (k/10000^{2i/d}), p_{k, 2i+1} = \\\\cos(k/10000^{2i/d})$$logseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[164,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[164,logseq____&quot;^Flogseq____&quot;,163,536870930]],[logseq____&quot;^15logseq____&quot;,[164,logseq____&quot;^Xlogseq____&quot;,161,536870930]],[logseq____&quot;^15logseq____&quot;,[164,logseq____&quot;^Vlogseq____&quot;,161,536870930]],[logseq____&quot;^15logseq____&quot;,[164,logseq____&quot;^Ulogseq____&quot;,161,536870930]],[logseq____&quot;^15logseq____&quot;,[164,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-e79d-45d4-8549-f18e9a1942d2logseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[165,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;$L = 32, d = 128$时的sinusoidal positional encoding:logseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[165,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[165,logseq____&quot;^Flogseq____&quot;,164,536870930]],[logseq____&quot;^15logseq____&quot;,[165,logseq____&quot;^Xlogseq____&quot;,161,536870930]],[logseq____&quot;^15logseq____&quot;,[165,logseq____&quot;^Vlogseq____&quot;,161,536870930]],[logseq____&quot;^15logseq____&quot;,[165,logseq____&quot;^Ulogseq____&quot;,161,536870930]],[logseq____&quot;^15logseq____&quot;,[165,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-cfbd-4364-ab14-48ef2e939ccflogseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[166,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;黑色表示-1，白色表示1，灰色表示0logseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[166,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[166,logseq____&quot;^Flogseq____&quot;,165,536870930]],[logseq____&quot;^15logseq____&quot;,[166,logseq____&quot;^Xlogseq____&quot;,161,536870930]],[logseq____&quot;^15logseq____&quot;,[166,logseq____&quot;^Vlogseq____&quot;,165,536870930]],[logseq____&quot;^15logseq____&quot;,[166,logseq____&quot;^Ulogseq____&quot;,161,536870930]],[logseq____&quot;^15logseq____&quot;,[166,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-90ab-4ed3-8fe4-5860e358c6b3logseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[167,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;![sinoidual-positional-encoding_1685374418562_0.png](../assets/sinoidual-positional-encoding_1685374418562_0_1711464554944_0.png){:height 269, :width 903}logseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[167,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[167,logseq____&quot;^Flogseq____&quot;,166,536870930]],[logseq____&quot;^15logseq____&quot;,[167,logseq____&quot;^Xlogseq____&quot;,161,536870930]],[logseq____&quot;^15logseq____&quot;,[167,logseq____&quot;^Vlogseq____&quot;,165,536870930]],[logseq____&quot;^15logseq____&quot;,[167,logseq____&quot;^Ulogseq____&quot;,161,536870930]],[logseq____&quot;^15logseq____&quot;,[167,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-e20d-4dc0-838e-b196f392c24elogseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[168,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;#加性位置编码logseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[168,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[168,logseq____&quot;^Flogseq____&quot;,165,536870930]],[logseq____&quot;^15logseq____&quot;,[168,logseq____&quot;^Xlogseq____&quot;,161,536870930]],[logseq____&quot;^15logseq____&quot;,[168,logseq____&quot;^Vlogseq____&quot;,161,536870930]],[logseq____&quot;^15logseq____&quot;,[168,logseq____&quot;^Ulogseq____&quot;,161,536870930]],[logseq____&quot;^15logseq____&quot;,[168,logseq____&quot;^Ulogseq____&quot;,162,536870930]],[logseq____&quot;^15logseq____&quot;,[168,logseq____&quot;^Hlogseq____&quot;,162,536870930]],[logseq____&quot;^15logseq____&quot;,[168,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-e775-4be2-b6a8-9f4fdb70a177logseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[169,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;~```python\\n  class SinusoidalPositionalEmbedding(nn.Embedding):\\n      \\logseq____&quot;\\logseq____&quot;\\logseq____&quot;This module produces sinusoidal positional embeddings of any length.\\logseq____&quot;\\logseq____&quot;\\logseq____&quot;\\n      def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None) -logseq____&gt; None:\\n          super().__init__(num_positions, embedding_dim)\\n          self.weight = self._init_weight(self.weight)\\n\\n      @staticmethod\\n      def _init_weight(out: nn.Parameter) -logseq____&gt; nn.Parameter:\\n          \\logseq____&quot;\\logseq____&quot;\\logseq____&quot;\\n          Identical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in\\n          the 2nd half of the vector. [dim // 2:]\\n          \\logseq____&quot;\\logseq____&quot;\\logseq____&quot;\\n          n_pos, dim = out.shape\\n          position_enc = np.array(\\n              [[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)]\\n          )\\n          out.requires_grad = False  # set early to avoid an error in pytorch-1.8+\\n          sentinel = dim // 2 if dim % 2 == 0 else (dim // 2) + 1\\n          out[:, 0:sentinel] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\\n          out[:, sentinel:] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\\n          out.detach_()\\n          return out\\n\\n      @torch.no_grad()\\n      def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0) -logseq____&gt; torch.Tensor:\\n          \\logseq____&quot;\\logseq____&quot;\\logseq____&quot;`input_ids_shape` is expected to be [bsz x seqlen].\\logseq____&quot;\\logseq____&quot;\\logseq____&quot;\\n          bsz, seq_len = input_ids_shape[:2]\\n          positions = torch.arange(\\n              past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\\n          )\\n          return super().forward(positions)\\n```logseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[169,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[169,logseq____&quot;^Flogseq____&quot;,168,536870930]],[logseq____&quot;^15logseq____&quot;,[169,logseq____&quot;^Xlogseq____&quot;,161,536870930]],[logseq____&quot;^15logseq____&quot;,[169,logseq____&quot;^Vlogseq____&quot;,161,536870930]],[logseq____&quot;^15logseq____&quot;,[169,logseq____&quot;^Ulogseq____&quot;,161,536870930]],[logseq____&quot;^15logseq____&quot;,[169,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-2caf-4023-bf2a-5fdbf7bed80dlogseq____&quot;,536870930]],[logseq____&quot;^15logseq____&quot;,[171,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;speculative decoding使用小模型进行 decode 一定的 token，再将已decode 的 token 送回大模型（oracle model）进行检查，并将检查正确的tokens 加入已经解码的 tokens 中logseq____&quot;,536870931]],[logseq____&quot;^15logseq____&quot;,[171,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870931]],[logseq____&quot;^15logseq____&quot;,[171,logseq____&quot;^Flogseq____&quot;,116,536870931]],[logseq____&quot;^15logseq____&quot;,[171,logseq____&quot;^Xlogseq____&quot;,116,536870931]],[logseq____&quot;^15logseq____&quot;,[171,logseq____&quot;^Vlogseq____&quot;,116,536870931]],[logseq____&quot;^15logseq____&quot;,[171,logseq____&quot;^Ulogseq____&quot;,116,536870931]],[logseq____&quot;^15logseq____&quot;,[171,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-3c8c-420c-aa8c-d00454d8d061logseq____&quot;,536870931]],[logseq____&quot;^15logseq____&quot;,[172,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;~```python\\ndef generate(prompt: str, tokens_to_generate: int, n_draft: int = 8) -logseq____&gt; str:\\n    tokens: list[int] = tokenize(prompt)\\n    for i in range(tokens_to_generate):\\n        # generate `n_draft` draft tokens in the usual autoregressive way\\n        draft = tokens[:]\\n        for _ in range(n_draft):\\n            logits = draft_model.forward(draft)\\n            draft.append(argmax(logits[-1]))\\n        # run the draft tokens through the oracle model all at once\\n        logits = model.forward(draft)\\n        checked = logits[len(tokens) - 1 :].argmax(-1)\\n        # find the index of the first draft/oracle mismatch—welogseq____&apos;ll accept every\\n        # token before it\\n        # (the index might be past the end of the draft, if every draft token\\n        # was correct)\\n        n_accepted = next(\\n            idx + 1\\n            for idx, (checked, draft) in enumerate(\\n                # we add None here because the oracle model generates one extra\\n                # token (the prediction for the last draft token)\\n                zip(checked, draft[len(tokens) :] + [None])\\n            )\\n            if checked != draft\\n        )\\n        tokens.extend(checked[:n_accepted])\\n    return detokenize(tokens)\\n```logseq____&quot;,536870931]],[logseq____&quot;^15logseq____&quot;,[172,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870931]],[logseq____&quot;^15logseq____&quot;,[172,logseq____&quot;^Flogseq____&quot;,171,536870931]],[logseq____&quot;^15logseq____&quot;,[172,logseq____&quot;^Xlogseq____&quot;,116,536870931]],[logseq____&quot;^15logseq____&quot;,[172,logseq____&quot;^Vlogseq____&quot;,171,536870931]],[logseq____&quot;^15logseq____&quot;,[172,logseq____&quot;^Ulogseq____&quot;,116,536870931]],[logseq____&quot;^15logseq____&quot;,[172,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-60e6-487e-a26e-91388105bdb6logseq____&quot;,536870931]],[logseq____&quot;^15logseq____&quot;,[173,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;**优化**：使用概率阈值而不是 decode 数量控制draft 解码多少次logseq____&quot;,536870931]],[logseq____&quot;^15logseq____&quot;,[173,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870931]],[logseq____&quot;^15logseq____&quot;,[173,logseq____&quot;^Flogseq____&quot;,171,536870931]],[logseq____&quot;^15logseq____&quot;,[173,logseq____&quot;^Xlogseq____&quot;,116,536870931]],[logseq____&quot;^15logseq____&quot;,[173,logseq____&quot;^Vlogseq____&quot;,116,536870931]],[logseq____&quot;^15logseq____&quot;,[173,logseq____&quot;^Ulogseq____&quot;,116,536870931]],[logseq____&quot;^15logseq____&quot;,[173,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-7d92-4742-b71b-1366b002b407logseq____&quot;,536870931]],[logseq____&quot;^15logseq____&quot;,[174,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;~```python\\n\\ndef speculative_threshold(\\n    prompt: str,\\n    max_draft: int = 16,\\n    threshold: float = 0.4,\\n    threshold_all_correct_boost: float = 0.1,\\n):\\n\\n    tokens = encoder.encode(prompt)\\n    # homegrown KV cache setup has an `n_tokens` method that returns the length\\n    # of the cached sequence, and a `truncate` method to truncate that sequence\\n    # to a specific token\\n\\n    model_kv = gpt2.KVCache()\\n    draft_kv = gpt2.KVCache()\\n    while True:\\n        # generate up to `max_draft` draft tokens autoregressively, stopping\\n        # early if we fall below `threshold`\\n        draft = tokens[:]\\n        drafted_probs = []\\n        for _ in range(max_draft):\\n            logits = draft_model.forward(draft[draft_kv.n_tokens() :], draft_kv)\\n            next_id = np.argmax(logits[-1])\\n            next_prob = gpt2.softmax(logits[-1])[next_id]\\n            if not len(drafted_probs):\\n                drafted_probs.append(next_prob)\\n            else:\\n                drafted_probs.append(next_prob * drafted_probs[-1])\\n            draft.append(int(next_id))\\n            if drafted_probs[-1] logseq____&lt; threshold:\\n                break\\n        n_draft = len(draft) - len(tokens)\\n        # run draft tokens through the oracle model\\n        logits = model.forward(draft[model_kv.n_tokens() :], model_kv)\\n        checked = logits[-n_draft - 1 :].argmax(-1)\\n        n_accepted = next(\\n            idx + 1\\n            for idx, (checked, draft) in enumerate(\\n                zip(checked, draft[len(tokens) :] + [None])\\n            )\\n            if checked != draft\\n        )\\n        yield from checked[:n_accepted]\\n        tokens.extend(checked[:n_accepted])\\n        if n_accepted logseq____&lt;= n_draft:\\n            # adjust threshold towards prob of last accepted token, if we\\n            # ignored any draft tokens\\n            threshold = (threshold + drafted_probs[n_accepted - 1]) / 2\\n        else:\\n            # otherwise, lower the threshold slightly, welogseq____&apos;re probably being\\n            # too conservative\\n            threshold -= threshold_all_correct_boost\\n        # clamp to avoid pathological thresholds\\n        threshold = min(max(threshold, 0.05), 0.95)\\n        # donlogseq____&apos;t include oracle token in kv cache\\n        model_kv.truncate(len(tokens) - 1)\\n        draft_kv.truncate(len(tokens) - 1)\\n```logseq____&quot;,536870931]],[logseq____&quot;^15logseq____&quot;,[174,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870931]],[logseq____&quot;^15logseq____&quot;,[174,logseq____&quot;^Flogseq____&quot;,173,536870931]],[logseq____&quot;^15logseq____&quot;,[174,logseq____&quot;^Xlogseq____&quot;,116,536870931]],[logseq____&quot;^15logseq____&quot;,[174,logseq____&quot;^Vlogseq____&quot;,173,536870931]],[logseq____&quot;^15logseq____&quot;,[174,logseq____&quot;^Ulogseq____&quot;,116,536870931]],[logseq____&quot;^15logseq____&quot;,[174,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-26a5-46b3-bd26-3d1d46634a64logseq____&quot;,536870931]],[logseq____&quot;^15logseq____&quot;,[176,logseq____&quot;^Klogseq____&quot;,1723260458799,536870932]],[logseq____&quot;^15logseq____&quot;,[176,logseq____&quot;^@logseq____&quot;,false,536870932]],[logseq____&quot;^15logseq____&quot;,[176,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;transformer中的门控单元logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[176,logseq____&quot;^11logseq____&quot;,logseq____&quot;transformer中的门控单元logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[176,logseq____&quot;^Blogseq____&quot;,1723260458799,536870932]],[logseq____&quot;^15logseq____&quot;,[176,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-8563-442c-a01a-db7347ebcfeblogseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[177,logseq____&quot;^Klogseq____&quot;,1723260458800,536870932]],[logseq____&quot;^15logseq____&quot;,[177,logseq____&quot;^@logseq____&quot;,false,536870932]],[logseq____&quot;^15logseq____&quot;,[177,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;t5logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[177,logseq____&quot;^11logseq____&quot;,logseq____&quot;T5logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[177,logseq____&quot;^Blogseq____&quot;,1723260458800,536870932]],[logseq____&quot;^15logseq____&quot;,[177,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-0e06-4eff-97cd-d47c41f7a236logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[178,logseq____&quot;^Klogseq____&quot;,1723260458801,536870932]],[logseq____&quot;^15logseq____&quot;,[178,logseq____&quot;^@logseq____&quot;,false,536870932]],[logseq____&quot;^15logseq____&quot;,[178,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;mistrallogseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[178,logseq____&quot;^11logseq____&quot;,logseq____&quot;Mistrallogseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[178,logseq____&quot;^Blogseq____&quot;,1723260458801,536870932]],[logseq____&quot;^15logseq____&quot;,[178,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-b8c6-4187-8a76-09d49b1b4f9blogseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[179,logseq____&quot;^Klogseq____&quot;,1723260458802,536870932]],[logseq____&quot;^15logseq____&quot;,[179,logseq____&quot;^@logseq____&quot;,false,536870932]],[logseq____&quot;^15logseq____&quot;,[179,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;llamalogseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[179,logseq____&quot;^11logseq____&quot;,logseq____&quot;LLAMAlogseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[179,logseq____&quot;^Blogseq____&quot;,1723260458802,536870932]],[logseq____&quot;^15logseq____&quot;,[179,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-cc87-49ad-96fe-5e7253cc07d5logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[180,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## GLU 门控线性单元logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[180,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[180,logseq____&quot;^Flogseq____&quot;,185,536870932]],[logseq____&quot;^15logseq____&quot;,[180,logseq____&quot;^Xlogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[180,logseq____&quot;^Vlogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[180,logseq____&quot;^Ulogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[180,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870932]],[logseq____&quot;^15logseq____&quot;,[180,logseq____&quot;^Jlogseq____&quot;,[],536870932]],[logseq____&quot;^15logseq____&quot;,[180,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-0b47-4176-841f-d6897135862elogseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[181,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;GAU 在线性化注意力机制，速度更快，内存更省。logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[181,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[181,logseq____&quot;^Flogseq____&quot;,192,536870932]],[logseq____&quot;^15logseq____&quot;,[181,logseq____&quot;^Xlogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[181,logseq____&quot;^Vlogseq____&quot;,188,536870932]],[logseq____&quot;^15logseq____&quot;,[181,logseq____&quot;^Ulogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[181,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-0f04-40db-a6d6-29c854ebd617logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[182,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;$$FFN(x, W_1, W_2, b_1, b_2) = ReLU(xW_1 + b_1) W_2 + b_2$$logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[182,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[182,logseq____&quot;^Flogseq____&quot;,185,536870932]],[logseq____&quot;^15logseq____&quot;,[182,logseq____&quot;^Xlogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[182,logseq____&quot;^Vlogseq____&quot;,185,536870932]],[logseq____&quot;^15logseq____&quot;,[182,logseq____&quot;^Ulogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[182,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-5d4d-49b5-94ff-d8d6bbc1e6b5logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[183,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;### SwiGLUlogseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[183,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[183,logseq____&quot;^Flogseq____&quot;,190,536870932]],[logseq____&quot;^15logseq____&quot;,[183,logseq____&quot;^Xlogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[183,logseq____&quot;^Vlogseq____&quot;,180,536870932]],[logseq____&quot;^15logseq____&quot;,[183,logseq____&quot;^Ulogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[183,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,3],536870932]],[logseq____&quot;^15logseq____&quot;,[183,logseq____&quot;^Jlogseq____&quot;,[],536870932]],[logseq____&quot;^15logseq____&quot;,[183,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-e5e2-4e27-9ffa-b75e49bf07e2logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[184,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;按 [[T5]] 的实现，FFN 为无 bias 的版本logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[184,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[184,logseq____&quot;^Flogseq____&quot;,182,536870932]],[logseq____&quot;^15logseq____&quot;,[184,logseq____&quot;^Xlogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[184,logseq____&quot;^Vlogseq____&quot;,185,536870932]],[logseq____&quot;^15logseq____&quot;,[184,logseq____&quot;^Ulogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[184,logseq____&quot;^Ulogseq____&quot;,177,536870932]],[logseq____&quot;^15logseq____&quot;,[184,logseq____&quot;^Hlogseq____&quot;,177,536870932]],[logseq____&quot;^15logseq____&quot;,[184,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-87ca-45c5-b619-8dacfd41b804logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[185,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;传统的 transformer 中在注意力机制之间使用 FFN 进行连接，logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[185,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[185,logseq____&quot;^Flogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[185,logseq____&quot;^Xlogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[185,logseq____&quot;^Vlogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[185,logseq____&quot;^Ulogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[185,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-cab1-4274-a2c5-0f8e5d9daf99logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[186,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;GAU 由谷歌![Transformer Quality in Linear Time](https://arxiv.org/pdf/2202.10447.pdf) 论文中提出，结合 GLU 与 attention。logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[186,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[186,logseq____&quot;^Flogseq____&quot;,188,536870932]],[logseq____&quot;^15logseq____&quot;,[186,logseq____&quot;^Xlogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[186,logseq____&quot;^Vlogseq____&quot;,188,536870932]],[logseq____&quot;^15logseq____&quot;,[186,logseq____&quot;^Ulogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[186,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-0357-4c28-aa9d-e76bd56e2c40logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[187,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;$$GEGLU(x, W, V, b, c) = GELU(xW+b)\\\\odot(xV+c)$$logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[187,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[187,logseq____&quot;^Flogseq____&quot;,190,536870932]],[logseq____&quot;^15logseq____&quot;,[187,logseq____&quot;^Xlogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[187,logseq____&quot;^Vlogseq____&quot;,190,536870932]],[logseq____&quot;^15logseq____&quot;,[187,logseq____&quot;^Ulogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[187,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-583b-4d12-84ac-88c0718d3891logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[188,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## GAU 门控注意力单元logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[188,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[188,logseq____&quot;^Flogseq____&quot;,180,536870932]],[logseq____&quot;^15logseq____&quot;,[188,logseq____&quot;^Xlogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[188,logseq____&quot;^Vlogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[188,logseq____&quot;^Ulogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[188,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870932]],[logseq____&quot;^15logseq____&quot;,[188,logseq____&quot;^Jlogseq____&quot;,[],536870932]],[logseq____&quot;^15logseq____&quot;,[188,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-94bb-4307-9f7f-20ddba514e8clogseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[189,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;$$ \\\\text{SwiGLU}\\\\left(x, W, V, b, c, \\\\beta\\\\right) = \\\\text{Swish}_{\\\\beta}\\\\left(xW + b\\\\right) \\\\otimes \\\\left(xV + c\\\\right) $$logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[189,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[189,logseq____&quot;^Flogseq____&quot;,183,536870932]],[logseq____&quot;^15logseq____&quot;,[189,logseq____&quot;^Xlogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[189,logseq____&quot;^Vlogseq____&quot;,183,536870932]],[logseq____&quot;^15logseq____&quot;,[189,logseq____&quot;^Ulogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[189,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-8ee4-43e1-88f1-5b0beb90b808logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[190,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;### GEGLUlogseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[190,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[190,logseq____&quot;^Flogseq____&quot;,191,536870932]],[logseq____&quot;^15logseq____&quot;,[190,logseq____&quot;^Xlogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[190,logseq____&quot;^Vlogseq____&quot;,180,536870932]],[logseq____&quot;^15logseq____&quot;,[190,logseq____&quot;^Ulogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[190,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,3],536870932]],[logseq____&quot;^15logseq____&quot;,[190,logseq____&quot;^Jlogseq____&quot;,[],536870932]],[logseq____&quot;^15logseq____&quot;,[190,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-b31a-4892-a33f-66d67b7f93aelogseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[191,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;GLU  为门控线性单元，现在被广泛地使用于大模型中用于替换 FFN，如 [[LLAMA]]、 [[Mistral]] 中均使用了 GLU 取代了 FFNlogseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[191,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[191,logseq____&quot;^Flogseq____&quot;,180,536870932]],[logseq____&quot;^15logseq____&quot;,[191,logseq____&quot;^Xlogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[191,logseq____&quot;^Vlogseq____&quot;,180,536870932]],[logseq____&quot;^15logseq____&quot;,[191,logseq____&quot;^Ulogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[191,logseq____&quot;^Ulogseq____&quot;,178,536870932]],[logseq____&quot;^15logseq____&quot;,[191,logseq____&quot;^Ulogseq____&quot;,179,536870932]],[logseq____&quot;^15logseq____&quot;,[191,logseq____&quot;^Hlogseq____&quot;,178,536870932]],[logseq____&quot;^15logseq____&quot;,[191,logseq____&quot;^Hlogseq____&quot;,179,536870932]],[logseq____&quot;^15logseq____&quot;,[191,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-9103-4180-a5db-d82354931c3elogseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[192,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;![gau.png](../assets/gau.png){:height 253, :width 856}logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[192,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[192,logseq____&quot;^Flogseq____&quot;,186,536870932]],[logseq____&quot;^15logseq____&quot;,[192,logseq____&quot;^Xlogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[192,logseq____&quot;^Vlogseq____&quot;,188,536870932]],[logseq____&quot;^15logseq____&quot;,[192,logseq____&quot;^Ulogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[192,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-80d8-48b0-9507-b151f12248aclogseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[193,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;![glu_results.png](../assets/glu_results_1711382219077_0.png)logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[193,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[193,logseq____&quot;^Flogseq____&quot;,195,536870932]],[logseq____&quot;^15logseq____&quot;,[193,logseq____&quot;^Xlogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[193,logseq____&quot;^Vlogseq____&quot;,195,536870932]],[logseq____&quot;^15logseq____&quot;,[193,logseq____&quot;^Ulogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[193,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-96bb-4341-9f69-69c454ea14f8logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[194,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;$$\\n\\n\\\\begin{equation}\\\\boldsymbol{O}=(\\\\boldsymbol{U}\\\\odot\\\\boldsymbol{V})\\\\boldsymbol{W}_o,\\\\quad \\\\boldsymbol{U}=\\\\phi_u(\\\\boldsymbol{X}\\\\boldsymbol{W}_u),\\\\quad\\\\boldsymbol{V}=\\\\phi_v(\\\\boldsymbol{X}\\\\boldsymbol{W}_v)\\\\end{equation}\\n\\n$$logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[194,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[194,logseq____&quot;^Flogseq____&quot;,186,536870932]],[logseq____&quot;^15logseq____&quot;,[194,logseq____&quot;^Xlogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[194,logseq____&quot;^Vlogseq____&quot;,186,536870932]],[logseq____&quot;^15logseq____&quot;,[194,logseq____&quot;^Ulogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[194,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-7ee6-476c-9daf-9966d2be0e5blogseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[195,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;$GELU =x \\\\Phi(x)$ 与 $Swish_\\\\beta(x) = x \\\\sigma(\\\\beta x)$ 为两类激活函数。Google 曾经使用多种激活函数的变种进行门控线性单元的实验，并指出这些变种在迁移学习的设置下，对于预训练中的去噪的目标函数，产生的perplexity更低，且下游任务的效果更好。logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[195,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[195,logseq____&quot;^Flogseq____&quot;,183,536870932]],[logseq____&quot;^15logseq____&quot;,[195,logseq____&quot;^Xlogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[195,logseq____&quot;^Vlogseq____&quot;,180,536870932]],[logseq____&quot;^15logseq____&quot;,[195,logseq____&quot;^Ulogseq____&quot;,176,536870932]],[logseq____&quot;^15logseq____&quot;,[195,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-d11e-43ec-9ddf-07b51522ba0flogseq____&quot;,536870932]],[logseq____&quot;^15logseq____&quot;,[197,logseq____&quot;^Klogseq____&quot;,1723260458837,536870933]],[logseq____&quot;^15logseq____&quot;,[197,logseq____&quot;^@logseq____&quot;,false,536870933]],[logseq____&quot;^15logseq____&quot;,[197,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;位置编码logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[197,logseq____&quot;^11logseq____&quot;,logseq____&quot;位置编码logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[197,logseq____&quot;^Blogseq____&quot;,1723260458837,536870933]],[logseq____&quot;^15logseq____&quot;,[197,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-64be-49eb-907e-11bae09381cdlogseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[198,logseq____&quot;^Klogseq____&quot;,1723260458838,536870933]],[logseq____&quot;^15logseq____&quot;,[198,logseq____&quot;^@logseq____&quot;,false,536870933]],[logseq____&quot;^15logseq____&quot;,[198,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;transformer xllogseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[198,logseq____&quot;^11logseq____&quot;,logseq____&quot;Transformer XLlogseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[198,logseq____&quot;^Blogseq____&quot;,1723260458838,536870933]],[logseq____&quot;^15logseq____&quot;,[198,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-a9bc-4324-a267-0aacd0480d22logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[199,logseq____&quot;^Klogseq____&quot;,1723260458839,536870933]],[logseq____&quot;^15logseq____&quot;,[199,logseq____&quot;^@logseq____&quot;,false,536870933]],[logseq____&quot;^15logseq____&quot;,[199,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;ropelogseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[199,logseq____&quot;^11logseq____&quot;,logseq____&quot;RoPElogseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[199,logseq____&quot;^Blogseq____&quot;,1723260458839,536870933]],[logseq____&quot;^15logseq____&quot;,[199,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-5647-461c-95f7-4d6f91e5251flogseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[200,logseq____&quot;^Klogseq____&quot;,1723260458840,536870933]],[logseq____&quot;^15logseq____&quot;,[200,logseq____&quot;^@logseq____&quot;,false,536870933]],[logseq____&quot;^15logseq____&quot;,[200,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;ntk-ropelogseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[200,logseq____&quot;^11logseq____&quot;,logseq____&quot;NTK-RoPElogseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[200,logseq____&quot;^Blogseq____&quot;,1723260458840,536870933]],[logseq____&quot;^15logseq____&quot;,[200,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-ba12-4c6a-8ce8-9f77a0b23a57logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[201,logseq____&quot;^Klogseq____&quot;,1723260458841,536870933]],[logseq____&quot;^15logseq____&quot;,[201,logseq____&quot;^@logseq____&quot;,false,536870933]],[logseq____&quot;^15logseq____&quot;,[201,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;bloomlogseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[201,logseq____&quot;^11logseq____&quot;,logseq____&quot;BLOOMlogseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[201,logseq____&quot;^Blogseq____&quot;,1723260458841,536870933]],[logseq____&quot;^15logseq____&quot;,[201,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-96f7-4cf0-9b04-bd64ed49f4e2logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[202,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;首次用在 BERT中的[[sinusoidal position encoding]]logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[202,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[202,logseq____&quot;^Flogseq____&quot;,212,536870933]],[logseq____&quot;^15logseq____&quot;,[202,logseq____&quot;^Xlogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[202,logseq____&quot;^Vlogseq____&quot;,212,536870933]],[logseq____&quot;^15logseq____&quot;,[202,logseq____&quot;^Ulogseq____&quot;,161,536870933]],[logseq____&quot;^15logseq____&quot;,[202,logseq____&quot;^Ulogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[202,logseq____&quot;^Hlogseq____&quot;,161,536870933]],[logseq____&quot;^15logseq____&quot;,[202,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-eb91-4f92-9714-a7826cf2defflogseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[203,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;[Extending context size via RoPE scaling](https://github.com/ggerganov/llama.cpp/discussions/1965){:height 34, :width 297}logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[203,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[203,logseq____&quot;^Flogseq____&quot;,216,536870933]],[logseq____&quot;^15logseq____&quot;,[203,logseq____&quot;^Xlogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[203,logseq____&quot;^Vlogseq____&quot;,206,536870933]],[logseq____&quot;^15logseq____&quot;,[203,logseq____&quot;^Ulogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[203,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-d283-4c36-b1b1-e7e75c915c27logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[204,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;### 重参数化logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[204,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[204,logseq____&quot;^Flogseq____&quot;,217,536870933]],[logseq____&quot;^15logseq____&quot;,[204,logseq____&quot;^Xlogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[204,logseq____&quot;^Vlogseq____&quot;,208,536870933]],[logseq____&quot;^15logseq____&quot;,[204,logseq____&quot;^Ulogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[204,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,3],536870933]],[logseq____&quot;^15logseq____&quot;,[204,logseq____&quot;^Jlogseq____&quot;,[],536870933]],[logseq____&quot;^15logseq____&quot;,[204,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-80a6-4a9c-b9a6-35621732e8bflogseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[205,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;### 可学习式logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[205,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[205,logseq____&quot;^Flogseq____&quot;,212,536870933]],[logseq____&quot;^15logseq____&quot;,[205,logseq____&quot;^Xlogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[205,logseq____&quot;^Vlogseq____&quot;,213,536870933]],[logseq____&quot;^15logseq____&quot;,[205,logseq____&quot;^Ulogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[205,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,3],536870933]],[logseq____&quot;^15logseq____&quot;,[205,logseq____&quot;^Jlogseq____&quot;,[],536870933]],[logseq____&quot;^15logseq____&quot;,[205,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-edab-4fe1-afca-03608f686bd6logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[206,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;### RoPElogseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[206,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[206,logseq____&quot;^Flogseq____&quot;,204,536870933]],[logseq____&quot;^15logseq____&quot;,[206,logseq____&quot;^Xlogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[206,logseq____&quot;^Vlogseq____&quot;,208,536870933]],[logseq____&quot;^15logseq____&quot;,[206,logseq____&quot;^Ulogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[206,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,3],536870933]],[logseq____&quot;^15logseq____&quot;,[206,logseq____&quot;^Jlogseq____&quot;,[],536870933]],[logseq____&quot;^15logseq____&quot;,[206,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-338c-4a67-bb2c-0f8f7945507dlogseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[207,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;$$q_ik_j^T=(x_i+p_i)W_QW_K^T(x_j+p_j)^T=(x_iW_Q+p_iW_Q)(W_K^Tx_j^T+W_K^Tp_j^T)$$logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[207,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[207,logseq____&quot;^Flogseq____&quot;,208,536870933]],[logseq____&quot;^15logseq____&quot;,[207,logseq____&quot;^Xlogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[207,logseq____&quot;^Vlogseq____&quot;,208,536870933]],[logseq____&quot;^15logseq____&quot;,[207,logseq____&quot;^Ulogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[207,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-6aae-43e8-bed3-0a76c9bfadfblogseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[208,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## 相对位置编码logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[208,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[208,logseq____&quot;^Flogseq____&quot;,213,536870933]],[logseq____&quot;^15logseq____&quot;,[208,logseq____&quot;^Xlogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[208,logseq____&quot;^Vlogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[208,logseq____&quot;^Ulogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[208,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870933]],[logseq____&quot;^15logseq____&quot;,[208,logseq____&quot;^Jlogseq____&quot;,[],536870933]],[logseq____&quot;^15logseq____&quot;,[208,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-6ce1-4050-be1c-c5041a22b181logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[209,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;[[BLOOM]] 176B 给 Attention 加上 ALiBi bias 矩阵 (不同注意力头系数不同)logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[209,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[209,logseq____&quot;^Flogseq____&quot;,220,536870933]],[logseq____&quot;^15logseq____&quot;,[209,logseq____&quot;^Xlogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[209,logseq____&quot;^Vlogseq____&quot;,220,536870933]],[logseq____&quot;^15logseq____&quot;,[209,logseq____&quot;^Ulogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[209,logseq____&quot;^Ulogseq____&quot;,201,536870933]],[logseq____&quot;^15logseq____&quot;,[209,logseq____&quot;^Hlogseq____&quot;,201,536870933]],[logseq____&quot;^15logseq____&quot;,[209,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-f039-48d4-8565-8f00b4cca976logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[210,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[210,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[210,logseq____&quot;^Flogseq____&quot;,208,536870933]],[logseq____&quot;^15logseq____&quot;,[210,logseq____&quot;^Xlogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[210,logseq____&quot;^Vlogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[210,logseq____&quot;^Ulogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[210,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-b91c-478e-b2dc-6c9942785a77logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[211,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;[[ALiBi, Kerple and Sandwich]]logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[211,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[211,logseq____&quot;^Flogseq____&quot;,209,536870933]],[logseq____&quot;^15logseq____&quot;,[211,logseq____&quot;^Xlogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[211,logseq____&quot;^Vlogseq____&quot;,220,536870933]],[logseq____&quot;^15logseq____&quot;,[211,logseq____&quot;^Ulogseq____&quot;,26,536870933]],[logseq____&quot;^15logseq____&quot;,[211,logseq____&quot;^Ulogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[211,logseq____&quot;^Hlogseq____&quot;,26,536870933]],[logseq____&quot;^15logseq____&quot;,[211,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-a68c-4fe4-8a45-ea74a51f8e6alogseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[212,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;### 三角式(Sinusoidal)logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[212,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[212,logseq____&quot;^Flogseq____&quot;,213,536870933]],[logseq____&quot;^15logseq____&quot;,[212,logseq____&quot;^Xlogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[212,logseq____&quot;^Vlogseq____&quot;,213,536870933]],[logseq____&quot;^15logseq____&quot;,[212,logseq____&quot;^Ulogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[212,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,3],536870933]],[logseq____&quot;^15logseq____&quot;,[212,logseq____&quot;^Jlogseq____&quot;,[],536870933]],[logseq____&quot;^15logseq____&quot;,[212,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-c08a-4b94-8856-cc222ade8dfclogseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[213,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## 绝对位置编码logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[213,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[213,logseq____&quot;^Flogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[213,logseq____&quot;^Xlogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[213,logseq____&quot;^Vlogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[213,logseq____&quot;^Ulogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[213,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870933]],[logseq____&quot;^15logseq____&quot;,[213,logseq____&quot;^Jlogseq____&quot;,[],536870933]],[logseq____&quot;^15logseq____&quot;,[213,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-1a76-434c-a2e8-af606cfb8475logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[214,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;BERT 式logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[214,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[214,logseq____&quot;^Flogseq____&quot;,205,536870933]],[logseq____&quot;^15logseq____&quot;,[214,logseq____&quot;^Xlogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[214,logseq____&quot;^Vlogseq____&quot;,205,536870933]],[logseq____&quot;^15logseq____&quot;,[214,logseq____&quot;^Ulogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[214,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-9e02-4aa9-872b-f2d0df93122blogseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[215,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;[[RoPE]]logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[215,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[215,logseq____&quot;^Flogseq____&quot;,206,536870933]],[logseq____&quot;^15logseq____&quot;,[215,logseq____&quot;^Xlogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[215,logseq____&quot;^Vlogseq____&quot;,206,536870933]],[logseq____&quot;^15logseq____&quot;,[215,logseq____&quot;^Ulogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[215,logseq____&quot;^Ulogseq____&quot;,199,536870933]],[logseq____&quot;^15logseq____&quot;,[215,logseq____&quot;^Hlogseq____&quot;,199,536870933]],[logseq____&quot;^15logseq____&quot;,[215,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-2bdd-4ba3-acf6-8e4c887821b3logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[216,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;[[NTK-RoPE]]logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[216,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[216,logseq____&quot;^Flogseq____&quot;,215,536870933]],[logseq____&quot;^15logseq____&quot;,[216,logseq____&quot;^Xlogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[216,logseq____&quot;^Vlogseq____&quot;,206,536870933]],[logseq____&quot;^15logseq____&quot;,[216,logseq____&quot;^Ulogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[216,logseq____&quot;^Ulogseq____&quot;,200,536870933]],[logseq____&quot;^15logseq____&quot;,[216,logseq____&quot;^Hlogseq____&quot;,200,536870933]],[logseq____&quot;^15logseq____&quot;,[216,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-5b26-40ed-91ae-45bf3d77e5delogseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[217,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;google: $$a_{i,j} = softmax(x_iW_Q(x_jW_K + R^K_{i,j}))$$logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[217,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[217,logseq____&quot;^Flogseq____&quot;,207,536870933]],[logseq____&quot;^15logseq____&quot;,[217,logseq____&quot;^Xlogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[217,logseq____&quot;^Vlogseq____&quot;,208,536870933]],[logseq____&quot;^15logseq____&quot;,[217,logseq____&quot;^Ulogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[217,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-7e02-4c13-934f-95ed0d6bebbelogseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[218,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;$$x_iW_QW_K^Tx_j^T + x_iW_QW_K^TR^T_{i-j} + uW_QW_K^Tx_j^T + vW_QW^T_KR^T_{i-j}$$logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[218,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[218,logseq____&quot;^Flogseq____&quot;,219,536870933]],[logseq____&quot;^15logseq____&quot;,[218,logseq____&quot;^Xlogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[218,logseq____&quot;^Vlogseq____&quot;,219,536870933]],[logseq____&quot;^15logseq____&quot;,[218,logseq____&quot;^Ulogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[218,logseq____&quot;^Ulogseq____&quot;,198,536870933]],[logseq____&quot;^15logseq____&quot;,[218,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-53ac-4733-aa90-64c2c87dddeblogseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[219,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;[[Transformer XL]] 提出了一种相对位置编码，对 key 与 query 的点积操作进行了重参数化logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[219,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[219,logseq____&quot;^Flogseq____&quot;,204,536870933]],[logseq____&quot;^15logseq____&quot;,[219,logseq____&quot;^Xlogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[219,logseq____&quot;^Vlogseq____&quot;,204,536870933]],[logseq____&quot;^15logseq____&quot;,[219,logseq____&quot;^Ulogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[219,logseq____&quot;^Ulogseq____&quot;,198,536870933]],[logseq____&quot;^15logseq____&quot;,[219,logseq____&quot;^Hlogseq____&quot;,198,536870933]],[logseq____&quot;^15logseq____&quot;,[219,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-0ee6-43c0-85aa-81b844353bf8logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[220,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;### ALiBilogseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[220,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[220,logseq____&quot;^Flogseq____&quot;,206,536870933]],[logseq____&quot;^15logseq____&quot;,[220,logseq____&quot;^Xlogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[220,logseq____&quot;^Vlogseq____&quot;,208,536870933]],[logseq____&quot;^15logseq____&quot;,[220,logseq____&quot;^Ulogseq____&quot;,197,536870933]],[logseq____&quot;^15logseq____&quot;,[220,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,3],536870933]],[logseq____&quot;^15logseq____&quot;,[220,logseq____&quot;^Jlogseq____&quot;,[],536870933]],[logseq____&quot;^15logseq____&quot;,[220,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-48b6-49f1-b979-4b70bc92eea4logseq____&quot;,536870933]],[logseq____&quot;^15logseq____&quot;,[222,logseq____&quot;^Klogseq____&quot;,1723260458874,536870934]],[logseq____&quot;^15logseq____&quot;,[222,logseq____&quot;^@logseq____&quot;,false,536870934]],[logseq____&quot;^15logseq____&quot;,[222,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;gemmalogseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[222,logseq____&quot;^11logseq____&quot;,logseq____&quot;Gemmalogseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[222,logseq____&quot;^Blogseq____&quot;,1723260458874,536870934]],[logseq____&quot;^15logseq____&quot;,[222,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-3501-42d1-b007-9d7e509924fblogseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[223,logseq____&quot;^Klogseq____&quot;,1723260458876,536870934]],[logseq____&quot;^15logseq____&quot;,[223,logseq____&quot;^@logseq____&quot;,false,536870934]],[logseq____&quot;^15logseq____&quot;,[223,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;大模型 transformerlogseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[223,logseq____&quot;^11logseq____&quot;,logseq____&quot;大模型 transformerlogseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[223,logseq____&quot;^Blogseq____&quot;,1723260458876,536870934]],[logseq____&quot;^15logseq____&quot;,[223,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-b0f2-42c5-a11a-2ad8e901cc65logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[224,logseq____&quot;^Klogseq____&quot;,1723260458878,536870934]],[logseq____&quot;^15logseq____&quot;,[224,logseq____&quot;^@logseq____&quot;,false,536870934]],[logseq____&quot;^15logseq____&quot;,[224,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;层归一化logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[224,logseq____&quot;^11logseq____&quot;,logseq____&quot;层归一化logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[224,logseq____&quot;^Blogseq____&quot;,1723260458878,536870934]],[logseq____&quot;^15logseq____&quot;,[224,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-4d28-4dc9-9990-3d6c2e256b78logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[225,logseq____&quot;^Klogseq____&quot;,1723260458873,536870934]],[logseq____&quot;^15logseq____&quot;,[225,logseq____&quot;^@logseq____&quot;,false,536870934]],[logseq____&quot;^15logseq____&quot;,[225,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;qwenlogseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[225,logseq____&quot;^11logseq____&quot;,logseq____&quot;qwenlogseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[225,logseq____&quot;^Blogseq____&quot;,1723260458873,536870934]],[logseq____&quot;^15logseq____&quot;,[225,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-eac7-4c9a-bd79-78282b136c86logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[226,logseq____&quot;^Klogseq____&quot;,1723260458874,536870934]],[logseq____&quot;^15logseq____&quot;,[226,logseq____&quot;^@logseq____&quot;,false,536870934]],[logseq____&quot;^15logseq____&quot;,[226,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;groklogseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[226,logseq____&quot;^11logseq____&quot;,logseq____&quot;Groklogseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[226,logseq____&quot;^Blogseq____&quot;,1723260458874,536870934]],[logseq____&quot;^15logseq____&quot;,[226,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-b2be-4761-8e39-c20349db19dblogseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[227,logseq____&quot;^Klogseq____&quot;,1723260458875,536870934]],[logseq____&quot;^15logseq____&quot;,[227,logseq____&quot;^@logseq____&quot;,false,536870934]],[logseq____&quot;^15logseq____&quot;,[227,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;glmlogseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[227,logseq____&quot;^11logseq____&quot;,logseq____&quot;GLMlogseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[227,logseq____&quot;^Blogseq____&quot;,1723260458875,536870934]],[logseq____&quot;^15logseq____&quot;,[227,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-01a9-4b84-8f9b-1b7f28f246a7logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[228,logseq____&quot;^Klogseq____&quot;,1723260458876,536870934]],[logseq____&quot;^15logseq____&quot;,[228,logseq____&quot;^@logseq____&quot;,false,536870934]],[logseq____&quot;^15logseq____&quot;,[228,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;llama 2logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[228,logseq____&quot;^11logseq____&quot;,logseq____&quot;LLAMA 2logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[228,logseq____&quot;^Blogseq____&quot;,1723260458876,536870934]],[logseq____&quot;^15logseq____&quot;,[228,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-f1a9-45ed-bbc0-627b1f785b30logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[229,logseq____&quot;^Klogseq____&quot;,1723260458875,536870934]],[logseq____&quot;^15logseq____&quot;,[229,logseq____&quot;^@logseq____&quot;,false,536870934]],[logseq____&quot;^15logseq____&quot;,[229,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;palmlogseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[229,logseq____&quot;^11logseq____&quot;,logseq____&quot;PaLMlogseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[229,logseq____&quot;^Blogseq____&quot;,1723260458875,536870934]],[logseq____&quot;^15logseq____&quot;,[229,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-73d7-4e29-b58f-fa2e441e5ce0logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[230,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;|模型 | 位置编码|层归一化| 门控单元| Attention|  tokenizer| MoE|\\n|---    |---------|--------| ----------|-----------|----|---|\\n| [[LLAMA]] |RoPE| PreNorm| SwiGLU| | |\\n| [[LLAMA 2]] |RoPE| PreNorm, RMSNorm| SwiGLU|GQA||no|\\n| [[Mistral]] |RoPE| PreNorm| SwiGLU| GQA||no|\\n| [[PaLM]] |RoPE|PreNorm| SwiGLU| MQA||no|\\n|[[BLOOM]]|ALiBi|PreNorm，Embedding LayerNorm|GeLU, FFN||no|\\n|[[GLM]]|RoPE|||GEGLU||no|\\n| [[Grok]] |RoPE|PreNorm||MQA||yes||\\n|[[Gemma]]|RoPE| SandwichNorm| GeGLU|MQA||no|\\n|[[qwen]]|RoPE|PreNorm|SwiGLU|||no|logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[230,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[230,logseq____&quot;^Flogseq____&quot;,223,536870934]],[logseq____&quot;^15logseq____&quot;,[230,logseq____&quot;^Xlogseq____&quot;,223,536870934]],[logseq____&quot;^15logseq____&quot;,[230,logseq____&quot;^Vlogseq____&quot;,223,536870934]],[logseq____&quot;^15logseq____&quot;,[230,logseq____&quot;^Ulogseq____&quot;,178,536870934]],[logseq____&quot;^15logseq____&quot;,[230,logseq____&quot;^Ulogseq____&quot;,179,536870934]],[logseq____&quot;^15logseq____&quot;,[230,logseq____&quot;^Ulogseq____&quot;,201,536870934]],[logseq____&quot;^15logseq____&quot;,[230,logseq____&quot;^Ulogseq____&quot;,222,536870934]],[logseq____&quot;^15logseq____&quot;,[230,logseq____&quot;^Ulogseq____&quot;,223,536870934]],[logseq____&quot;^15logseq____&quot;,[230,logseq____&quot;^Ulogseq____&quot;,225,536870934]],[logseq____&quot;^15logseq____&quot;,[230,logseq____&quot;^Ulogseq____&quot;,226,536870934]],[logseq____&quot;^15logseq____&quot;,[230,logseq____&quot;^Ulogseq____&quot;,227,536870934]],[logseq____&quot;^15logseq____&quot;,[230,logseq____&quot;^Ulogseq____&quot;,228,536870934]],[logseq____&quot;^15logseq____&quot;,[230,logseq____&quot;^Ulogseq____&quot;,229,536870934]],[logseq____&quot;^15logseq____&quot;,[230,logseq____&quot;^Hlogseq____&quot;,178,536870934]],[logseq____&quot;^15logseq____&quot;,[230,logseq____&quot;^Hlogseq____&quot;,179,536870934]],[logseq____&quot;^15logseq____&quot;,[230,logseq____&quot;^Hlogseq____&quot;,201,536870934]],[logseq____&quot;^15logseq____&quot;,[230,logseq____&quot;^Hlogseq____&quot;,222,536870934]],[logseq____&quot;^15logseq____&quot;,[230,logseq____&quot;^Hlogseq____&quot;,225,536870934]],[logseq____&quot;^15logseq____&quot;,[230,logseq____&quot;^Hlogseq____&quot;,226,536870934]],[logseq____&quot;^15logseq____&quot;,[230,logseq____&quot;^Hlogseq____&quot;,227,536870934]],[logseq____&quot;^15logseq____&quot;,[230,logseq____&quot;^Hlogseq____&quot;,228,536870934]],[logseq____&quot;^15logseq____&quot;,[230,logseq____&quot;^Hlogseq____&quot;,229,536870934]],[logseq____&quot;^15logseq____&quot;,[230,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-c589-4529-b7be-feb9fd86a7c9logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[231,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;[[位置编码]]logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[231,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[231,logseq____&quot;^Flogseq____&quot;,230,536870934]],[logseq____&quot;^15logseq____&quot;,[231,logseq____&quot;^Xlogseq____&quot;,223,536870934]],[logseq____&quot;^15logseq____&quot;,[231,logseq____&quot;^Vlogseq____&quot;,223,536870934]],[logseq____&quot;^15logseq____&quot;,[231,logseq____&quot;^Ulogseq____&quot;,197,536870934]],[logseq____&quot;^15logseq____&quot;,[231,logseq____&quot;^Ulogseq____&quot;,223,536870934]],[logseq____&quot;^15logseq____&quot;,[231,logseq____&quot;^Hlogseq____&quot;,197,536870934]],[logseq____&quot;^15logseq____&quot;,[231,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-84d9-4468-ae3a-97df29c6eb0clogseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[232,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;[[层归一化]]logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[232,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[232,logseq____&quot;^Flogseq____&quot;,231,536870934]],[logseq____&quot;^15logseq____&quot;,[232,logseq____&quot;^Xlogseq____&quot;,223,536870934]],[logseq____&quot;^15logseq____&quot;,[232,logseq____&quot;^Vlogseq____&quot;,223,536870934]],[logseq____&quot;^15logseq____&quot;,[232,logseq____&quot;^Ulogseq____&quot;,223,536870934]],[logseq____&quot;^15logseq____&quot;,[232,logseq____&quot;^Ulogseq____&quot;,224,536870934]],[logseq____&quot;^15logseq____&quot;,[232,logseq____&quot;^Hlogseq____&quot;,224,536870934]],[logseq____&quot;^15logseq____&quot;,[232,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-564d-4e9a-8193-0cdb0a1b78aelogseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[233,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;[[transformer中的门控单元]]logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[233,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[233,logseq____&quot;^Flogseq____&quot;,232,536870934]],[logseq____&quot;^15logseq____&quot;,[233,logseq____&quot;^Xlogseq____&quot;,223,536870934]],[logseq____&quot;^15logseq____&quot;,[233,logseq____&quot;^Vlogseq____&quot;,223,536870934]],[logseq____&quot;^15logseq____&quot;,[233,logseq____&quot;^Ulogseq____&quot;,176,536870934]],[logseq____&quot;^15logseq____&quot;,[233,logseq____&quot;^Ulogseq____&quot;,223,536870934]],[logseq____&quot;^15logseq____&quot;,[233,logseq____&quot;^Hlogseq____&quot;,176,536870934]],[logseq____&quot;^15logseq____&quot;,[233,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-17ca-4fc0-83b4-6d52e5115d6elogseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[234,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;[[attention in fast way]]logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[234,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[234,logseq____&quot;^Flogseq____&quot;,233,536870934]],[logseq____&quot;^15logseq____&quot;,[234,logseq____&quot;^Xlogseq____&quot;,223,536870934]],[logseq____&quot;^15logseq____&quot;,[234,logseq____&quot;^Vlogseq____&quot;,223,536870934]],[logseq____&quot;^15logseq____&quot;,[234,logseq____&quot;^Ulogseq____&quot;,106,536870934]],[logseq____&quot;^15logseq____&quot;,[234,logseq____&quot;^Ulogseq____&quot;,223,536870934]],[logseq____&quot;^15logseq____&quot;,[234,logseq____&quot;^Hlogseq____&quot;,106,536870934]],[logseq____&quot;^15logseq____&quot;,[234,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-6bad-495b-97ae-b903b2fddc89logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[235,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[235,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[235,logseq____&quot;^Flogseq____&quot;,234,536870934]],[logseq____&quot;^15logseq____&quot;,[235,logseq____&quot;^Xlogseq____&quot;,223,536870934]],[logseq____&quot;^15logseq____&quot;,[235,logseq____&quot;^Vlogseq____&quot;,223,536870934]],[logseq____&quot;^15logseq____&quot;,[235,logseq____&quot;^Ulogseq____&quot;,223,536870934]],[logseq____&quot;^15logseq____&quot;,[235,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-0adb-4e0c-ada9-4daba7b80247logseq____&quot;,536870934]],[logseq____&quot;^15logseq____&quot;,[237,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;原始的 BERT 论文采用 Post-LN 结构，而Post-LN 结构容易发散，近年来大模型多用Pre-LN结构logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[237,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[237,logseq____&quot;^Flogseq____&quot;,243,536870935]],[logseq____&quot;^15logseq____&quot;,[237,logseq____&quot;^Xlogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[237,logseq____&quot;^Vlogseq____&quot;,243,536870935]],[logseq____&quot;^15logseq____&quot;,[237,logseq____&quot;^Ulogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[237,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-8fc2-4166-b93e-2ce94e624953logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[238,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;此外，RMSNorm 还可以引入可学习的缩放因子$g_i$和偏移参数$b_i$, 从而得到$\\\\bar{a}_i = \\\\frac{a_i}{RMS(a)}g_i + b_i$logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[238,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[238,logseq____&quot;^Flogseq____&quot;,240,536870935]],[logseq____&quot;^15logseq____&quot;,[238,logseq____&quot;^Xlogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[238,logseq____&quot;^Vlogseq____&quot;,244,536870935]],[logseq____&quot;^15logseq____&quot;,[238,logseq____&quot;^Ulogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[238,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-7a2d-49f0-b354-39e03533f8c5logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[239,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;![layer_norm.png](../assets/layer_norm_1711464349921_0.png)logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[239,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[239,logseq____&quot;^Flogseq____&quot;,248,536870935]],[logseq____&quot;^15logseq____&quot;,[239,logseq____&quot;^Xlogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[239,logseq____&quot;^Vlogseq____&quot;,243,536870935]],[logseq____&quot;^15logseq____&quot;,[239,logseq____&quot;^Ulogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[239,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-139f-4451-b6e8-fd96ed0fe0a7logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[240,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;$$\\\\bar{a}_i = \\\\frac{a_i}{RMS(a)}$$logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[240,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[240,logseq____&quot;^Flogseq____&quot;,242,536870935]],[logseq____&quot;^15logseq____&quot;,[240,logseq____&quot;^Xlogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[240,logseq____&quot;^Vlogseq____&quot;,244,536870935]],[logseq____&quot;^15logseq____&quot;,[240,logseq____&quot;^Ulogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[240,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-858d-4e46-abeb-a122079db609logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[241,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## Deep Normlogseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[241,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[241,logseq____&quot;^Flogseq____&quot;,243,536870935]],[logseq____&quot;^15logseq____&quot;,[241,logseq____&quot;^Xlogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[241,logseq____&quot;^Vlogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[241,logseq____&quot;^Ulogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[241,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870935]],[logseq____&quot;^15logseq____&quot;,[241,logseq____&quot;^Jlogseq____&quot;,[],536870935]],[logseq____&quot;^15logseq____&quot;,[241,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-6b60-4e47-9201-7ec98bf63210logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[242,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;$$RMS(a) = \\\\sqrt{\\\\frac{1}{n} \\\\sum_{i=1}^n a_i^2}$$logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[242,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[242,logseq____&quot;^Flogseq____&quot;,244,536870935]],[logseq____&quot;^15logseq____&quot;,[242,logseq____&quot;^Xlogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[242,logseq____&quot;^Vlogseq____&quot;,244,536870935]],[logseq____&quot;^15logseq____&quot;,[242,logseq____&quot;^Ulogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[242,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-92df-4e31-be28-eb3581b7bb13logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[243,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;**## pre norm / post norm / sandwich ln**logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[243,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[243,logseq____&quot;^Flogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[243,logseq____&quot;^Xlogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[243,logseq____&quot;^Vlogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[243,logseq____&quot;^Ulogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[243,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-346f-47ad-a397-830119a07d4elogseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[244,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;与 Layer Norm 相比，少减去均值，可节省训练所需时间logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[244,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[244,logseq____&quot;^Flogseq____&quot;,247,536870935]],[logseq____&quot;^15logseq____&quot;,[244,logseq____&quot;^Xlogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[244,logseq____&quot;^Vlogseq____&quot;,247,536870935]],[logseq____&quot;^15logseq____&quot;,[244,logseq____&quot;^Ulogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[244,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-2b01-4b0a-bd26-7a1424eeab10logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[245,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;~```python\\ndef rms_norm(x, weight=None, eps=1e-05):\\n    output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + eps)\\n    if weight is not None:\\n        return output * weight\\n    return output\\n```logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[245,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[245,logseq____&quot;^Flogseq____&quot;,244,536870935]],[logseq____&quot;^15logseq____&quot;,[245,logseq____&quot;^Xlogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[245,logseq____&quot;^Vlogseq____&quot;,247,536870935]],[logseq____&quot;^15logseq____&quot;,[245,logseq____&quot;^Ulogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[245,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-aa1d-4a91-8ff9-eeb84270797flogseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[246,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;$$DeepNorm(x) = LayerNorm(\\\\alpha \\\\cdot x + Network(x))$$ , 其中 $$\\\\alpha = (2N)^{\\\\frac{1}{2}}$$logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[246,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[246,logseq____&quot;^Flogseq____&quot;,241,536870935]],[logseq____&quot;^15logseq____&quot;,[246,logseq____&quot;^Xlogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[246,logseq____&quot;^Vlogseq____&quot;,241,536870935]],[logseq____&quot;^15logseq____&quot;,[246,logseq____&quot;^Ulogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[246,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-97c4-41ec-aacc-dddee5364050logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[247,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## [RMSNorm](https://arxiv.org/abs/1910.07467)logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[247,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[247,logseq____&quot;^Flogseq____&quot;,241,536870935]],[logseq____&quot;^15logseq____&quot;,[247,logseq____&quot;^Xlogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[247,logseq____&quot;^Vlogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[247,logseq____&quot;^Ulogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[247,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870935]],[logseq____&quot;^15logseq____&quot;,[247,logseq____&quot;^Jlogseq____&quot;,[],536870935]],[logseq____&quot;^15logseq____&quot;,[247,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-95f8-4070-9108-117f146b1892logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[248,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;数百亿/多模态混合精度训练(FP16)中，Pre-LN 也不稳定， Sandwich-LN可以缓解这个现象logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[248,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[248,logseq____&quot;^Flogseq____&quot;,237,536870935]],[logseq____&quot;^15logseq____&quot;,[248,logseq____&quot;^Xlogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[248,logseq____&quot;^Vlogseq____&quot;,243,536870935]],[logseq____&quot;^15logseq____&quot;,[248,logseq____&quot;^Ulogseq____&quot;,224,536870935]],[logseq____&quot;^15logseq____&quot;,[248,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-08b2-4577-bb23-d60389597218logseq____&quot;,536870935]],[logseq____&quot;^15logseq____&quot;,[250,logseq____&quot;^Klogseq____&quot;,1723260458922,536870936]],[logseq____&quot;^15logseq____&quot;,[250,logseq____&quot;^@logseq____&quot;,false,536870936]],[logseq____&quot;^15logseq____&quot;,[250,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;搜索与推荐logseq____&quot;,536870936]],[logseq____&quot;^15logseq____&quot;,[250,logseq____&quot;^11logseq____&quot;,logseq____&quot;搜索与推荐logseq____&quot;,536870936]],[logseq____&quot;^15logseq____&quot;,[250,logseq____&quot;^Blogseq____&quot;,1723260458922,536870936]],[logseq____&quot;^15logseq____&quot;,[250,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-d293-4607-ba06-18e85598e482logseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[251,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;logseq____&quot;,536870936]],[logseq____&quot;^15logseq____&quot;,[251,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870936]],[logseq____&quot;^15logseq____&quot;,[251,logseq____&quot;^Flogseq____&quot;,250,536870936]],[logseq____&quot;^15logseq____&quot;,[251,logseq____&quot;^Xlogseq____&quot;,250,536870936]],[logseq____&quot;^15logseq____&quot;,[251,logseq____&quot;^Vlogseq____&quot;,250,536870936]],[logseq____&quot;^15logseq____&quot;,[251,logseq____&quot;^Ulogseq____&quot;,250,536870936]],[logseq____&quot;^15logseq____&quot;,[251,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-68be-4d64-8f07-db6dcd4bae23logseq____&quot;,536870936]],[logseq____&quot;^15logseq____&quot;,[253,logseq____&quot;^Klogseq____&quot;,1723260458950,536870937]],[logseq____&quot;^15logseq____&quot;,[253,logseq____&quot;^@logseq____&quot;,false,536870937]],[logseq____&quot;^15logseq____&quot;,[253,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;预训练与语言模型logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[253,logseq____&quot;^11logseq____&quot;,logseq____&quot;预训练与语言模型logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[253,logseq____&quot;^Blogseq____&quot;,1723260458950,536870937]],[logseq____&quot;^15logseq____&quot;,[253,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-c363-4491-8762-dcedb8262aaelogseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[254,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;**人工模板工程**通过人工设计启发式的模板，例如LAMA数据集设计了完形填空式的模板。但是与特征工程类似地，人工模板需要时间与经验，对于一些复杂的任务，如语义解析，则需要专家级的设计者。logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[254,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[254,logseq____&quot;^Flogseq____&quot;,258,536870937]],[logseq____&quot;^15logseq____&quot;,[254,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[254,logseq____&quot;^Vlogseq____&quot;,273,536870937]],[logseq____&quot;^15logseq____&quot;,[254,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[254,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-444a-4325-a86a-adf9395388b3logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[255,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;Prefix tuning ：在输入前加入与任务相关的连续前缀向量，这个前缀向量是可训练的，而后续的 LM 模型参数则是冻结的。其实质是优化在给定可训练矩阵 $M_{\\\\phi}$ 和固定预训练语言模型 $\\\\theta$ 下的一个对数似然函数：logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[255,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[255,logseq____&quot;^Flogseq____&quot;,265,536870937]],[logseq____&quot;^15logseq____&quot;,[255,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[255,logseq____&quot;^Vlogseq____&quot;,265,536870937]],[logseq____&quot;^15logseq____&quot;,[255,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[255,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-b84f-4950-b54f-71fe83d9cb58logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[256,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## 思路logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[256,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[256,logseq____&quot;^Flogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[256,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[256,logseq____&quot;^Vlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[256,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[256,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870937]],[logseq____&quot;^15logseq____&quot;,[256,logseq____&quot;^Jlogseq____&quot;,[],536870937]],[logseq____&quot;^15logseq____&quot;,[256,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-152e-4fef-b7a7-3a4cb571fad2logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[257,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;Discrete Prompt 使用 PLM 的 embedding 层 $e(\\\\cdot)$ 将原输入编码成 $\\\\{e([P_{0:i}]),e(x),e([P_{i+1:m}]),e(y)\\\\}$， 此处的每个 P 对应一个token，如（a）所示；logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[257,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[257,logseq____&quot;^Flogseq____&quot;,275,536870937]],[logseq____&quot;^15logseq____&quot;,[257,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[257,logseq____&quot;^Vlogseq____&quot;,262,536870937]],[logseq____&quot;^15logseq____&quot;,[257,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[257,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-5c52-4578-b327-0c6f9a6ebdc8logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[258,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;prompt 工程是指找到一个模板函数 $f_{prompt}(x)$能够将输入应用到下游任务上，并取得很好的效果。prompt 通常有两种最常见的形式，一种是完形填空式(cloze prompts)；另一种是前缀式模板(prefix prompts)。\\nid:: 6447e9d7-8670-4899-8369-22d4f70c6feclogseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[258,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[258,logseq____&quot;^Flogseq____&quot;,273,536870937]],[logseq____&quot;^15logseq____&quot;,[258,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[258,logseq____&quot;^Vlogseq____&quot;,273,536870937]],[logseq____&quot;^15logseq____&quot;,[258,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[258,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;~:idlogseq____&quot;,logseq____&quot;6447e9d7-8670-4899-8369-22d4f70c6feclogseq____&quot;],536870937]],[logseq____&quot;^15logseq____&quot;,[258,logseq____&quot;^Jlogseq____&quot;,[logseq____&quot;^1?logseq____&quot;],536870937]],[logseq____&quot;^15logseq____&quot;,[258,logseq____&quot;^4logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^1?logseq____&quot;,logseq____&quot;6447e9d7-8670-4899-8369-22d4f70c6feclogseq____&quot;],536870937]],[logseq____&quot;^15logseq____&quot;,[258,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u6447e9d7-8670-4899-8369-22d4f70c6feclogseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[259,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;Prefix Tuning 会在每个 transformer 层以及输入层预先设置一系列前缀。实现上 huggingface 通过 `past_key_values` 对 prefix 部分以缓存的形式拼接到 transformer 的每一层中，`P-Tuning V2` 采用了类似的实现。logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[259,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[259,logseq____&quot;^Flogseq____&quot;,260,536870937]],[logseq____&quot;^15logseq____&quot;,[259,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[259,logseq____&quot;^Vlogseq____&quot;,255,536870937]],[logseq____&quot;^15logseq____&quot;,[259,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[259,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-81d2-48a4-8cb5-9df79c39ad80logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[260,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;$$\\\\max_\\\\phi \\\\log P(y|x;\\\\theta;\\\\phi) = \\\\max_\\\\phi \\\\sum_{y_i} log P(y_i|h_{\\\\lt i};\\\\theta; \\\\phi) $$logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[260,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[260,logseq____&quot;^Flogseq____&quot;,255,536870937]],[logseq____&quot;^15logseq____&quot;,[260,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[260,logseq____&quot;^Vlogseq____&quot;,255,536870937]],[logseq____&quot;^15logseq____&quot;,[260,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[260,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-eb68-4b43-8042-c43bc36c0a80logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[261,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;prompt在利用预训练的基础之上，定制合适的模板，这种方式可以轻易地完成 few-shot 或者 zero-shot 任务。从数学的角度来看，监督学习是建模 $P(y|x;\\\\theta)$，而 prompt 则是建模 $P(x;\\\\theta)$，通过模型告诉我们输入文本 x 出现的概率，然后利用这个概率去预测正确的 y，这样其实可以减少需要标注的数据。logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[261,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[261,logseq____&quot;^Flogseq____&quot;,278,536870937]],[logseq____&quot;^15logseq____&quot;,[261,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[261,logseq____&quot;^Vlogseq____&quot;,256,536870937]],[logseq____&quot;^15logseq____&quot;,[261,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[261,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-b795-4411-88ae-1c0bc891cf81logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[262,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;P-Tuning 采用的也是参数化的 Prompt，与 Discrete Prompt 的区别：logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[262,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[262,logseq____&quot;^Flogseq____&quot;,277,536870937]],[logseq____&quot;^15logseq____&quot;,[262,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[262,logseq____&quot;^Vlogseq____&quot;,271,536870937]],[logseq____&quot;^15logseq____&quot;,[262,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[262,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-8b0a-4505-bc86-d6e0cc348b12logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[263,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;![截屏2022-02-15 下午5.34.23.png](../assets/prompting1.png){:height 399, :width 543}logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[263,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[263,logseq____&quot;^Flogseq____&quot;,256,536870937]],[logseq____&quot;^15logseq____&quot;,[263,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[263,logseq____&quot;^Vlogseq____&quot;,256,536870937]],[logseq____&quot;^15logseq____&quot;,[263,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[263,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-d76b-446f-b67f-0a0d26e7f416logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[264,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;P-Tuning prompt 的编码将与其他 embedding 一起传入预训练模型中，而不是加在模型的每一层前；logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[264,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[264,logseq____&quot;^Flogseq____&quot;,276,536870937]],[logseq____&quot;^15logseq____&quot;,[264,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[264,logseq____&quot;^Vlogseq____&quot;,279,536870937]],[logseq____&quot;^15logseq____&quot;,[264,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[264,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-5e02-43d1-8bc8-8d19a11bc203logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[265,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;Prefix Tuning , Prompt Tuning 与 P-Tuninglogseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[265,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[265,logseq____&quot;^Flogseq____&quot;,270,536870937]],[logseq____&quot;^15logseq____&quot;,[265,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[265,logseq____&quot;^Vlogseq____&quot;,270,536870937]],[logseq____&quot;^15logseq____&quot;,[265,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[265,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-0094-46ba-aee2-25432863c323logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[266,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;而 P-Tuning 则使用可学习的参数替代，将原输入编码为$\\\\{h_0,…,h_i,e(x),h_{i+1},…,h_m,e(y)\\\\}$ ，此处 P 为pseudo token，并没有实际指向的token。logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[266,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[266,logseq____&quot;^Flogseq____&quot;,257,536870937]],[logseq____&quot;^15logseq____&quot;,[266,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[266,logseq____&quot;^Vlogseq____&quot;,262,536870937]],[logseq____&quot;^15logseq____&quot;,[266,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[266,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-dfa8-4bbb-a1f7-752138f6c66clogseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[267,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;Prompt Tuning：利用离散模板作为初始化，然后将其连续化。即先定义一个基于 virtual tokens 的离散模板，然后微调 embedding 来使任务效果更好。通过在输入嵌入前添加提示来表示任务来实现。只 finetune token embedding；logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[267,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[267,logseq____&quot;^Flogseq____&quot;,255,536870937]],[logseq____&quot;^15logseq____&quot;,[267,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[267,logseq____&quot;^Vlogseq____&quot;,265,536870937]],[logseq____&quot;^15logseq____&quot;,[267,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[267,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-4be8-41b5-9b82-c257c087bea6logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[268,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;引入了 anchor token；logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[268,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[268,logseq____&quot;^Flogseq____&quot;,264,536870937]],[logseq____&quot;^15logseq____&quot;,[268,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[268,logseq____&quot;^Vlogseq____&quot;,279,536870937]],[logseq____&quot;^15logseq____&quot;,[268,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[268,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-cb64-4a7e-a30d-496ff6f4be12logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[269,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;![promting2](../assets/prompting2.png){:height 264, :width 554}logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[269,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[269,logseq____&quot;^Flogseq____&quot;,261,536870937]],[logseq____&quot;^15logseq____&quot;,[269,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[269,logseq____&quot;^Vlogseq____&quot;,261,536870937]],[logseq____&quot;^15logseq____&quot;,[269,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[269,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-ff95-4ff8-9236-777747909ad0logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[270,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;**自动模板学习**可以细分离散模板(discrete prompts)，离散模板是实际的文本字符串。另一种是连续模板(continuous prompts)，是指上是在底层的语言模型中的嵌入空间中进行描述。连续模板移除了两个限制：一是放松了模板词只能选择有限的自然语言词的限制；二是移除了模板参数公能读取自 LM 模型的限制，也就是说模型也能有自己的参数，并根据下游任务进行训练。logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[270,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[270,logseq____&quot;^Flogseq____&quot;,254,536870937]],[logseq____&quot;^15logseq____&quot;,[270,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[270,logseq____&quot;^Vlogseq____&quot;,273,536870937]],[logseq____&quot;^15logseq____&quot;,[270,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[270,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-a035-4c85-bab9-0f6636c0b2cflogseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[271,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;P-Tuning：结合了 prefix tuning 与 prompt tuning，利用离散模版作为初始化，将其使用 encoder (LSTM 或者 MLP)连续化后，将一些可微调的 embedding 插入到固定的模板中。logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[271,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[271,logseq____&quot;^Flogseq____&quot;,267,536870937]],[logseq____&quot;^15logseq____&quot;,[271,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[271,logseq____&quot;^Vlogseq____&quot;,265,536870937]],[logseq____&quot;^15logseq____&quot;,[271,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[271,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-d38e-494f-9ef3-0bb45c754ccalogseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[272,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;[P-tuning 实现](https://kexue.fm/archives/8295)logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[272,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[272,logseq____&quot;^Flogseq____&quot;,279,536870937]],[logseq____&quot;^15logseq____&quot;,[272,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[272,logseq____&quot;^Vlogseq____&quot;,271,536870937]],[logseq____&quot;^15logseq____&quot;,[272,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[272,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-2168-4502-adf8-1e751f5b613elogseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[273,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## Prompt 工程logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[273,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[273,logseq____&quot;^Flogseq____&quot;,256,536870937]],[logseq____&quot;^15logseq____&quot;,[273,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[273,logseq____&quot;^Vlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[273,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[273,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870937]],[logseq____&quot;^15logseq____&quot;,[273,logseq____&quot;^Jlogseq____&quot;,[],536870937]],[logseq____&quot;^15logseq____&quot;,[273,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-a8dc-4a00-b462-8cfe99006949logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[274,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;对比预训练模型与模板学习, [[预训练与语言模型]] 是通过语言模型衔接下接任务进行微调，而模板学习是将下流任务融合到语言模型中。logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[274,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[274,logseq____&quot;^Flogseq____&quot;,261,536870937]],[logseq____&quot;^15logseq____&quot;,[274,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[274,logseq____&quot;^Vlogseq____&quot;,256,536870937]],[logseq____&quot;^15logseq____&quot;,[274,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[274,logseq____&quot;^Ulogseq____&quot;,253,536870937]],[logseq____&quot;^15logseq____&quot;,[274,logseq____&quot;^Hlogseq____&quot;,253,536870937]],[logseq____&quot;^15logseq____&quot;,[274,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-5e8a-42fb-ae69-7d8735f65b5clogseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[275,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;假设原输入为: $T=\\\\{[P_{0:i}],x,[P_{i+1:m}],y\\\\}$。logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[275,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[275,logseq____&quot;^Flogseq____&quot;,262,536870937]],[logseq____&quot;^15logseq____&quot;,[275,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[275,logseq____&quot;^Vlogseq____&quot;,262,536870937]],[logseq____&quot;^15logseq____&quot;,[275,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[275,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-2d4a-43ba-b30f-03d3b8179a61logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[276,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;P-Tuning 的 prompt 字符可加入在任何位置；logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[276,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[276,logseq____&quot;^Flogseq____&quot;,279,536870937]],[logseq____&quot;^15logseq____&quot;,[276,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[276,logseq____&quot;^Vlogseq____&quot;,279,536870937]],[logseq____&quot;^15logseq____&quot;,[276,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[276,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-14f9-4fea-8cc5-4ab97b98c489logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[277,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;![ptuning.png](../assets/ptuning.png){:height 243, :width 958}logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[277,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[277,logseq____&quot;^Flogseq____&quot;,271,536870937]],[logseq____&quot;^15logseq____&quot;,[277,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[277,logseq____&quot;^Vlogseq____&quot;,271,536870937]],[logseq____&quot;^15logseq____&quot;,[277,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[277,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-27cb-43cf-a31f-cf0cd59302a2logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[278,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;prompt将原来的输入文本填入一个带有输入与输出槽位的模板，然后利用预训练语言模型预测整个句子，最终利用出这个句子推导出最终的答案。logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[278,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[278,logseq____&quot;^Flogseq____&quot;,263,536870937]],[logseq____&quot;^15logseq____&quot;,[278,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[278,logseq____&quot;^Vlogseq____&quot;,256,536870937]],[logseq____&quot;^15logseq____&quot;,[278,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[278,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-4601-43b6-b419-19aa46ccc3adlogseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[279,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;与 Prefix Tuning 的区别：logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[279,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[279,logseq____&quot;^Flogseq____&quot;,262,536870937]],[logseq____&quot;^15logseq____&quot;,[279,logseq____&quot;^Xlogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[279,logseq____&quot;^Vlogseq____&quot;,271,536870937]],[logseq____&quot;^15logseq____&quot;,[279,logseq____&quot;^Ulogseq____&quot;,45,536870937]],[logseq____&quot;^15logseq____&quot;,[279,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-cf55-4a66-a91e-5417728a904blogseq____&quot;,536870937]],[logseq____&quot;^15logseq____&quot;,[281,logseq____&quot;^Klogseq____&quot;,1723260458963,536870938]],[logseq____&quot;^15logseq____&quot;,[281,logseq____&quot;^@logseq____&quot;,false,536870938]],[logseq____&quot;^15logseq____&quot;,[281,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;生成模型的解码方法logseq____&quot;,536870938]],[logseq____&quot;^15logseq____&quot;,[281,logseq____&quot;^11logseq____&quot;,logseq____&quot;生成模型的解码方法logseq____&quot;,536870938]],[logseq____&quot;^15logseq____&quot;,[281,logseq____&quot;^Blogseq____&quot;,1723260458963,536870938]],[logseq____&quot;^15logseq____&quot;,[281,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-9c35-4d0e-b82f-10b5f41eaed1logseq____&quot;,536870938]],[logseq____&quot;^15logseq____&quot;,[282,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;[[outlines:大模型固定格式解码]]logseq____&quot;,536870938]],[logseq____&quot;^15logseq____&quot;,[282,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870938]],[logseq____&quot;^15logseq____&quot;,[282,logseq____&quot;^Flogseq____&quot;,281,536870938]],[logseq____&quot;^15logseq____&quot;,[282,logseq____&quot;^Xlogseq____&quot;,281,536870938]],[logseq____&quot;^15logseq____&quot;,[282,logseq____&quot;^Vlogseq____&quot;,281,536870938]],[logseq____&quot;^15logseq____&quot;,[282,logseq____&quot;^Ulogseq____&quot;,149,536870938]],[logseq____&quot;^15logseq____&quot;,[282,logseq____&quot;^Ulogseq____&quot;,281,536870938]],[logseq____&quot;^15logseq____&quot;,[282,logseq____&quot;^Hlogseq____&quot;,149,536870938]],[logseq____&quot;^15logseq____&quot;,[282,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-aa13-4e91-acd2-c49a4ad9806blogseq____&quot;,536870938]],[logseq____&quot;^15logseq____&quot;,[284,logseq____&quot;^Klogseq____&quot;,1723260459010,536870939]],[logseq____&quot;^15logseq____&quot;,[284,logseq____&quot;^@logseq____&quot;,false,536870939]],[logseq____&quot;^15logseq____&quot;,[284,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;窗口注意力logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[284,logseq____&quot;^11logseq____&quot;,logseq____&quot;窗口注意力logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[284,logseq____&quot;^Blogseq____&quot;,1723260459010,536870939]],[logseq____&quot;^15logseq____&quot;,[284,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-3707-427f-a2c3-caadc302c373logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[285,logseq____&quot;^Klogseq____&quot;,1723260459011,536870939]],[logseq____&quot;^15logseq____&quot;,[285,logseq____&quot;^@logseq____&quot;,false,536870939]],[logseq____&quot;^15logseq____&quot;,[285,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;longer context prompting and retrieval augment generationlogseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[285,logseq____&quot;^11logseq____&quot;,logseq____&quot;Longer Context Prompting and Retrieval Augment Generationlogseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[285,logseq____&quot;^Blogseq____&quot;,1723260459011,536870939]],[logseq____&quot;^15logseq____&quot;,[285,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-0a10-40a1-a09a-1f143c4e7160logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[286,logseq____&quot;^Klogseq____&quot;,1723260459011,536870939]],[logseq____&quot;^15logseq____&quot;,[286,logseq____&quot;^@logseq____&quot;,false,536870939]],[logseq____&quot;^15logseq____&quot;,[286,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;dual chunk attention with yarnlogseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[286,logseq____&quot;^11logseq____&quot;,logseq____&quot;Dual chunk attention with YARNlogseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[286,logseq____&quot;^Blogseq____&quot;,1723260459011,536870939]],[logseq____&quot;^15logseq____&quot;,[286,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-d3f9-49ff-9363-bac3feba81e3logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[287,logseq____&quot;^Klogseq____&quot;,1723260459009,536870939]],[logseq____&quot;^15logseq____&quot;,[287,logseq____&quot;^@logseq____&quot;,false,536870939]],[logseq____&quot;^15logseq____&quot;,[287,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;ntklogseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[287,logseq____&quot;^11logseq____&quot;,logseq____&quot;ntklogseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[287,logseq____&quot;^Blogseq____&quot;,1723260459009,536870939]],[logseq____&quot;^15logseq____&quot;,[287,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-023f-443f-accc-9eab262e44aalogseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[288,logseq____&quot;^Klogseq____&quot;,1723260459008,536870939]],[logseq____&quot;^15logseq____&quot;,[288,logseq____&quot;^@logseq____&quot;,false,536870939]],[logseq____&quot;^15logseq____&quot;,[288,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;lognscalinglogseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[288,logseq____&quot;^11logseq____&quot;,logseq____&quot;logNscalinglogseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[288,logseq____&quot;^Blogseq____&quot;,1723260459008,536870939]],[logseq____&quot;^15logseq____&quot;,[288,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-d449-4dc7-b1a5-fe550287b878logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[289,logseq____&quot;^Klogseq____&quot;,1723260459012,536870939]],[logseq____&quot;^15logseq____&quot;,[289,logseq____&quot;^@logseq____&quot;,false,536870939]],[logseq____&quot;^15logseq____&quot;,[289,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;cotlogseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[289,logseq____&quot;^11logseq____&quot;,logseq____&quot;CoTlogseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[289,logseq____&quot;^Blogseq____&quot;,1723260459012,536870939]],[logseq____&quot;^15logseq____&quot;,[289,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-0208-4470-b0ad-93c4a76e36fclogseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[290,logseq____&quot;^Klogseq____&quot;,1723260459008,536870939]],[logseq____&quot;^15logseq____&quot;,[290,logseq____&quot;^@logseq____&quot;,false,536870939]],[logseq____&quot;^15logseq____&quot;,[290,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;语言模型理解更长的输入logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[290,logseq____&quot;^11logseq____&quot;,logseq____&quot;语言模型理解更长的输入logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[290,logseq____&quot;^Blogseq____&quot;,1723260459008,536870939]],[logseq____&quot;^15logseq____&quot;,[290,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-9bb7-460b-9a38-25c0152bdf1clogseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[291,logseq____&quot;^Klogseq____&quot;,1723260459009,536870939]],[logseq____&quot;^15logseq____&quot;,[291,logseq____&quot;^@logseq____&quot;,false,536870939]],[logseq____&quot;^15logseq____&quot;,[291,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;transformer familylogseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[291,logseq____&quot;^11logseq____&quot;,logseq____&quot;Transformer Familylogseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[291,logseq____&quot;^Blogseq____&quot;,1723260459009,536870939]],[logseq____&quot;^15logseq____&quot;,[291,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-b204-4841-a491-062f71d37063logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[292,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## 问题\\n为什么对于模型来说扩展到更长的序列长度上会比较难？\\ndistracting tokens EOS 等使模型效果变差\\nlogseq____&gt; Our analysis in Section 4 indicates that length generalization pathologies persist even when we use the padded scratchpad strategy that makes sure that it’s not untrained position encodings and/or the EOS token prediction that causes the aforementioned pathologies. This points to the fact that the transformer doesn’t learn to attend to the “right” section of the input and scratchpad that implements the sequential strategy that generalizes to longer lengths — it’s thrown off by distractor tokens in the input and/or the preceding scratchpad targets. [Anil](https://arxiv.org/abs/2207.04901)\\n\\n更后面的位置编码更新的次数少于前面的位置编码\\nlogseq____&gt; If the network is only trained with short instances, position biases that handle longer positional distances might not be trained, explaining poor length generalization. [Tao](https://arxiv.org/abs/2305.04859)\\n\\n在位置编码中的 bias 会复制窗口注意力机制的效果，会通过使长距离感受域的标记间依赖性衰减\\nlogseq____&gt; Intuitively, ALiBi encourages a token to focus on neighbors based on its temporal biases ma- trix. When two tokens are distant, ALiBi becomes highly similar to windowed attention[Chi](https://arxiv.org/abs/2212.10356)\\n\\n\\n使用线性位置编码时，为了让模型能够处理任意长度的序列，需要让查询向量和键向量之间的注意力权重不依赖于序列的总长度，而只由 token 之间的相对位置决定。而这一现象往往是理想情况下。\\nlogseq____&gt; Proposition 4. Consider linear positional encoding, i.e. pi = i/C for some (large) constant C. Then, perfect length generalization to arbitrary length requires $W_T W_{Kp} = 0$ . [Liu](https://arxiv.org/pdf/2306.00946.pdf)\\n\\n位置编码没有外推性\\nlogseq____&gt; 在直觉上，相信很多读者觉得像[Sinusoidal](https://kexue.fm/archives/8231)或[RoPE](https://kexue.fm/archives/8265)之类的函数式位置编码，它们没有训练参数，长度外推性应该很好才对，但事实上并非如此，这类位置编码并没有在长度外推方面表现出什么优势。为什么会这样呢？其实是大家在假设函数式位置编码的外推性时，忘了它的基本前提——“光滑性”。 \\nlogseq____&gt; 其实，外推性就是局部推断整体，对此我们应该并不陌生，泰勒级数近似就是经典的例子，它只需要知道函数某点处若干阶导数的值，就可以对一个邻域内的值做有效估计，它依赖的就是给定函数的高阶光滑性（高阶导数存在且有界）。但是[Sinusoidal](https://kexue.fm/archives/8231)或[RoPE](https://kexue.fm/archives/8265)是这种函数吗？并不是。它们是一系列正余弦函数的组合，其相位函数是k/100002i/d，当2i/d≈0时，函数近似就是sink,cosk，这算是关于位置编码k的高频振荡函数了，而不是直线或者渐近趋于直线之类的函数，所以基于它的模型往往外推行为难以预估。能否设计不振荡的位置编码？很难，位置编码函数如果不振荡，那么往往缺乏足够的容量去编码足够多的位置信息，也就是某种意义上来说，位置编码函数的复杂性本身也是编码位置的要求。 [Su](https://kexue.fm/archives/9431)logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[292,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[292,logseq____&quot;^Flogseq____&quot;,290,536870939]],[logseq____&quot;^15logseq____&quot;,[292,logseq____&quot;^Xlogseq____&quot;,290,536870939]],[logseq____&quot;^15logseq____&quot;,[292,logseq____&quot;^Vlogseq____&quot;,290,536870939]],[logseq____&quot;^15logseq____&quot;,[292,logseq____&quot;^Ulogseq____&quot;,290,536870939]],[logseq____&quot;^15logseq____&quot;,[292,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870939]],[logseq____&quot;^15logseq____&quot;,[292,logseq____&quot;^Jlogseq____&quot;,[],536870939]],[logseq____&quot;^15logseq____&quot;,[292,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-3768-4ad2-88c0-6d265cceeaaflogseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[293,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## Transformer 网络\\n对于transformer来说，直接使用增加 context 长度会导致时间 $O(L^2d)$ 与显存 $O(L^2)$ ，直接增长训练数据是不太现实的，所以通过改进网络结构，来实现让语言模型理解更长的输入，参看 [[Transformer Family]]\\nLonger Context\\n\\n除以log N 调整 attention值， N为输入序列长度 #logNscaling\\nlogseq____&gt; Although this is not a logical consequence of Hahn’s lemma, it is a consequence of the behavior that Hahn’s lemma predicts. Fortunately, this problem can be fixed with a simple modification, multiplying attention logits by log 𝑛. This modification also improves length generalization in machine translation. [Chiang](https://arxiv.org/pdf/2202.12172.pdf)\\n\\n使用 ReLU 替换 attention 中的 softmax (模型不一定收敛)\\nlogseq____&gt; By changing the total number of key-value slots, we find that ReLU performs better than Softmax when the number of slots is larger. We explore the reason by calculating the ratio of top scores among all activations and find that the activation weights are highly centralized in a small number of slots, thus insufficient to utilize the context information of other slots, while ReLU is able to alleviate this problem. Given the superior performance of ReLU when scaling to a large number of value slots, we then explore how ReLU performs on SAN where Softmax may have a trouble modeling long-sequences (Sun et al., 2022). Unfortunately, directly alternating Softmax to ReLU does not converge. With theoretical and experimental analysis, we find that the variance of SAN results with ReLU activation grows with the length of the input sequence, and the dynamic variance will lead to an unstable training process. Therefore, a variance reduction factor and regularization loss functions are introduced to solve this problem. As a result, we make it possible to utilize ReLU on self-attention, which performs better than Softmax when dealing with long input sequences. [Shen](https://arxiv.org/abs/2302.06461)logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[293,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[293,logseq____&quot;^Flogseq____&quot;,292,536870939]],[logseq____&quot;^15logseq____&quot;,[293,logseq____&quot;^Xlogseq____&quot;,290,536870939]],[logseq____&quot;^15logseq____&quot;,[293,logseq____&quot;^Vlogseq____&quot;,290,536870939]],[logseq____&quot;^15logseq____&quot;,[293,logseq____&quot;^Ulogseq____&quot;,288,536870939]],[logseq____&quot;^15logseq____&quot;,[293,logseq____&quot;^Ulogseq____&quot;,290,536870939]],[logseq____&quot;^15logseq____&quot;,[293,logseq____&quot;^Ulogseq____&quot;,291,536870939]],[logseq____&quot;^15logseq____&quot;,[293,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870939]],[logseq____&quot;^15logseq____&quot;,[293,logseq____&quot;^Jlogseq____&quot;,[],536870939]],[logseq____&quot;^15logseq____&quot;,[293,logseq____&quot;^Hlogseq____&quot;,288,536870939]],[logseq____&quot;^15logseq____&quot;,[293,logseq____&quot;^Hlogseq____&quot;,291,536870939]],[logseq____&quot;^15logseq____&quot;,[293,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-f112-4f5f-9373-351ce5ddc939logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[294,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## 位置编码\\n\\n[[ALiBi, Kerple and Sandwich]] #窗口注意力\\n\\n[[RoPE]] 展现了比 Sinusoidal 更好的外推性, [[NTK-RoPE]] 优化了外推性 #ntk\\n\\n[Randomized Positional Encodings](https://github.com/deepmind/randomized_positional_encodings)  （ACL 2023）\\nlogseq____&gt; We assume that each training step will perform a step of loss minimization on a batch of data of fixed size. Let `U(S)` denote the discrete uniform distribution over set `S`, and let `Pk := {S ⊆ {1, . . . , L}` | |`S`| = `k`}. For each training step, we first sample a random length `n ∼ U({1, . . . , N})` (following Delétang et al., 2023) and then a random set of indices `I ∼ U(Pn)`. We then sort `I` in ascending order, such that `I = {i1, . . . , in} for i1 logseq____&lt; i2 logseq____&lt; · · · logseq____&lt; in`, noting that `I` is sampled without replacement. Finally, we compute our randomized positional encoding for token `1 ≤ j ≤ N` as `RPE(j, ·) := PE(ij , ·)`. At test time, when processing a sequence of length `M logseq____&gt; N`, we use the same procedure but for all token positions `1 ≤ j ≤ M`. The intuition behind our method is to preserve the known good properties of relative encoding but in a way that is independent of the maximum training length N and thus allows generalization to longer sequences at test time. [Ruoss](https://arxiv.org/abs/2305.16843)\\n![random_pe](../assets/random_pe.png){:height 316, :width 374}\\n\\n位置内插： 将 inference 时的长文本乘以因子 $$L_{train}/L_{test}$$\\nlogseq____&gt; Therefore, instead of extrapolate the attention score in Eqn. 3 to s logseq____&gt; L, how about we define an attention score $\\\\tilde{a}(s) = a(Ls/L′)$ where L′ is the longer context window? Formally, we replace RoPE f by f′ defined as follows $f(x,m) = f (x, mL/Llogseq____&apos;)$ [Chen from Meta](https://arxiv.org/pdf/2306.15595.pdf)\\n\\n\\nQWEN1 \\n![qwen1_exp](../assets/qiwen_exp.png){:height 420, :width 517}\\n\\nQWEN2 [[Dual chunk attention with YARN]]logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[294,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[294,logseq____&quot;^Flogseq____&quot;,293,536870939]],[logseq____&quot;^15logseq____&quot;,[294,logseq____&quot;^Xlogseq____&quot;,290,536870939]],[logseq____&quot;^15logseq____&quot;,[294,logseq____&quot;^Vlogseq____&quot;,290,536870939]],[logseq____&quot;^15logseq____&quot;,[294,logseq____&quot;^Ulogseq____&quot;,26,536870939]],[logseq____&quot;^15logseq____&quot;,[294,logseq____&quot;^Ulogseq____&quot;,199,536870939]],[logseq____&quot;^15logseq____&quot;,[294,logseq____&quot;^Ulogseq____&quot;,200,536870939]],[logseq____&quot;^15logseq____&quot;,[294,logseq____&quot;^Ulogseq____&quot;,284,536870939]],[logseq____&quot;^15logseq____&quot;,[294,logseq____&quot;^Ulogseq____&quot;,286,536870939]],[logseq____&quot;^15logseq____&quot;,[294,logseq____&quot;^Ulogseq____&quot;,287,536870939]],[logseq____&quot;^15logseq____&quot;,[294,logseq____&quot;^Ulogseq____&quot;,290,536870939]],[logseq____&quot;^15logseq____&quot;,[294,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870939]],[logseq____&quot;^15logseq____&quot;,[294,logseq____&quot;^Jlogseq____&quot;,[],536870939]],[logseq____&quot;^15logseq____&quot;,[294,logseq____&quot;^Hlogseq____&quot;,26,536870939]],[logseq____&quot;^15logseq____&quot;,[294,logseq____&quot;^Hlogseq____&quot;,199,536870939]],[logseq____&quot;^15logseq____&quot;,[294,logseq____&quot;^Hlogseq____&quot;,200,536870939]],[logseq____&quot;^15logseq____&quot;,[294,logseq____&quot;^Hlogseq____&quot;,284,536870939]],[logseq____&quot;^15logseq____&quot;,[294,logseq____&quot;^Hlogseq____&quot;,286,536870939]],[logseq____&quot;^15logseq____&quot;,[294,logseq____&quot;^Hlogseq____&quot;,287,536870939]],[logseq____&quot;^15logseq____&quot;,[294,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-c168-43b7-9a5a-a8f06b4601d5logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[295,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## Context Parallel\\nLLAMA 3.1 中使用了上下文并行（CP）来提高在扩展 Llama 3 的上下文长度时的内存效率，并使其能够在长度高达 128K 的极长序列上进行训练。在 CP 中，沿着序列维度进行划分，将输入序列划分为 2 × CP 个块，使得每个 CP 级别可以接收到两个块，以便更好地进行负载平衡。第 i 个 CP 级别将接收到第 i 个块和第 (2 × CP − 1 − i) 个块。\\n换句话说，在预训练的时候通过切片 context 来完成长文本的训练：\\n![context parallel](../assets/cp.png){:height 380, :width 710}logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[295,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[295,logseq____&quot;^Flogseq____&quot;,294,536870939]],[logseq____&quot;^15logseq____&quot;,[295,logseq____&quot;^Xlogseq____&quot;,290,536870939]],[logseq____&quot;^15logseq____&quot;,[295,logseq____&quot;^Vlogseq____&quot;,290,536870939]],[logseq____&quot;^15logseq____&quot;,[295,logseq____&quot;^Ulogseq____&quot;,290,536870939]],[logseq____&quot;^15logseq____&quot;,[295,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870939]],[logseq____&quot;^15logseq____&quot;,[295,logseq____&quot;^Jlogseq____&quot;,[],536870939]],[logseq____&quot;^15logseq____&quot;,[295,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-81a3-4e68-ad04-b68b20688ef0logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[296,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## 模版工程\\nlogseq____&gt; 通过[[CoT]]和标记式标记的方式，可以引导预训练模型学习如何进行长度外推。模型在总结完成任务的步骤并标记完成状态的过程中，学习到了如何在上下文中利用更长的上下文信息进行推理。[Bueno](https://arxiv.org/abs/2208.11445)\\n[[Longer Context Prompting and Retrieval Augment Generation]]logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[296,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[296,logseq____&quot;^Flogseq____&quot;,295,536870939]],[logseq____&quot;^15logseq____&quot;,[296,logseq____&quot;^Xlogseq____&quot;,290,536870939]],[logseq____&quot;^15logseq____&quot;,[296,logseq____&quot;^Vlogseq____&quot;,290,536870939]],[logseq____&quot;^15logseq____&quot;,[296,logseq____&quot;^Ulogseq____&quot;,285,536870939]],[logseq____&quot;^15logseq____&quot;,[296,logseq____&quot;^Ulogseq____&quot;,289,536870939]],[logseq____&quot;^15logseq____&quot;,[296,logseq____&quot;^Ulogseq____&quot;,290,536870939]],[logseq____&quot;^15logseq____&quot;,[296,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870939]],[logseq____&quot;^15logseq____&quot;,[296,logseq____&quot;^Jlogseq____&quot;,[],536870939]],[logseq____&quot;^15logseq____&quot;,[296,logseq____&quot;^Hlogseq____&quot;,285,536870939]],[logseq____&quot;^15logseq____&quot;,[296,logseq____&quot;^Hlogseq____&quot;,289,536870939]],[logseq____&quot;^15logseq____&quot;,[296,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-0409-45d0-86e3-b9a5c4e9d9balogseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[297,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## Helpful Links\\nhttps://kaiokendev.github.io/context\\nhttps://kaiokendev.github.io/til\\nhttps://spaces.ac.cn/archives/9948\\nrerope：https://spaces.ac.cn/archives/9708logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[297,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[297,logseq____&quot;^Flogseq____&quot;,296,536870939]],[logseq____&quot;^15logseq____&quot;,[297,logseq____&quot;^Xlogseq____&quot;,290,536870939]],[logseq____&quot;^15logseq____&quot;,[297,logseq____&quot;^Vlogseq____&quot;,290,536870939]],[logseq____&quot;^15logseq____&quot;,[297,logseq____&quot;^Ulogseq____&quot;,290,536870939]],[logseq____&quot;^15logseq____&quot;,[297,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870939]],[logseq____&quot;^15logseq____&quot;,[297,logseq____&quot;^Jlogseq____&quot;,[],536870939]],[logseq____&quot;^15logseq____&quot;,[297,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2a-fb65-456b-bbe4-0d0a7a7215fblogseq____&quot;,536870939]],[logseq____&quot;^15logseq____&quot;,[299,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;logseq____&quot;,536870940]],[logseq____&quot;^15logseq____&quot;,[299,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870940]],[logseq____&quot;^15logseq____&quot;,[299,logseq____&quot;^Flogseq____&quot;,253,536870940]],[logseq____&quot;^15logseq____&quot;,[299,logseq____&quot;^Xlogseq____&quot;,253,536870940]],[logseq____&quot;^15logseq____&quot;,[299,logseq____&quot;^Vlogseq____&quot;,253,536870940]],[logseq____&quot;^15logseq____&quot;,[299,logseq____&quot;^Ulogseq____&quot;,253,536870940]],[logseq____&quot;^15logseq____&quot;,[299,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-b296-4258-9b5b-d7d8e9f1024clogseq____&quot;,536870940]],[logseq____&quot;^15logseq____&quot;,[301,logseq____&quot;^Klogseq____&quot;,1723260459036,536870941]],[logseq____&quot;^15logseq____&quot;,[301,logseq____&quot;^@logseq____&quot;,false,536870941]],[logseq____&quot;^15logseq____&quot;,[301,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;👋 hi, therelogseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[301,logseq____&quot;^11logseq____&quot;,logseq____&quot;👋 Hi, therelogseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[301,logseq____&quot;^Blogseq____&quot;,1723260459036,536870941]],[logseq____&quot;^15logseq____&quot;,[301,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-27a1-4d61-a6a5-3453bb61785elogseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[302,logseq____&quot;^Klogseq____&quot;,1723260459037,536870941]],[logseq____&quot;^15logseq____&quot;,[302,logseq____&quot;^@logseq____&quot;,false,536870941]],[logseq____&quot;^15logseq____&quot;,[302,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;算法工程与代码logseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[302,logseq____&quot;^11logseq____&quot;,logseq____&quot;算法工程与代码logseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[302,logseq____&quot;^Blogseq____&quot;,1723260459037,536870941]],[logseq____&quot;^15logseq____&quot;,[302,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-cf06-4936-979d-9787728d1de0logseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[303,logseq____&quot;^Klogseq____&quot;,1723260459037,536870941]],[logseq____&quot;^15logseq____&quot;,[303,logseq____&quot;^@logseq____&quot;,false,536870941]],[logseq____&quot;^15logseq____&quot;,[303,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;leetcodelogseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[303,logseq____&quot;^11logseq____&quot;,logseq____&quot;LEETCODElogseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[303,logseq____&quot;^Blogseq____&quot;,1723260459037,536870941]],[logseq____&quot;^15logseq____&quot;,[303,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-2761-4633-b643-6fbda549fa53logseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[304,logseq____&quot;^Klogseq____&quot;,1723260459037,536870941]],[logseq____&quot;^15logseq____&quot;,[304,logseq____&quot;^@logseq____&quot;,false,536870941]],[logseq____&quot;^15logseq____&quot;,[304,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;alphafoldlogseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[304,logseq____&quot;^11logseq____&quot;,logseq____&quot;AlphaFoldlogseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[304,logseq____&quot;^Blogseq____&quot;,1723260459037,536870941]],[logseq____&quot;^15logseq____&quot;,[304,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-eab0-461c-af58-96e37c067cc7logseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[305,logseq____&quot;^Klogseq____&quot;,1723260459038,536870941]],[logseq____&quot;^15logseq____&quot;,[305,logseq____&quot;^@logseq____&quot;,false,536870941]],[logseq____&quot;^15logseq____&quot;,[305,logseq____&quot;^Ylogseq____&quot;,logseq____&quot;自然语言处理logseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[305,logseq____&quot;^11logseq____&quot;,logseq____&quot;自然语言处理logseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[305,logseq____&quot;^Blogseq____&quot;,1723260459038,536870941]],[logseq____&quot;^15logseq____&quot;,[305,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-c035-4484-bb2c-4e8786f1bca5logseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[306,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;This is Jay Luo. I am gradually documenting my work notes as an AI engineer in this blog. Meanwhile, thoughts and other writings will appear in the third part.logseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[306,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[306,logseq____&quot;^Flogseq____&quot;,301,536870941]],[logseq____&quot;^15logseq____&quot;,[306,logseq____&quot;^Xlogseq____&quot;,301,536870941]],[logseq____&quot;^15logseq____&quot;,[306,logseq____&quot;^Vlogseq____&quot;,301,536870941]],[logseq____&quot;^15logseq____&quot;,[306,logseq____&quot;^Ulogseq____&quot;,301,536870941]],[logseq____&quot;^15logseq____&quot;,[306,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-d8ff-48c7-a93a-4f96da80e39elogseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[307,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## About me\\n**2014.9 - 2021.7**.    UnderGrad and Graduate  at Sichuan University**\\n**2021.7 - 2022.8**.    AI engineer at Bytedance**\\n**2022.8 - Now**.         AI engineer at BioMaplogseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[307,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[307,logseq____&quot;^Flogseq____&quot;,306,536870941]],[logseq____&quot;^15logseq____&quot;,[307,logseq____&quot;^Xlogseq____&quot;,301,536870941]],[logseq____&quot;^15logseq____&quot;,[307,logseq____&quot;^Vlogseq____&quot;,301,536870941]],[logseq____&quot;^15logseq____&quot;,[307,logseq____&quot;^Ulogseq____&quot;,301,536870941]],[logseq____&quot;^15logseq____&quot;,[307,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870941]],[logseq____&quot;^15logseq____&quot;,[307,logseq____&quot;^Jlogseq____&quot;,[],536870941]],[logseq____&quot;^15logseq____&quot;,[307,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-5654-4b0d-83a9-9336fae3f95flogseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[308,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## Recent Update\\n[[RLHF中的PPO算法笔记]]\\n[[outlines:大模型固定格式解码]]\\n[[语言模型理解更长的输入]]logseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[308,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[308,logseq____&quot;^Flogseq____&quot;,307,536870941]],[logseq____&quot;^15logseq____&quot;,[308,logseq____&quot;^Xlogseq____&quot;,301,536870941]],[logseq____&quot;^15logseq____&quot;,[308,logseq____&quot;^Vlogseq____&quot;,301,536870941]],[logseq____&quot;^15logseq____&quot;,[308,logseq____&quot;^Ulogseq____&quot;,95,536870941]],[logseq____&quot;^15logseq____&quot;,[308,logseq____&quot;^Ulogseq____&quot;,149,536870941]],[logseq____&quot;^15logseq____&quot;,[308,logseq____&quot;^Ulogseq____&quot;,290,536870941]],[logseq____&quot;^15logseq____&quot;,[308,logseq____&quot;^Ulogseq____&quot;,301,536870941]],[logseq____&quot;^15logseq____&quot;,[308,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870941]],[logseq____&quot;^15logseq____&quot;,[308,logseq____&quot;^Jlogseq____&quot;,[],536870941]],[logseq____&quot;^15logseq____&quot;,[308,logseq____&quot;^Hlogseq____&quot;,95,536870941]],[logseq____&quot;^15logseq____&quot;,[308,logseq____&quot;^Hlogseq____&quot;,149,536870941]],[logseq____&quot;^15logseq____&quot;,[308,logseq____&quot;^Hlogseq____&quot;,290,536870941]],[logseq____&quot;^15logseq____&quot;,[308,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-87b4-4229-962a-acea15fff32flogseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[309,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## The place to collect knowledge\\nTODO  organizing and sharing work notes.\\n[[预训练与语言模型]]\\n[[自然语言处理]]\\n[[搜索与推荐]]\\n[[算法工程与代码]]\\n[[AlphaFold]]\\n[[LEETCODE]]logseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[309,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[309,logseq____&quot;^Flogseq____&quot;,308,536870941]],[logseq____&quot;^15logseq____&quot;,[309,logseq____&quot;^Xlogseq____&quot;,301,536870941]],[logseq____&quot;^15logseq____&quot;,[309,logseq____&quot;^Vlogseq____&quot;,301,536870941]],[logseq____&quot;^15logseq____&quot;,[309,logseq____&quot;^Ulogseq____&quot;,250,536870941]],[logseq____&quot;^15logseq____&quot;,[309,logseq____&quot;^Ulogseq____&quot;,253,536870941]],[logseq____&quot;^15logseq____&quot;,[309,logseq____&quot;^Ulogseq____&quot;,301,536870941]],[logseq____&quot;^15logseq____&quot;,[309,logseq____&quot;^Ulogseq____&quot;,302,536870941]],[logseq____&quot;^15logseq____&quot;,[309,logseq____&quot;^Ulogseq____&quot;,303,536870941]],[logseq____&quot;^15logseq____&quot;,[309,logseq____&quot;^Ulogseq____&quot;,304,536870941]],[logseq____&quot;^15logseq____&quot;,[309,logseq____&quot;^Ulogseq____&quot;,305,536870941]],[logseq____&quot;^15logseq____&quot;,[309,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870941]],[logseq____&quot;^15logseq____&quot;,[309,logseq____&quot;^Jlogseq____&quot;,[],536870941]],[logseq____&quot;^15logseq____&quot;,[309,logseq____&quot;^Hlogseq____&quot;,250,536870941]],[logseq____&quot;^15logseq____&quot;,[309,logseq____&quot;^Hlogseq____&quot;,253,536870941]],[logseq____&quot;^15logseq____&quot;,[309,logseq____&quot;^Hlogseq____&quot;,302,536870941]],[logseq____&quot;^15logseq____&quot;,[309,logseq____&quot;^Hlogseq____&quot;,303,536870941]],[logseq____&quot;^15logseq____&quot;,[309,logseq____&quot;^Hlogseq____&quot;,304,536870941]],[logseq____&quot;^15logseq____&quot;,[309,logseq____&quot;^Hlogseq____&quot;,305,536870941]],[logseq____&quot;^15logseq____&quot;,[309,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-1b47-4780-b728-bace4f5540f9logseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[310,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## The place to share\\nTODO Making my writings openlogseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[310,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[310,logseq____&quot;^Flogseq____&quot;,309,536870941]],[logseq____&quot;^15logseq____&quot;,[310,logseq____&quot;^Xlogseq____&quot;,301,536870941]],[logseq____&quot;^15logseq____&quot;,[310,logseq____&quot;^Vlogseq____&quot;,301,536870941]],[logseq____&quot;^15logseq____&quot;,[310,logseq____&quot;^Ulogseq____&quot;,301,536870941]],[logseq____&quot;^15logseq____&quot;,[310,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870941]],[logseq____&quot;^15logseq____&quot;,[310,logseq____&quot;^Jlogseq____&quot;,[],536870941]],[logseq____&quot;^15logseq____&quot;,[310,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-a46a-4aac-bc80-67cbc3210019logseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[311,logseq____&quot;^Qlogseq____&quot;,logseq____&quot;## Connect\\n[📧](mailto:sculuo96@gmail.com) \\n[🐙](https://github.com/luome)logseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[311,logseq____&quot;^Ologseq____&quot;,logseq____&quot;^16logseq____&quot;,536870941]],[logseq____&quot;^15logseq____&quot;,[311,logseq____&quot;^Flogseq____&quot;,310,536870941]],[logseq____&quot;^15logseq____&quot;,[311,logseq____&quot;^Xlogseq____&quot;,301,536870941]],[logseq____&quot;^15logseq____&quot;,[311,logseq____&quot;^Vlogseq____&quot;,301,536870941]],[logseq____&quot;^15logseq____&quot;,[311,logseq____&quot;^Ulogseq____&quot;,301,536870941]],[logseq____&quot;^15logseq____&quot;,[311,logseq____&quot;^?logseq____&quot;,[logseq____&quot;^ logseq____&quot;,logseq____&quot;^19logseq____&quot;,2],536870941]],[logseq____&quot;^15logseq____&quot;,[311,logseq____&quot;^Jlogseq____&quot;,[],536870941]],[logseq____&quot;^15logseq____&quot;,[311,logseq____&quot;^;logseq____&quot;,logseq____&quot;~u66b6de2b-12c1-4ddd-9804-ead65a79c292logseq____&quot;,536870941]]]]]]"</script>
 <script>window.logseq_state="{:ui/theme \"light\", :ui/radix-color :blue, :config {\"local\" {:feature/enable-whiteboards? false, :shortcuts {}, :default-templates {:journals \"\"}, :feature/enable-journals? false, :query/views {:pprint (fn [r] [:pre.code (pprint r)])}, :macros {}, :shortcut/doc-mode-enter-for-new-block? false, :favorites [\"👋 hi, there\" \"recent update\" \"hi, there\" \"welcome\"], :ui/show-empty-bullets? false, :file/name-format :triple-lowbar, :feature/enable-flashcards? false, :preferred-workflow :now, :publishing/all-pages-public? true, :ui/show-brackets? false, :ref/default-open-blocks-level 2, :feature/enable-block-timestamps? false, :start-of-week 6, :ref/linked-references-collapsed-threshold 50, :outliner/block-title-collapse-enabled? false, :commands [], :ui/show-full-blocks? false, :meta/version 1, :hidden [], :default-queries {:journals [{:title \"🔨 NOW\", :query [:find (pull ?h [*]) :in $ ?start ?today :where [?h :block/marker ?marker] [(contains? #{\"NOW\" \"DOING\"} ?marker)] [?h :block/page ?p] [?p :block/journal? true] [?p :block/journal-day ?d] [(>= ?d ?start)] [(<= ?d ?today)]], :inputs [:14d :today], :result-transform (fn [result] (sort-by (fn [h] (get h :block/priority \"Z\")) result)), :group-by-page? false, :collapsed? false} {:title \"📅 NEXT\", :query [:find (pull ?h [*]) :in $ ?start ?next :where [?h :block/marker ?marker] [(contains? #{\"NOW\" \"LATER\" \"TODO\"} ?marker)] [?h :block/page ?p] [?p :block/journal? true] [?p :block/journal-day ?d] [(> ?d ?start)] [(< ?d ?next)]], :inputs [:today :7d-after], :group-by-page? false, :collapsed? false}]}, :ui/auto-expand-block-refs? true, :ui/enable-tooltip? true, :query/result-transforms {:sort-by-priority (fn [result] (sort-by (fn [h] (get h :block/priority \"Z\")) result))}, :property-pages/enabled? true, :block/content-max-length 10000, :ui/show-command-doc? true, :feature/enable-search-remove-accents? true, :default-home {:page \"👋 Hi, there\"}}}}"</script>
 <script type="text/javascript">// Single Page Apps for GitHub Pages
      // https://github.com/rafgraph/spa-github-pages
      // Copyright (c) 2016 Rafael Pedicini, licensed under the MIT License
      // ----------------------------------------------------------------------
      // This script checks to see if a redirect is present in the query string
      // and converts it back into the correct url and adds it to the
      // browser's history using window.history.replaceState(...),
      // which won't cause the browser to attempt to load the new url.
      // When the single page app is loaded further down in this file,
      // the correct url will be waiting in the browser's history for
      // the single page app to route accordingly.
      (function(l) {
        if (l.search) {
          var q = {};
          l.search.slice(1).split('&').forEach(function(v) {
            var a = v.split('=');
            q[a[0]] = a.slice(1).join('=').replace(/~and~/g, '&');
          });
          if (q.p !== undefined) {
            window.history.replaceState(null, null,
              l.pathname.slice(0, -1) + (q.p || '') +
              (q.q ? ('?' + q.q) : '') +
              l.hash
            );
          }
        }
      }(window.location))</script>
 <script src="static/js/react.production.min.js"></script>
 <script src="static/js/react-dom.production.min.js"></script>
 <script src="static/js/ui.js"></script>
 <script src="static/js/main.js"></script>
 <script src="static/js/interact.min.js"></script>
 <script src="static/js/highlight.min.js"></script>
 <script src="static/js/katex.min.js"></script>
 <script src="static/js/html2canvas.min.js"></script>
 <script src="static/js/code-editor.js"></script>
 <script src="static/js/custom.js"></script>
</body>
