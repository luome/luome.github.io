public:: true

- {{renderer :tocgen2}}
-
- **为什么使用预训练模型？**
	- 1.Pre-training on the huge text corpus can learn universal language representations and help with the downstream tasks.
	  2. Pre-training provides a better model initialization, which usually leads to a better generalization performance and speeds up convergence on the target task.
	  3. Pre-training can be regarded as a kind of regularization to avoid overfitting on small data
- ## Transformer
	- [[Transformer]]
	- [[Transformer Family]]
- ## 语言模型
	- [语言模型简介](../assets/周五分享_1676819672849_0.pptx)
	- ### 自编码语言模型
		- 遮蔽语言模型
			- Bert #prefixlm
				- [[BERT]]
				- [[RoBERTa]]
				- [[ALBERT]]  #模型压缩
				- [[ERNIE]] 将BERT中的随机masking改为实体或短语级别的masking（引入知识，使得模型能够从中学习到更多句法语义知识，在许多中文任务上取得SOTA。
		- Seq2Seq MLM
			- [[T5]]
	- ### 自回归语言模型
		- 单向语言模型
			- [[GPT]] #causallm
		- 双向语言模型
			- ELMO #prefixlm
	- ### 融合自回归与自编码
		- Permuted LM
			- [[XLNET]] #causallm
		- [[UNILM]]
		- [[BART]]
		- [[GLM]]
	- ### pretraining with different learning method
		- [[MT-DNN]] 多任务+预训练
		- [[DIM Loss in NLP pretraining]] 最大互信息损失函数
	- ### 文本表示
		- [[BERT-Flow]]
		- [[BERT-whitening]]
		- [[SimCSE]]
	- #prefixlm 与 #causallm 的区别：
		- prefixlm 可以在处理输入序列时考虑了当前词或标记的前后上下文。这意味着模型不仅考虑当前词之前的词（左上文），还考虑当前词之后的词（右上文）。
		- causallm 在生成或处理一个词或标记时只考虑该词之前的上下文（也就是左上文）
		- 但是 prefixlm 也可以适配生成模型，模型先接收一个包含问题或提示的输入序列，并基于这个“前缀”生成一个与之相关的输出或回答。可以通过 attention mask 实现。如 unilm
- ## Large Language Model
	- [[LargeLanguageModel]]是在预训练基础上，用大量参数来训练语言模型
	- ### 数据
		- [[LLM数据]]
	- ### 模型
		- [[LLAMA]] #llm
		- [[GLM]] #llm
		- [[PaLM]] #llm
		- [[BLOOM]] #llm
		- [[GPT3]] #llm
		- [[Gopher]]
		- [[instructGPT]] #llm
		- ![Galactica](../assets/galactica_1683972858902_0.pdf) 科学领域#llm
		- [[Phi textbooks are all you need]] #llm
		- [[Gemini]]
		- [[StarCoder]]
		- [[Falcon]]
	- ### 评估
		- [[Holistic Evaluation of Language Models]]
		- [[Big Bench]]
	- ### MoE
		- [[sparse upcycling]]
		- [[Mixtral]]
	- ### [[alignment for llm]]
		- reinforcement learning from feedback
			- [[RLHF]]
			- [[RLAIF]]
		- [[LIMA]]
		- [[DPO]] #强化学习
	- ### 模版工程 #prompting
		- [[in context learning]]
		- [[Longer Context Prompting and Retrieval Augment Generation]] #rag
- ## 多模态预训练
	- [[ViT]]
	- [[VL-BERT]]
	- [[UNITER]]
	- [[BEiT-3]]
	- [[MAE]]
- ## 多模态生成
	- [[Sora]]
	- [[生成算法]]
- ## 预训练技术
	- [[预训练技术]]  #位置编码
	- [[Finetune, Adapter, Prompt tuning& LoRA]] #LoRA
	- ### 生成策略
		- [[生成模型的解码方法]]
	- ### 框架与分布式
		- [[Huggingface]]
		- [[Megatron-如何训练GPT]] #megatron-lm
		- [[分布式训练]]
	- ### LLM 加速
		- [[faster inference]]
		- [[llm 加速]]
	- 量化语言模型
		- [[量化LLM]]
- ## Insights
	- [[LLM Insights]]
	- [[reading list for andrej karpathys intro llm]]
	- [[Training Compute-Optimal Large Language Models]]
	- [[语言模型理解更长的输入]]
	-
- ## Survey
	- ![Transformer and Pre-trained Models, A Survey.pdf](../assets/Transformer_and_Pre-trained_Models_A_Survey.pdf)
	- [预训练语言模型.xmind](../assets/预训练语言模型_1676819647079_0.xmind)